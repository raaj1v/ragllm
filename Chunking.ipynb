{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final both (removal of duplicate index (after chucking) and removal of duplicate of document(set())\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def get_embeddings_via_api(sentence: str) -> List[float]:\n",
    "    \"\"\"Get embeddings from API (using all-MiniLM-L6-v2 model)\"\"\"\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",  # Endpoint of your embedding API\n",
    "        json={\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"input\": [sentence]}\n",
    "    )\n",
    "    # Ensure response was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error in API call: {response.status_code} - {response.text}\")\n",
    "\n",
    "def split_into_sentences(text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Split text into sentences with metadata and remove exact duplicates\"\"\"\n",
    "    sentences = [{'sentence': s, 'index': i} \n",
    "                 for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "    \n",
    "    # Remove exact duplicates by using a set to track unique sentences\n",
    "    unique_sentences = []\n",
    "    seen_sentences = set()  # Set to track seen sentences (exact match)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # If the sentence hasn't been seen before, add it\n",
    "        if sentence['sentence'] not in seen_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "            seen_sentences.add(sentence['sentence'])\n",
    "\n",
    "    return combine_sentences(unique_sentences)\n",
    "\n",
    "def combine_sentences(sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Combine sentences with context\"\"\"\n",
    "    combined = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        context = []\n",
    "        for j in range(max(0, i - buffer_size), i):\n",
    "            context.append(sentences[j]['sentence'])\n",
    "        context.append(sent['sentence'])\n",
    "        for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "            context.append(sentences[j]['sentence'])\n",
    "        sent['combined_sentence'] = ' '.join(context)\n",
    "        combined.append(sent)\n",
    "    return combined\n",
    "\n",
    "def create_chunks(sentences: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create document chunks based on semantic similarity using API for embeddings\"\"\"\n",
    "    embeddings = [get_embeddings_via_api(s['combined_sentence']) for s in sentences]\n",
    "    distances = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "            np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1])\n",
    "        )\n",
    "        distances.append(1 - similarity)\n",
    "\n",
    "    threshold = np.percentile(distances, 95)\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    for i, distance in enumerate(distances):\n",
    "        if distance > threshold:\n",
    "            chunk = {\n",
    "                'chunk': ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]]),\n",
    "                'indices': [s['index'] for s in sentences[start_idx:i + 1]]  # Track indices\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "            start_idx = i + 1\n",
    "    if start_idx < len(sentences):\n",
    "        chunk = {\n",
    "            'chunk': ' '.join([s['sentence'] for s in sentences[start_idx:]]),\n",
    "            'indices': [s['index'] for s in sentences[start_idx:]]  # Track indices\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "    return len(text.split()) * 1.3  # Rough estimate of tokens\n",
    "\n",
    "def chunk_by_tokens(semantic_chunks: List[Dict[str, Any]], max_chunk_tokens: int) -> List[str]:\n",
    "    \"\"\"Split semantic chunks into smaller chunks based on token count\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    all_used_indices = set()  # To track used indices\n",
    "    \n",
    "    for chunk in semantic_chunks:\n",
    "        estimated_tokens = estimate_tokens(chunk['chunk'])\n",
    "        \n",
    "        if current_tokens + estimated_tokens > max_chunk_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [chunk['chunk']]\n",
    "            current_tokens = estimated_tokens\n",
    "            all_used_indices.update(chunk['indices'])  # Keep track of indices in the current chunk\n",
    "        else:\n",
    "            current_chunk.append(chunk['chunk'])\n",
    "            current_tokens += estimated_tokens\n",
    "            all_used_indices.update(chunk['indices'])  # Add current chunk's indices to the set\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    # Return final chunks along with the set of all used indices\n",
    "    return chunks, all_used_indices\n",
    "\n",
    "def process_document(file_path: str, max_chunk_tokens: int) -> List[str]:\n",
    "    \"\"\"Process document and split into chunks\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        sentences = split_into_sentences(text)\n",
    "        semantic_chunks = create_chunks(sentences)\n",
    "        chunked_texts, all_used_indices = chunk_by_tokens(semantic_chunks, max_chunk_tokens)\n",
    "        return chunked_texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage:\n",
    "file = f\"/data/searchEnhancement/sumit/78804029.txt\"\n",
    "\n",
    "# Define a maximum token size\n",
    "max_chunk_tokens = 100000  # or whatever value fits your requirements\n",
    "\n",
    "# Process the document\n",
    "chunks = process_document(file, max_chunk_tokens)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sumit code\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "             model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            # \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\"\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\"\n",
    "            # \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            # \"List of all the dates mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD amount,tender fee, tender value\": \"Important Dates\",\n",
    "            # \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.request_count = 0 \n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 1000) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            self.request_count += 1  # Increment the request counter\n",
    "            \n",
    "            # Print the current request details\n",
    "            print(f\"Request {self.request_count}:\")\n",
    "            print(f\"Query: {query}\")\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document\n",
    "        chunks = self.process_document(file_path)\n",
    "        combined_text = \" \".join(chunks)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        results = {}\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, combined_text): title\n",
    "                for query, title in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] = response\n",
    "                except Exception as e:\n",
    "                    results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file = \"/data/Pqmatch/testing/78804029/78804029.txt\"\n",
    "    \n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
