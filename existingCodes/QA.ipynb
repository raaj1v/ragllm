{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "@dataclass\n",
    "class TenderQuery:\n",
    "    \"\"\"Data class to store query information\"\"\"\n",
    "    query: str\n",
    "    title: str\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, verbose=True, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            # \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"Extract all point of Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            # \"List all mandatory qualification criteria, including blacklisting status and required certifications.\": \"Mandatory Qualification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"List of all the dates with its time mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD date\":\"Importants Date\",\n",
    "            # \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Split into sentences and create chunks\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 3500) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str, output_path: str) -> None:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document\n",
    "        chunks = self.process_document(file_path)\n",
    "        combined_text = \" \".join(chunks)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        results = defaultdict(str)\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, combined_text): title\n",
    "                for query, title in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] = response\n",
    "                except Exception as e:\n",
    "                    results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Create DataFrame and save results\n",
    "        df = pd.DataFrame({\n",
    "            'Title': list(self.queries.values()),\n",
    "            'Response': [results[title] for title in self.queries.values()]\n",
    "        })\n",
    "        \n",
    "        # Save to Excel\n",
    "        df.to_excel(output_path, index=False)\n",
    "        \n",
    "        # Also save as JSON for backup\n",
    "        json_path = output_path.rsplit('.', 1)[0] + '.json'\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dict(results), f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = TenderAnalyzer()\n",
    "    \n",
    "    # Process tender document\n",
    "    input_file = '/data/Pqmatch/testing/78216093/78216093.txt'\n",
    "    output_file = '78216093.xlsx'\n",
    "    \n",
    "    analyzer.analyze_tender(input_file, output_file)\n",
    "    print(f\"Analysis completed. Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return result in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# import warnings\n",
    "# import numpy as np\n",
    "# from typing import Dict, List, Any\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from langchain.schema import Document\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "# from langchain.callbacks import get_openai_callback\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Suppress warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Environment setup\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# class TenderAnalyzer:\n",
    "#     \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "#     def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "#         self.model = SentenceTransformer(model_name)\n",
    "#         self.llm = ChatOpenAI(\n",
    "#              model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "#             openai_api_base=\"http://localhost:8000/v1\",\n",
    "#             openai_api_key=\"FAKE\",\n",
    "#             max_tokens=500,\n",
    "#             temperature=0.1\n",
    "#         )\n",
    "#         self.chain = load_qa_chain(self.llm, verbose=True, chain_type='stuff')\n",
    "#         self.queries = {\n",
    "#             \"Identify the functional requirements, also referred to as the scope of work, specified in the document.\": \"Scope of Work\",\n",
    "#             \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "#             \"List all mandatory qualification criteria, including blacklisting status and required certifications.\": \"Mandatory Qualification Criteria\",\n",
    "#             \"Summarize the work specifications that bidders must meet to fulfill the tender requirements.\": \"Specifications\",\n",
    "#             \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "#             \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "#             \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "#         }\n",
    "\n",
    "#     def process_document(self, file_path: str) -> List[str]:\n",
    "#         \"\"\"Process document and split into chunks\"\"\"\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             text = f.read()\n",
    "        \n",
    "#         # Split into sentences and create chunks\n",
    "#         sentences = self._split_into_sentences(text)\n",
    "#         chunks = self._create_chunks(sentences)\n",
    "#         return self._chunk_by_tokens(chunks)\n",
    "\n",
    "#     def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"Split text into sentences with metadata\"\"\"\n",
    "#         sentences = [{'sentence': s, 'index': i} \n",
    "#                     for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "#         return self._combine_sentences(sentences)\n",
    "\n",
    "#     def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"Combine sentences with context\"\"\"\n",
    "#         combined = []\n",
    "#         for i, sent in enumerate(sentences):\n",
    "#             context = []\n",
    "#             # Add previous sentences\n",
    "#             for j in range(max(0, i - buffer_size), i):\n",
    "#                 context.append(sentences[j]['sentence'])\n",
    "#             # Add current and next sentences\n",
    "#             context.append(sent['sentence'])\n",
    "#             for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "#                 context.append(sentences[j]['sentence'])\n",
    "#             sent['combined_sentence'] = ' '.join(context)\n",
    "#             combined.append(sent)\n",
    "#         return combined\n",
    "\n",
    "#     def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "#         \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "#         # Create embeddings\n",
    "#         embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "#         # Calculate distances\n",
    "#         distances = []\n",
    "#         for i in range(len(embeddings) - 1):\n",
    "#             similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "#                 np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "#             distances.append(1 - similarity)\n",
    "        \n",
    "#         # Split into chunks\n",
    "#         threshold = np.percentile(distances, 95)\n",
    "#         chunks = []\n",
    "#         start_idx = 0\n",
    "        \n",
    "#         for i, distance in enumerate(distances):\n",
    "#             if distance > threshold:\n",
    "#                 chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "#                 chunks.append(chunk)\n",
    "#                 start_idx = i + 1\n",
    "        \n",
    "#         if start_idx < len(sentences):\n",
    "#             chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "#             chunks.append(chunk)\n",
    "        \n",
    "#         return chunks\n",
    "\n",
    "#     def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 3500) -> List[str]:\n",
    "#         \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "#         max_chars = max_tokens * 2\n",
    "#         chunks = []\n",
    "#         for text in texts:\n",
    "#             text_chunks = [text[i:i + max_chars] \n",
    "#                          for i in range(0, len(text), max_chars)]\n",
    "#             chunks.extend(text_chunks)\n",
    "#         return chunks\n",
    "\n",
    "#     def process_query(self, query: str, text: str) -> str:\n",
    "#         \"\"\"Process a single query against the text\"\"\"\n",
    "#         try:\n",
    "#             with get_openai_callback() as cb:\n",
    "#                 response = self.chain.run(\n",
    "#                     input_documents=[Document(page_content=text)],\n",
    "#                     question=query\n",
    "#                 )\n",
    "#             return response.strip()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing query: {e}\")\n",
    "#             return f\"Error: {str(e)}\"\n",
    "\n",
    "#     def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "#         \"\"\"Main analysis function\"\"\"\n",
    "#         # Process document\n",
    "#         chunks = self.process_document(file_path)\n",
    "#         combined_text = \" \".join(chunks)\n",
    "        \n",
    "#         # Process queries in parallel\n",
    "#         results = {}\n",
    "#         with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "#             future_to_query = {\n",
    "#                 executor.submit(self.process_query, query, combined_text): title\n",
    "#                 for query, title in self.queries.items()\n",
    "#             }\n",
    "            \n",
    "#             for future in as_completed(future_to_query):\n",
    "#                 title = future_to_query[future]\n",
    "#                 try:\n",
    "#                     response = future.result()\n",
    "#                     results[title] = response\n",
    "#                 except Exception as e:\n",
    "#                     results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "#         return results\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     # Initialize analyzer\n",
    "#     analyzer = TenderAnalyzer()\n",
    "    \n",
    "#     # Process tender document\n",
    "#     input_file = '/data/Pqmatch/testing/78216093/78216093.txt'\n",
    "    \n",
    "#     # Analyze and get results\n",
    "#     results = analyzer.analyze_tender(input_file)\n",
    "    \n",
    "#     # Output path for JSON\n",
    "#     output_file = '/data/QAAPI/stored_files/77774640_analysis.json'\n",
    "    \n",
    "#     # Ensure output directory exists\n",
    "#     os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "#     # Save results as JSON\n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "#     print(f\"Analysis completed. Results saved to {output_file}\")\n",
    "    \n",
    "#     # Return the results for potential further processing\n",
    "#     return results\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return response in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Prequalification Criteria\": \"Based on the provided text data, the following clauses specify pre-qualification criteria or eligibility criteria:\\n\\n1. Clause 1.3.1 of the SBD (Standard Bidding Document):\\n\\n\\\"A Bidder shall be a registered contractor in Kerala Public Works Department or from any State or Central Government Engineering Departments which are having similar functionalities like Kerala PWD in the required category as specified in the NIT.\\\"\\n\\n2. Clause 1.3.2 of the SBD:\\n\\n\\\"Only those bidders having a valid and active registration, on the date of bid submission, shall submit bids online on the e-GP website.\\\"\\n\\n3. Clause 1.3.4 of the SBD:\\n\\n\\\"All Bidders are required to register in the e-procurement portal. The Bidder intending to participate in the bid is required to register in the e-tenders portal using his Login ID and attach his valid Digital Signature Certificate (DSC) to his unique Login ID.\\\"\\n\\n4. Clause 1.3.5 of the SBD:\\n\\n\\\"A firm/bidder shall submit only one bid in the same bidding process. A Bidder (either as a firm or as an individual or as a partner of a firm) who submits or participates in more than one bid will cause all the proposals in which the Bidder has participated to be disqualified.\\\"\\n\\n5. Clause 1.3.6 of the SBD:\\n\\n\\\"Joint ventures or Consortiums of two or more registered contactors are not permitted.\\\"\\n\\n6. Clause 1.3.7 of the SBD (modified clause):\\n\\n\\\"The Bidder shall have valid GST Registration.\\\"\\n\\n7. Clause 12.1 of the Special Conditions of Contract:\\n\\n\\\"No subcontracting shall be done without prior written approval of Agreement Authority. Maximum value of works to be sub-contracted is limited to 25% of Contract value.\\\"\\n\\n8. Clause 16.1 of the Special Conditions of Contract:\\n\\n\\\"The contractor shall employ engineering personnel in addition to other supporting staff as detailed below for tenure of the contract for works supervision depending upon the cost of work.\\\"\\n\\n9. Clause 18.1 of the Special Conditions of Contract:\\n\\n\\\"In case of Civil works awarded by Government of Kerala deduction towards KVAT at the prevailing rates(as provided in the Contract Data) will be done on the gross amount of bill payable for the bidders every time.\\\"\\n\\n10. Clause 19.1 of the Special Conditions of Contract:\\n\\n\\\"Deduction towards the Kerala Construction Workers Welfare Fund Board contribution will be\",\n",
      "    \"Scope of Work\": \"The functional requirements, also referred to as the scope of work, specified in the document are:\\n\\n1. Concreting of 113.12 mtr of footpath concrete with a width of 2.5 mtr.\\n2. Earthwork excavation for road formation, footpath concreting, and Granite Name board.\\n3. Necessary provision for earthwork excavation for road formation, footpath concreting, Granite Name board, etc.\\n4. Execution of the work as per MORD specifications with a cost index of Munnar @39.83%.\\n5. Provision for GST @18% is included in the estimate.\\n6. The total estimate cost comes to Rs.3,50,000/- which is within the AS Amount of Rs. 3.50 Lakhs (SCP).\\n7. The work is to be executed by e-tendering with a period of completion of 6 Months.\\n8. Necessary documents including SC Feasibility Certificate, Company NOC, Site Plan, etc. are attached with the estimate.\\n\\nThese requirements are specified in the \\\"Brief Description of Work\\\" section of the document.\",\n",
      "    \"Important Dates\": \"Here is the list of extracted dates, times, and monetary values along with their specific labels or descriptions:\\n\\n**Dates:**\\n\\n1. 18-11-2024 (Date of publication of NIT)\\n2. 19-11-2024 (Date of document download/sale start)\\n3. 28-11-2024 (Date of document download/sale end)\\n4. 28-11-2024 (Last date and time for bid submission)\\n5. 30-11-2024 (Date and time of opening of tender)\\n6. 4-2-2016 (Date of Finance Department circular No-8/2016/Fin)\\n7. 10-9-2015 (Date of GO(Rt) No-1339/2015/PWD)\\n8. 11-9-2015 (Date of GO(Rt) No-1346/2015/PWD)\\n9. 24-7-2015 (Date of GO(Ms) No-65/2015//PWD)\\n10. 2024-11-18 (Date of printing of documents)\\n\\n**Times:**\\n\\n1. 06:00 PM (Time of publication of NIT)\\n2. 06:00 PM (Time of document download/sale start)\\n3. 18:00 (Last date and time for bid submission)\\n4. 11:00 AM (Date and time of opening of tender)\\n5. 20:48:19 (Time of printing of documents)\\n\\n**Monetary Values:**\\n\\n1. Rs. 296593 (Estimated probable amount of contract)\\n2. Rs. 3,50,000/- (Total estimate cost)\\n3. Rs. 3.50 Lakhs (AS Amount of Rs. 3.50 Lakhs (SCP))\\n4. Rs. 7415/- (Bid Security)\\n5. Rs. 655+(0)GST (Bid Submission fee (Tender fee))\\n6. Rs. 2.00 lakh (Minimum deduction for insurance)\\n7. Rs. 5.00 lakh (Maximum deduction for insurance)\\n8. Rs. 200 (Stamp paper value for preliminary agreement)\\n9. Rs. 1000 (Minimum fine for non-compliance with GST)\\n10. Rs. 25,000 (Maximum fine for non-compliance with GST)\\n11. Rs. 4 lakhs (Maximum bonus for timely completion)\\n12.\",\n",
      "    \"Specifications\": \"The work specifications for the tender requirements are as follows:\\n\\n1. The work involves concreting of 113.12 mtr of footpath concrete with a width of 2.5 mtr.\\n2. The estimate includes provision for earthwork excavation for road formation, footpath concreting, Granite Name board, etc.\\n3. The estimate is prepared as per MORD specifications with a cost index of Munnar @39.83%.\\n4. Provision for GST @18% is included in the estimate.\\n5. The total estimate cost comes to Rs.3,50,000/-, which is within the AS Amount of Rs. 3.50 Lakhs (SCP).\\n6. The work is to be executed by e-tendering with a period of completion of 6 months.\\n7. Necessary documents, including SC Feasibility Certificate, Company NOC, Site Plan, etc., are attached with the estimate.\\n\\nThe bidder must meet the following specifications:\\n\\n1. The bidder must have a valid GST registration.\\n2. The bidder must have a valid registration with the Kerala Public Works Department (KPWD) or any State or Central Government Engineering Departments with similar functionalities.\\n3. The bidder must have a valid bank account with core banking facility and an email address.\\n4. The bidder must submit the bid documents online only and in the designated cover(s)/envelope(s) on the e-GP website.\\n5. The bidder must remit the bid submission fee and bid security online during the time of bid submission.\\n6. The bidder must submit the price bid online only.\\n7. The bidder must meet the technical specifications and requirements mentioned in the bid document.\\n8. The bidder must have the necessary equipment, tools, and personnel to execute the work.\\n9. The bidder must have a valid license to carry out the work.\\n10. The bidder must comply with all labor laws and regulations, including the Payment of Wages Act, Employees Liability Act, Workmen's Compensation Act, Employees State Insurance Act, Employees Provident Fund Act, Industrial Disputes Act, Maternity Benefit Act, Contract Labour (Regulation and Abolition) Act, and Factories Act.\\n\\nThe bidder must also meet the following special conditions:\\n\\n1. The bidder must not employ any labor less than 18 years of age on the job.\\n2. The bidder must make necessary provisions for safeguarding and care of female labor's children and keeping them clear of the site.\\n3\",\n",
      "    \"Contact Details\": \"The contact details of the officer are not explicitly mentioned in the provided text. However, the document mentions the name of the officer as \\\"Assistant Engineer, LSGD Section Devikulam\\\" and the office address as \\\"O/o Office of the Assistant Engineer, LSGD Section Devikulam\\\".\",\n",
      "    \"Supporting Documents\": \"Based on the provided text data, the following supporting documents are required for this tender:\\n\\n1. Tender Notice (NIT)\\n2. Instructions to Bidders (ITB)\\n3. Special Conditions of Contract (SCC)\\n4. General Conditions of Contract (GCC)\\n5. Bill of Quantities (BoQ)\\n6. Drawings\\n7. Specifications\\n8. Form of Bid\\n9. Preliminary Agreement Format\\n10. Integrity Pact Certificate\\n11. Affidavit\\n12. Performance Guarantee by Bank\\n13. Requisition Form for e-Payment\\n14. Sample Guarantee Bond for termite-proof/water and leak-proof work\\n\\nAdditionally, the following documents are mentioned as being attached with the estimate:\\n\\n1. SC Feasibility Certificate\\n2. Company NOC\\n3. Site Plan\\n\\nPlease note that this list may not be exhaustive, and other documents may be required as specified in the tender notice or other supporting documents.\",\n",
      "    \"Mandatory Qualification Criteria\": \"Based on the provided documents, the following are the mandatory qualification criteria for the bidder:\\n\\n1. Registration with Kerala Public Works Department (KPWD) or any State or Central Government Engineering Departments with similar functionalities like KPWD.\\n2. Valid and active registration on the date of bid submission.\\n3. GST Registration (added as a new sub-clause 1.3.7 in the Instructions to Bidders).\\n4. Valid GST registration certificate issued by the Government of India (added as a new sub-clause 4.3.12 in the Instructions to Bidders).\\n5. Valid GST registration certificate issued by the Government of India (added as a new sub-clause 4.4.1.9 in the Instructions to Bidders).\\n6. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 19.1 in the Special Conditions of Contract).\\n7. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 4.4.1.9 in the Instructions to Bidders).\\n8. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 4.4.1.9 in the Instructions to Bidders).\\n9. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 19.1 in the Special Conditions of Contract).\\n10. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 4.4.1.9 in the Instructions to Bidders).\\n11. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 19.1 in the Special Conditions of Contract).\\n12. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 4.4.1.9 in the Instructions to Bidders).\\n13. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 19.1 in the Special Conditions of Contract).\\n14. Valid registration with the Kerala Construction Workers Welfare Fund Board (KCWWF) (added as a new sub-clause 4.4.1.9 in the Instructions to Bidders).\\n15. Valid registration with the\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "             model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"Identify the functional requirements, also referred to as the scope of work, specified in the document.\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all mandatory qualification criteria, including blacklisting status and required certifications.\": \"Mandatory Qualification Criteria\",\n",
    "            \"Summarize the work specifications that bidders must meet to fulfill the tender requirements.\": \"Specifications\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Split into sentences and create chunks\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 3500) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document\n",
    "        chunks = self.process_document(file_path)\n",
    "        combined_text = \" \".join(chunks)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        results = {}\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, combined_text): title\n",
    "                for query, title in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] = response\n",
    "                except Exception as e:\n",
    "                    results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file = '/data/Pqmatch/testing/78216093/78216093.txt'\n",
    "    \n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"Identify the functional requirements, also referred to as the scope of work, specified in the document.\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            # \"List all mandatory qualification criteria, including blacklisting status and required certifications.\": \"Mandatory Qualification Criteria\",\n",
    "            # \"Summarize the work specifications that bidders must meet to fulfill the tender requirements.\": \"Specifications\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Split into sentences and create chunks\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i}\n",
    "                     for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "\n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "\n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "\n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 3500) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars]\n",
    "                           for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document into chunks\n",
    "        chunks = self.process_document(file_path)\n",
    "\n",
    "        # Results storage\n",
    "        results = {title: \"\" for title in self.queries.values()}\n",
    "        \n",
    "        # Map futures to queries and titles\n",
    "        future_to_query = {}\n",
    "\n",
    "        # Limit workers to prevent overloading\n",
    "        max_workers = min(multiprocessing.cpu_count(), len(self.queries))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tasks for each chunk and query pair\n",
    "            for chunk in chunks:\n",
    "                for query, title in self.queries.items():\n",
    "                    future = executor.submit(self.process_query, query, chunk)\n",
    "                    future_to_query[future] = title\n",
    "\n",
    "            # Gather results\n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] += f\"{response}\\n\"\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing query '{title}': {e}\")\n",
    "                    results[title] += f\"Error: {str(e)}\\n\"\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file = '/data/Pqmatch/testing/78216093/78216093.txt'\n",
    "\n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "\n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tiktoken\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def truncate_text(text, max_tokens=7000):\n",
    "    \"\"\"\n",
    "    Truncate text to specified max tokens\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return tokenizer.decode(truncated_tokens)\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Count tokens in a given text\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        # Truncate each text to manage token count\n",
    "        truncated_texts = [truncate_text(text) for text in texts]\n",
    "        return self.model.encode(truncated_texts).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        # Truncate query text\n",
    "        truncated_text = truncate_text(text)\n",
    "        return self.model.encode(truncated_text).tolist()\n",
    "\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            # Extract text content from Document objects\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "def chunk_text_with_token_limit(text, max_tokens=7000):\n",
    "    \"\"\"\n",
    "    Chunk text while respecting token limits\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    # Split tokens into chunks\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i+max_tokens]\n",
    "        chunk = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_text(texts, labels=None):\n",
    "    # Default labels if not provided\n",
    "    if labels is None:\n",
    "        labels = [\n",
    "            \"Important Dates\", \n",
    "            \"Eligibility Criteria\", \n",
    "            \"Scope of Work\", \n",
    "            \"Contact Details\", \n",
    "            \"Tender Overview\", \n",
    "            \"Submission Guidelines\"\n",
    "        ]\n",
    "    \n",
    "    # Join all document texts into a single string\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    \n",
    "    # Truncate combined text to manage token count\n",
    "    combined_text = truncate_text(combined_text)\n",
    "    \n",
    "    # Split sentences\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', combined_text)\n",
    "    \n",
    "    # Create sentence objects with index\n",
    "    sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]\n",
    "    \n",
    "    # Function to combine sentences into chunks\n",
    "    def combine_sentences(sentences, buffer_size=1):\n",
    "        for i in range(len(sentences)):\n",
    "            combined_sentence = ''\n",
    "            for j in range(i - buffer_size, i):\n",
    "                if j >= 0:\n",
    "                    combined_sentence += sentences[j]['sentence'] + ' '\n",
    "            \n",
    "            combined_sentence += sentences[i]['sentence']\n",
    "            \n",
    "            for j in range(i + 1, i + 1 + buffer_size):\n",
    "                if j < len(sentences):\n",
    "                    combined_sentence += ' ' + sentences[j]['sentence']\n",
    "            sentences[i]['combined_sentence'] = combined_sentence\n",
    "        return sentences\n",
    "    \n",
    "    # Initialize Sentence Transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Combine sentences\n",
    "    sentences = combine_sentences(sentences)\n",
    "    \n",
    "    # Create embeddings\n",
    "    try:\n",
    "        embeddings = model.encode([x['combined_sentence'] for x in sentences])\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        embeddings = model.encode([x['combined_sentence'][:1000] for x in sentences])\n",
    "    \n",
    "    # Prepare labeled chunks\n",
    "    labeled_chunks = {label: [] for label in labels + ['Uncategorized']}\n",
    "    final_chunks = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Try to assign a label based on predefined labels\n",
    "        label_match = 'Uncategorized'\n",
    "        for label in labels:\n",
    "            if label.lower() in sentence['combined_sentence'].lower():\n",
    "                label_match = label\n",
    "                break\n",
    "        \n",
    "        # Limit chunk size\n",
    "        chunk = truncate_text(sentence['combined_sentence'], max_tokens=1000)\n",
    "        \n",
    "        # Add chunk with label prefix\n",
    "        labeled_chunk = f\"[{label_match}] {chunk}\"\n",
    "        final_chunks.append(labeled_chunk)\n",
    "        labeled_chunks[label_match].append(chunk)\n",
    "    \n",
    "    # Create embeddings for the knowledge base\n",
    "    try:\n",
    "        embeddings = CustomEmbeddings()\n",
    "        knowledge_base = FAISS.from_texts(final_chunks, embedding=embeddings)\n",
    "    except Exception as e:\n",
    "        print(f\"Knowledge base creation error: {e}\")\n",
    "        # Fallback: create knowledge base with truncated chunks\n",
    "        fallback_chunks = [truncate_text(chunk, max_tokens=500) for chunk in final_chunks]\n",
    "        knowledge_base = FAISS.from_texts(fallback_chunks, embedding=embeddings)\n",
    "    \n",
    "    return knowledge_base, labeled_chunks\n",
    "\n",
    "def process_query(knowledge_base, query, title):\n",
    "    # Truncate query to manage token count\n",
    "    query = truncate_text(query, max_tokens=3500)\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    docs = knowledge_base.similarity_search(query, k=3)  # Limit to 3 most relevant docs\n",
    "    \n",
    "    # Prepare docs to fit within token limit\n",
    "    context_docs = []\n",
    "    current_tokens = 0\n",
    "    for doc in docs:\n",
    "        doc_tokens = count_tokens(doc.page_content)\n",
    "        if current_tokens + doc_tokens <= 3500:\n",
    "            context_docs.append(doc)\n",
    "            current_tokens += doc_tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "     model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Load the QA chain\n",
    "    chain = load_qa_chain(llm, verbose=False, chain_type='stuff')\n",
    "    \n",
    "    # Run the chain and capture the response\n",
    "    try:\n",
    "        response = chain.run(input_documents=context_docs, question=query)\n",
    "        # Strip unnecessary context\n",
    "        response = response.strip()\n",
    "        \n",
    "        # Truncate response if it's too long\n",
    "        response = truncate_text(response, max_tokens=1000)\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "def main(folder_path, queries, output_file, labels=None):\n",
    "    # Load documents from the specified folder\n",
    "    all_docs_text = load_text_files_from_directory(folder_path)\n",
    "    \n",
    "    # Process texts and create knowledge base with labels\n",
    "    knowledge_base, labeled_chunks = process_text(all_docs_text, labels)\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    titles = []\n",
    "    responses = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor to run queries in parallel\n",
    "    with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "        future_to_query = {\n",
    "            executor.submit(process_query, knowledge_base, query, title): title \n",
    "            for query, title in queries.items()\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_query):\n",
    "            title = future_to_query[future]\n",
    "            try:\n",
    "                result_title, result_response = future.result()\n",
    "                titles.append(result_title)\n",
    "                responses.append((result_title, result_response))\n",
    "            except Exception as e:\n",
    "                print(f\"Query processing failed for title {title}: {e}\")\n",
    "                titles.append(title)\n",
    "                responses.append((title, f\"Error: {e}\"))\n",
    "    \n",
    "    # Create a DataFrame for responses\n",
    "    df_responses = pd.DataFrame({\n",
    "        'Title': [title for title in queries.values()],\n",
    "        'Response': [\n",
    "            next((resp for t, resp in responses if t == title), 'No response') \n",
    "            for title in queries.values()\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Create a DataFrame for labeled chunks\n",
    "    df_labels = pd.DataFrame.from_dict(labeled_chunks, orient='index')\n",
    "    df_labels.index.name = 'Label'\n",
    "    df_labels.reset_index(inplace=True)\n",
    "    \n",
    "    # Save DataFrames to Excel\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        df_responses.to_excel(writer, sheet_name='Responses', index=False)\n",
    "        df_labels.to_excel(writer, sheet_name='Labeled Chunks', index=False)\n",
    "    \n",
    "    print(f\"Responses and labeled chunks have been saved to '{output_file}'.\")\n",
    "    \n",
    "    return knowledge_base, labeled_chunks\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/data/tendergpt/livetender_txt/71484890\"\n",
    "    \n",
    "    # Predefined labels\n",
    "    labels = [\n",
    "        \"Important Dates\", \n",
    "        \"Eligibility Criteria\", \n",
    "        \"Scope of Work\", \n",
    "        \"Contact Details\", \n",
    "        \"Tender Overview\", \n",
    "        \"Submission Guidelines\"\n",
    "    ]\n",
    "    \n",
    "    queries = {\n",
    "        \"extract the all important date  with time and amount mentioned in this document\": \"Contact Details\"\n",
    "    }\n",
    "    \n",
    "    output_file = '77326167.xlsx'\n",
    "    \n",
    "    # Run the main function\n",
    "    knowledge_base, labeled_chunks = main(folder_path, queries, output_file, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"0\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "Responses have been saved to 'output.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "# from langchain.chains.qa import load_qa_chain\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# Get embeddings from external API\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    \"\"\"\n",
    "    Loads text content from all `.txt` files in the specified folder.\n",
    "    \"\"\"\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                all_text.append(content)\n",
    "    return all_text\n",
    "\n",
    "\n",
    "\n",
    "# Sentence processing and chunk creation\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Prepare sentence embeddings\n",
    "def process_text(texts):\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', combined_text)\n",
    "    sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]\n",
    "    \n",
    "    sentences = combine_sentences(sentences)\n",
    "    embeddings = model.encode([x['combined_sentence'] for x in sentences])\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Calculate cosine distances between consecutive sentences\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences\n",
    "\n",
    "\n",
    "# Custom similarity search using FAISS\n",
    "def similarity_search(faiss_index, query_embedding, top_k=5):\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)  # Ensure correct shape\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    return indices[0], distances[0]\n",
    "\n",
    "\n",
    "# FAISS semantic clustering\n",
    "def segment_text_with_faiss_label_assignment(semantic_chunks, threshold=-0.7):\n",
    "    labeled_segments = defaultdict(list)\n",
    "    for chunk in semantic_chunks:\n",
    "        if chunk.strip():\n",
    "            paragraph_embedding = model.encode(chunk).reshape(1, -1)\n",
    "            distances, label_indices = faiss_index.search(paragraph_embedding, len(labels))\n",
    "            similarities = 1 - distances\n",
    "            assigned_labels = [labels[i] for i, sim in enumerate(similarities[0]) if sim >= threshold]\n",
    "\n",
    "            if assigned_labels:\n",
    "                for label in assigned_labels:\n",
    "                    labeled_segments[label].append(chunk)\n",
    "            else:\n",
    "                labeled_segments[\"Other\"].append(chunk)\n",
    "    return labeled_segments\n",
    "\n",
    "\n",
    "# Main processing\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "labels = [\"Important Dates\", \"Eligibility or Prequalification Criteria\", \"Scope of Work\", \"Contact Details\"]\n",
    "\n",
    "folder_path = \"/data/tendergpt/testing/77153810\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "sentences = process_text(all_docs_text)\n",
    "\n",
    "distances, sentences = calculate_cosine_distances(sentences)\n",
    "\n",
    "breakpoint_percentile_threshold = 95\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "\n",
    "start_index = 0\n",
    "chunks = []\n",
    "for index in indices_above_thresh:\n",
    "    end_index = index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    start_index = index + 1\n",
    "\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "\n",
    "# Initialize FAISS knowledge base\n",
    "label_embeddings = model.encode(labels)\n",
    "dimension = label_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(label_embeddings)\n",
    "\n",
    "segmented_result = segment_text_with_faiss_label_assignment(chunks)\n",
    "\n",
    "# Save result to JSON\n",
    "out_file_path = r'/data/QAAPI/stored_files/out.json'\n",
    "with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(segmented_result, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "queries = {\n",
    "    \"Extract all the points of Eligibility criteria from the tender document\": \"PQ\"\n",
    "}\n",
    "\n",
    "\n",
    "def process_query(query, title):\n",
    "    query_embedding = model.encode(query)\n",
    "    indices, distances = similarity_search(faiss_index, query_embedding, top_k=5)\n",
    "\n",
    "    # Retrieve corresponding chunks based on indices\n",
    "    docs = [chunks[i] for i in indices if i < len(chunks)]\n",
    "    \n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Load the QA chain\n",
    "    chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Run the chain and capture the response\n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "titles = []\n",
    "responses = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "\n",
    "df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to 'output.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text file: /data/tendergpt/testing/77153810/77153810.txt\n",
      "646 sentences found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base created successfully\n",
      "Query processing failed for title PQ: 'CustomEmbeddings' object is not callable\n",
      "Responses have been saved to '77326167.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "import faiss\n",
    "\n",
    "# Custom embedding function and class\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    return [{'sentence': x, 'index': i} for i, x in enumerate(sentences)]\n",
    "\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        \n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "        \n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "        \n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        \n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def calculate_distances(sentences, embeddings):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = embeddings[i]\n",
    "        embedding_next = embeddings[i + 1]\n",
    "        \n",
    "        similarity = np.dot(embedding_current, embedding_next) / (\n",
    "            np.linalg.norm(embedding_current) * np.linalg.norm(embedding_next)\n",
    "        )\n",
    "        \n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    \n",
    "    return distances, sentences\n",
    "\n",
    "def create_knowledge_base(sentences, embedder):\n",
    "    # Convert sentences to Documents format\n",
    "    documents = []\n",
    "    for sentence in sentences:\n",
    "        doc = Document(\n",
    "            page_content=sentence['combined_sentence'],\n",
    "            metadata={'index': sentence['index']}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    embeddings = [sentence['combined_sentence_embedding'] for sentence in sentences]\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    \n",
    "    vector_store = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(texts, embeddings)),\n",
    "        embedding=embedder,\n",
    "        metadatas=[doc.metadata for doc in documents]\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def process_query(query, title, knowledge_base):\n",
    "    docs = knowledge_base.similarity_search(query, k=4)  # Retrieve top 4 relevant documents\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "    \n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "def process_tender_document(file_path, output_file):\n",
    "    # Process text\n",
    "    text = load_text_file(file_path)\n",
    "    print(f\"Loaded text file: {file_path}\")\n",
    "    \n",
    "    sentences = split_into_sentences(text)\n",
    "    print(f\"{len(sentences)} sentences found\")\n",
    "    \n",
    "    sentences = combine_sentences(sentences)\n",
    "    \n",
    "    embedder = CustomEmbeddings()\n",
    "    embeddings = embedder.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    \n",
    "    distances, sentences = calculate_distances(sentences, embeddings)\n",
    "    \n",
    "    # Create knowledge base\n",
    "    knowledge_base = create_knowledge_base(sentences, embedder)\n",
    "    print(\"Knowledge base created successfully\")\n",
    "    \n",
    "    # Define queries for analysis\n",
    "    queries = {\n",
    "        \"Extract all the points of Eligibility criteria from the tender document\": \"PQ\"\n",
    "    }\n",
    "    \n",
    "    # Process queries in parallel\n",
    "    titles = []\n",
    "    responses = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "        future_to_query = {\n",
    "            executor.submit(process_query, query, title, knowledge_base): title \n",
    "            for query, title in queries.items()\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_query):\n",
    "            title = future_to_query[future]\n",
    "            try:\n",
    "                result_title, result_response = future.result()\n",
    "                titles.append(result_title)\n",
    "                responses.append((result_title, result_response))\n",
    "            except Exception as e:\n",
    "                print(f\"Query processing failed for title {title}: {e}\")\n",
    "                titles.append(title)\n",
    "                responses.append((title, f\"Error: {e}\"))\n",
    "    \n",
    "    # Aggregate and save results\n",
    "    aggregated_responses = aggregate_responses(responses)\n",
    "    df = pd.DataFrame({\n",
    "        'Title': [title for title in queries.values()],\n",
    "        'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "    })\n",
    "    \n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Responses have been saved to '{output_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/data/tendergpt/testing/77153810/77153810.txt\"\n",
    "    output_file = \"77326167.xlsx\"\n",
    "    process_tender_document(file_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import faiss\n",
    "\n",
    "# Custom embedding class implementing LangChain's Embeddings interface\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self):\n",
    "        self.api_url = \"http://0.0.0.0:5002/embeddings\"\n",
    "        self.model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                json={\"model\": self.model, \"input\": texts}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return [item['embedding'] for item in data['data']]\n",
    "            else:\n",
    "                raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in embed_documents: {str(e)}\")\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a query.\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                json={\"model\": self.model, \"input\": [text]}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data['data'][0]['embedding']\n",
    "            else:\n",
    "                raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in embed_query: {str(e)}\")\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    return [{'sentence': x, 'index': i} for i, x in enumerate(sentences)]\n",
    "\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    combined_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        \n",
    "        # Add previous sentences\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "        \n",
    "        # Add current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "        \n",
    "        # Add following sentences\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        \n",
    "        combined_sentences.append({\n",
    "            'index': sentences[i]['index'],\n",
    "            'combined_sentence': combined_sentence.strip()\n",
    "        })\n",
    "    \n",
    "    return combined_sentences\n",
    "\n",
    "def create_knowledge_base(sentences, embedder):\n",
    "    try:\n",
    "        # Convert sentences to Documents format\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=sentence['combined_sentence'],\n",
    "                metadata={'index': sentence['index']}\n",
    "            )\n",
    "            for sentence in sentences\n",
    "        ]\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        vector_store = FAISS.from_texts(\n",
    "            texts,\n",
    "            embedder,\n",
    "            metadatas=[doc.metadata for doc in documents]\n",
    "        )\n",
    "        \n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error creating knowledge base: {str(e)}\")\n",
    "\n",
    "def process_query(query, title, knowledge_base):\n",
    "    try:\n",
    "        docs = knowledge_base.similarity_search(query, k=4)\n",
    "        \n",
    "        llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=4096,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "        \n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        \n",
    "        return title, response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_query: {str(e)}\")\n",
    "        return title, f\"Error processing query: {str(e)}\"\n",
    "\n",
    "def process_tender_document(file_path, output_file):\n",
    "    try:\n",
    "        # Process text\n",
    "        print(f\"Loading text file: {file_path}\")\n",
    "        text = load_text_file(file_path)\n",
    "        \n",
    "        print(\"Splitting text into sentences...\")\n",
    "        sentences = split_into_sentences(text)\n",
    "        print(f\"{len(sentences)} sentences found\")\n",
    "        \n",
    "        print(\"Combining sentences with context...\")\n",
    "        sentences = combine_sentences(sentences)\n",
    "        \n",
    "        print(\"Initializing embedding model...\")\n",
    "        embedder = CustomEmbeddings()\n",
    "        \n",
    "        print(\"Creating knowledge base...\")\n",
    "        knowledge_base = create_knowledge_base(sentences, embedder)\n",
    "        print(\"Knowledge base created successfully\")\n",
    "        \n",
    "        # Define queries for analysis\n",
    "        queries = {\n",
    "            \"Extract all the points of Eligibility criteria from the tender document. List each criterion separately.\": \"PQ\"\n",
    "        }\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        titles = []\n",
    "        responses = []\n",
    "        \n",
    "        print(\"Processing queries...\")\n",
    "        with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(process_query, query, title, knowledge_base): title \n",
    "                for query, title in queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    result_title, result_response = future.result()\n",
    "                    titles.append(result_title)\n",
    "                    responses.append((result_title, result_response))\n",
    "                except Exception as e:\n",
    "                    print(f\"Query processing failed for title {title}: {e}\")\n",
    "                    titles.append(title)\n",
    "                    responses.append((title, f\"Error: {e}\"))\n",
    "        \n",
    "        # Aggregate and save results\n",
    "        print(\"Saving results...\")\n",
    "        aggregated_responses = {\n",
    "            title: \"\\n\".join(response for t, response in responses if t == title)\n",
    "            for title in set(titles)\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Title': [title for title in queries.values()],\n",
    "            'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "        })\n",
    "        \n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"Responses have been saved to '{output_file}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_tender_document: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/data/tendergpt/testing/77153810/77153810.txt\"\n",
    "    output_file = \"77326167.xlsx\"\n",
    "    process_tender_document(file_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without visualizatiomn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import faiss\n",
    "\n",
    "# Custom embedding function and class\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "# Load and process text file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    return [{'sentence': x, 'index': i} for i, x in enumerate(sentences)]\n",
    "\n",
    "# Combine sentences with buffer\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        \n",
    "        # Add previous sentences\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "        \n",
    "        # Add current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "        \n",
    "        # Add following sentences\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        \n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Calculate distances between embeddings\n",
    "def calculate_distances(sentences, embeddings):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = embeddings[i]\n",
    "        embedding_next = embeddings[i + 1]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(embedding_current, embedding_next) / (\n",
    "            np.linalg.norm(embedding_current) * np.linalg.norm(embedding_next)\n",
    "        )\n",
    "        \n",
    "        # Convert to distance\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    \n",
    "    return distances, sentences\n",
    "\n",
    "# Semantic clustering and labeling functions\n",
    "def initialize_label_index(labels, embedder):\n",
    "    \"\"\"Initialize FAISS index with label embeddings\"\"\"\n",
    "    label_embeddings = embedder.embed_documents(labels)\n",
    "    label_embeddings = np.array(label_embeddings).astype('float32')\n",
    "    \n",
    "    dimension = len(label_embeddings[0])\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    faiss_index.add(label_embeddings)\n",
    "    \n",
    "    return faiss_index, label_embeddings\n",
    "\n",
    "def segment_text_with_labels(chunks, labels, embedder, threshold=-0.7):\n",
    "    \"\"\"Segment text chunks and assign labels based on semantic similarity\"\"\"\n",
    "    labeled_segments = defaultdict(list)\n",
    "    faiss_index, label_embeddings = initialize_label_index(labels, embedder)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():\n",
    "            chunk_embedding = np.array(embedder.embed_query(chunk)).reshape(1, -1).astype('float32')\n",
    "            \n",
    "            # Get similarity scores with all labels\n",
    "            distances, label_indices = faiss_index.search(chunk_embedding, len(labels))\n",
    "            similarities = 1 - distances / 2  # Convert L2 distances to approximate cosine similarities\n",
    "            \n",
    "            # Assign labels based on similarity threshold\n",
    "            assigned_labels = [labels[i] for i, sim in enumerate(similarities[0]) if sim >= threshold]\n",
    "            \n",
    "            for label in assigned_labels:\n",
    "                labeled_segments[label].append(chunk)\n",
    "            if not assigned_labels:\n",
    "                labeled_segments[\"Other\"].append(chunk)\n",
    "    \n",
    "    return labeled_segments\n",
    "\n",
    "# Main processing function\n",
    "def process_text(file_path):\n",
    "    # Load text\n",
    "    text = load_text_file(file_path)\n",
    "    \n",
    "    # Split into sentences\n",
    "    sentences = split_into_sentences(text)\n",
    "    \n",
    "    # Combine sentences with context\n",
    "    sentences = combine_sentences(sentences)\n",
    "    \n",
    "    # Create embeddings\n",
    "    embedder = CustomEmbeddings()\n",
    "    embeddings = embedder.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "    \n",
    "    # Store embeddings in sentences\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances, sentences = calculate_distances(sentences, embeddings)\n",
    "    \n",
    "    return sentences, distances, embeddings, embedder\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # File path and labels\n",
    "    file_path = \"/data/tendergpt/testing/77153810/77153810.txt\"\n",
    "    labels = [\"Important Date\", \"Eligibility or Prequalification Criteria\", \n",
    "              \"scope of work\", \"Contact Details\"]\n",
    "    \n",
    "    # Process text\n",
    "    sentences, distances, embeddings, embedder = process_text(file_path)\n",
    "    \n",
    "    # Identify chunk breakpoints\n",
    "    breakpoint_distance_threshold = np.percentile(distances, 95)\n",
    "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    for index in indices_above_thresh:\n",
    "        group = sentences[start_index:index + 1]\n",
    "        combined_text = ' '.join([d['sentence'] for d in group])\n",
    "        chunks.append(combined_text)\n",
    "        start_index = index + 1\n",
    "    \n",
    "    # Add final chunk\n",
    "    if start_index < len(sentences):\n",
    "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "        chunks.append(combined_text)\n",
    "    \n",
    "    # Segment and label chunks\n",
    "    labeled_segments = segment_text_with_labels(chunks, labels, embedder)\n",
    "    \n",
    "    # Save results to JSON\n",
    "    out_file_path = 'labeled_segments.json'\n",
    "    with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(labeled_segments, out_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working with semenation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from typing import List\n",
    "import faiss\n",
    "\n",
    "# Custom embedding function and class\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "# Load and process text file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    return [{'sentence': x, 'index': i} for i, x in enumerate(sentences)]\n",
    "\n",
    "# Combine sentences with buffer\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        \n",
    "        # Add previous sentences\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "        \n",
    "        # Add current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "        \n",
    "        # Add following sentences\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        \n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Calculate distances between embeddings\n",
    "def calculate_distances(sentences, embeddings):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = embeddings[i]\n",
    "        embedding_next = embeddings[i + 1]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(embedding_current, embedding_next) / (\n",
    "            np.linalg.norm(embedding_current) * np.linalg.norm(embedding_next)\n",
    "        )\n",
    "        \n",
    "        # Convert to distance\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    \n",
    "    return distances, sentences\n",
    "\n",
    "# Semantic clustering and labeling functions\n",
    "def initialize_label_index(labels, embedder):\n",
    "    \"\"\"Initialize FAISS index with label embeddings\"\"\"\n",
    "    label_embeddings = embedder.embed_documents(labels)\n",
    "    label_embeddings = np.array(label_embeddings).astype('float32')\n",
    "    \n",
    "    dimension = len(label_embeddings[0])\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    faiss_index.add(label_embeddings)\n",
    "    \n",
    "    return faiss_index, label_embeddings\n",
    "\n",
    "def segment_text_with_labels(chunks, labels, embedder, threshold=-0.7):\n",
    "    \"\"\"Segment text chunks and assign labels based on semantic similarity\"\"\"\n",
    "    labeled_segments = defaultdict(list)\n",
    "    faiss_index, label_embeddings = initialize_label_index(labels, embedder)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():\n",
    "            chunk_embedding = np.array(embedder.embed_query(chunk)).reshape(1, -1).astype('float32')\n",
    "            \n",
    "            # Get similarity scores with all labels\n",
    "            distances, label_indices = faiss_index.search(chunk_embedding, len(labels))\n",
    "            similarities = 1 - distances / 2  # Convert L2 distances to approximate cosine similarities\n",
    "            \n",
    "            # Assign labels based on similarity threshold\n",
    "            assigned_labels = [labels[i] for i, sim in enumerate(similarities[0]) if sim >= threshold]\n",
    "            \n",
    "            if assigned_labels:\n",
    "                print(f\"Assigned labels: {assigned_labels}\")\n",
    "                for label in assigned_labels:\n",
    "                    labeled_segments[label].append(chunk)\n",
    "            else:\n",
    "                labeled_segments[\"Other\"].append(chunk)\n",
    "    \n",
    "    # Print statistics\n",
    "    for label in labels + [\"Other\"]:\n",
    "        print(f\"{label}: {len(labeled_segments[label])} chunks\")\n",
    "    \n",
    "    return labeled_segments\n",
    "\n",
    "def visualize_clusters(labeled_segments, labels, embedder):\n",
    "    \"\"\"Visualize the clustering of text segments\"\"\"\n",
    "    colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']\n",
    "    scatter_points = []\n",
    "    labels_for_plot = []\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    for label_index, label in enumerate(labels):\n",
    "        paragraphs = labeled_segments[label]\n",
    "        if paragraphs:\n",
    "            paragraph_embeddings = embedder.embed_documents(paragraphs)\n",
    "            scatter_points.append(np.array(paragraph_embeddings))\n",
    "            labels_for_plot.extend([label] * len(paragraphs))\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    all_embeddings = np.vstack(scatter_points)\n",
    "    \n",
    "    # Cluster using KMeans\n",
    "    n_clusters = len(labels)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(all_embeddings)\n",
    "    \n",
    "    # Reduce dimensions for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, label in enumerate(labels):\n",
    "        indices = np.where(cluster_labels == i)[0]\n",
    "        if len(indices) > 0:\n",
    "            plt.scatter(reduced_embeddings[indices, 0], \n",
    "                       reduced_embeddings[indices, 1],\n",
    "                       label=label, \n",
    "                       color=colors[i % len(colors)], \n",
    "                       alpha=0.7)\n",
    "    \n",
    "    plt.title('Text Segmentation Clusters')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main processing function\n",
    "def process_text(file_path):\n",
    "    # Load text\n",
    "    text = load_text_file(file_path)\n",
    "    print(f\"Loaded text file: {file_path}\")\n",
    "    \n",
    "    # Split into sentences\n",
    "    sentences = split_into_sentences(text)\n",
    "    print(f\"{len(sentences)} sentences found\")\n",
    "    \n",
    "    # Combine sentences with context\n",
    "    sentences = combine_sentences(sentences)\n",
    "    \n",
    "    # Create embeddings\n",
    "    embedder = CustomEmbeddings()\n",
    "    embeddings = embedder.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "    \n",
    "    # Store embeddings in sentences\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances, sentences = calculate_distances(sentences, embeddings)\n",
    "    \n",
    "    return sentences, distances, embeddings, embedder\n",
    "\n",
    "# Visualization function for chunks\n",
    "def visualize_chunks(distances, breakpoint_percentile=95, y_upper_bound=0.2):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(distances)\n",
    "    plt.ylim(0, y_upper_bound)\n",
    "    plt.xlim(0, len(distances))\n",
    "    \n",
    "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile)\n",
    "    plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-')\n",
    "    \n",
    "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "    num_chunks = len(indices_above_thresh) + 1\n",
    "    \n",
    "    plt.text(x=(len(distances)*0.01), y=y_upper_bound/50, s=f\"{num_chunks} Chunks\")\n",
    "    \n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    for i, breakpoint_index in enumerate(indices_above_thresh):\n",
    "        start_index = 0 if i == 0 else indices_above_thresh[i - 1]\n",
    "        end_index = breakpoint_index\n",
    "        \n",
    "        plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)\n",
    "        plt.text(x=np.average([start_index, end_index]),\n",
    "                y=breakpoint_distance_threshold + (y_upper_bound)/20,\n",
    "                s=f\"Chunk #{i}\",\n",
    "                horizontalalignment='center',\n",
    "                rotation='vertical')\n",
    "    \n",
    "    if indices_above_thresh:\n",
    "        last_breakpoint = indices_above_thresh[-1]\n",
    "        if last_breakpoint < len(distances):\n",
    "            plt.axvspan(last_breakpoint, len(distances),\n",
    "                       facecolor=colors[len(indices_above_thresh) % len(colors)],\n",
    "                       alpha=0.25)\n",
    "            plt.text(x=np.average([last_breakpoint, len(distances)]),\n",
    "                    y=breakpoint_distance_threshold + (y_upper_bound)/20,\n",
    "                    s=f\"Chunk #{len(indices_above_thresh)}\",\n",
    "                    rotation='vertical')\n",
    "    \n",
    "    plt.title(\"Text Chunks Based On Embedding Breakpoints\")\n",
    "    plt.xlabel(\"Sentence Position\")\n",
    "    plt.ylabel(\"Cosine distance between sequential sentences\")\n",
    "    plt.show()\n",
    "    \n",
    "    return indices_above_thresh\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # File path and labels\n",
    "    file_path = \"/data/tendergpt/testing/77153810/77153810.txt\"\n",
    "    labels = [\"Important Date\", \"Eligibility or Prequalification Criteria\", \n",
    "              \"scope of work\", \"Contact Details\"]\n",
    "    \n",
    "    # Process text\n",
    "    sentences, distances, embeddings, embedder = process_text(file_path)\n",
    "    \n",
    "    # Visualize and get chunk breakpoints\n",
    "    indices_above_thresh = visualize_chunks(distances)\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    for index in indices_above_thresh:\n",
    "        group = sentences[start_index:index + 1]\n",
    "        combined_text = ' '.join([d['sentence'] for d in group])\n",
    "        chunks.append(combined_text)\n",
    "        start_index = index + 1\n",
    "    \n",
    "    # Add final chunk\n",
    "    if start_index < len(sentences):\n",
    "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "        chunks.append(combined_text)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Segment and label chunks\n",
    "    labeled_segments = segment_text_with_labels(chunks, labels, embedder)\n",
    "    \n",
    "    # Visualize clusters\n",
    "    visualize_clusters(labeled_segments, labels, embedder)\n",
    "    \n",
    "    # Save results to JSON\n",
    "    out_file_path = 'labeled_segments.json'\n",
    "    with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(labeled_segments, out_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Results saved to {out_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from langchain.llms import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "# New Query Processing Functions\n",
    "def process_query(query, title, knowledge_base):\n",
    "    \"\"\"Process a single query using the knowledge base\"\"\"\n",
    "    try:\n",
    "        # Retrieve relevant documents\n",
    "        docs = knowledge_base.similarity_search(query)\n",
    "        \n",
    "        # Initialize the language model\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=4096,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Create and run the QA chain\n",
    "        chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "        \n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        \n",
    "        return title, response.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {e}\")\n",
    "        return title, f\"Error processing query: {e}\"\n",
    "\n",
    "def aggregate_responses(responses):\n",
    "    \"\"\"Aggregate responses by title\"\"\"\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "def process_queries(knowledge_base, output_file):\n",
    "    \"\"\"Process all queries and save results\"\"\"\n",
    "    # Define queries\n",
    "    queries = {\n",
    "        \"Extract all points related to bidder eligibility criteria, pre-qualification requirements, \"\n",
    "        \"blacklisting criteria, and technical capability for bid submission as mentioned in this \"\n",
    "        \"tender document. Include specific requirements such as: experience, financial criteria, \"\n",
    "        \"certifications, technical qualifications, equipment or resource requirements, technical \"\n",
    "        \"capability specifications, prior project experience, legal compliance, mandatory documents \"\n",
    "        \"or certificates, and any conditions related to blacklisting or prior performance. Ensure \"\n",
    "        \"all technical qualifications, minimum standards, and capability-related conditions are \"\n",
    "        \"accurately extracted without including irrelevant information.\": \"PQ\"\n",
    "    }\n",
    "    \n",
    "    # Initialize results storage\n",
    "    titles = []\n",
    "    responses = []\n",
    "    \n",
    "    # Process queries in parallel\n",
    "    with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "        future_to_query = {\n",
    "            executor.submit(process_query, query, title, knowledge_base): title \n",
    "            for query, title in queries.items()\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_query):\n",
    "            title = future_to_query[future]\n",
    "            try:\n",
    "                result_title, result_response = future.result()\n",
    "                titles.append(result_title)\n",
    "                responses.append((result_title, result_response))\n",
    "            except Exception as e:\n",
    "                print(f\"Query processing failed for title {title}: {e}\")\n",
    "                titles.append(title)\n",
    "                responses.append((title, f\"Error: {e}\"))\n",
    "    \n",
    "    # Aggregate responses\n",
    "    aggregated_responses = aggregate_responses(responses)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Title': [title for title in queries.values()],\n",
    "        'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "    })\n",
    "    \n",
    "    # Save to Excel\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Responses have been saved to '{output_file}'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Modified main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    input_file_path = \"/data/tendergpt/testing/77153810/77153810.txt\"\n",
    "    output_file_path = \"77153810.xlsx\"\n",
    "    \n",
    "    # Labels for segmentation\n",
    "    labels = [\n",
    "        \"Important Date\",\n",
    "        \"Eligibility or Prequalification Criteria\",\n",
    "        \"Technical Requirements\",\n",
    "        \"Contact Details\"\n",
    "    ]\n",
    "    \n",
    "    # Process text and create knowledge base\n",
    "    print(\"Processing text and creating knowledge base...\")\n",
    "    sentences, distances, embeddings, embedder = process_text(input_file_path)\n",
    "    \n",
    "    # Visualize and get chunk breakpoints\n",
    "    indices_above_thresh = visualize_chunks(distances)\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    for index in indices_above_thresh:\n",
    "        group = sentences[start_index:index + 1]\n",
    "        combined_text = ' '.join([d['sentence'] for d in group])\n",
    "        chunks.append(combined_text)\n",
    "        start_index = index + 1\n",
    "    \n",
    "    # Add final chunk\n",
    "    if start_index < len(sentences):\n",
    "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "        chunks.append(combined_text)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Segment and label chunks\n",
    "    labeled_segments = segment_text_with_labels(chunks, labels, embedder)\n",
    "    \n",
    "    # Visualize clusters\n",
    "    visualize_clusters(labeled_segments, labels, embedder)\n",
    "    \n",
    "    # Save labeled segments\n",
    "    segments_file = 'labeled_segments.json'\n",
    "    with open(segments_file, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(labeled_segments, out_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Labeled segments saved to {segments_file}\")\n",
    "    \n",
    "    # Process queries and generate responses\n",
    "    print(\"Processing queries and generating responses...\")\n",
    "    results_df = process_queries(knowledge_base, output_file_path)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Results saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from typing import List\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings.base import Embeddings\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def get_embedding(text: str) -> List[float]:\n",
    "#     response = requests.post(\"http://0.0.0.0:5002/embeddings\",\n",
    "#         json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]})\n",
    "#     if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "#         return data['data'][0]['embedding']\n",
    "#     else:\n",
    "#         raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "# class CustomEmbeddings(Embeddings):\n",
    "#     def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "#         return [get_embedding(text) for text in texts]\n",
    "\n",
    "#     def embed_query(self, text: str) -> List[float]:\n",
    "#         return get_embedding(text)\n",
    "\n",
    "\n",
    "# def process_text(text):\n",
    "#     text = \"\\n\".join([doc.page_content for doc in text])\n",
    "    \n",
    "#     # Semantic-based text splitting\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1024,        # Adjust chunk size as needed\n",
    "#         chunk_overlap=512,      # Overlap for maintaining context\n",
    "#         length_function=len,\n",
    "#         separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]  # Split by paragraphs and sentences\n",
    "#     )\n",
    "    \n",
    "#     chunks = text_splitter.split_text(text)\n",
    "    \n",
    "#     # Create embeddings for the knowledge base\n",
    "#     embeddings = CustomEmbeddings()\n",
    "#     knowledgeBase = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "#     print(knowledgeBase)\n",
    "#     return knowledgeBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##new changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x7efd65f80a90>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "\n",
    "def process_text(texts):\n",
    "    # Join all document texts into a single string to preserve context\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    \n",
    "    # Text splitting based on semantic boundaries to keep context intact\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=4096,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    \n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(combined_text)\n",
    "    \n",
    "    # Create embeddings for the knowledge base\n",
    "    embeddings = CustomEmbeddings()\n",
    "    knowledge_base = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    print(knowledge_base)\n",
    "    return knowledge_base\n",
    "\n",
    "\n",
    "# Specify folder path and load documents\n",
    "# folder_path =f\"/data/tendergpt/livetender_txt/77774640\"\n",
    "folder_path = f\"/data/tendergpt/testing/77153810\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "\n",
    "# Process texts and create knowledge base\n",
    "knowledge_base = process_text(all_docs_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "\n",
    "def process_text(texts):\n",
    "    # Join all document texts into a single string to preserve context\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    \n",
    "    # Text splitting based on semantic boundaries to keep context intact\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=4096,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    \n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(combined_text)\n",
    "    \n",
    "    # Create embeddings for the knowledge base\n",
    "    embeddings = CustomEmbeddings()\n",
    "    knowledge_base = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    return knowledge_base\n",
    "\n",
    "\n",
    "# Specify folder path and load documents\n",
    "# folder_path =f\"/data/tendergpt/livetender_txt/77774640\"\n",
    "folder_path = f\"/data/tendergpt/testing/77153810\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "\n",
    "# Process texts and create knowledge base\n",
    "knowledge_base = process_text(all_docs_text)\n",
    "\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "labels = [\"Important Dates\", \"Eligibility or Prequalification Criteria\", \"Scope of Work\", \"Contact Details\"]\n",
    "outFile = r'G:\\Hetvi\\data_segregation\\PDF&TXT\\ZPPA-PU-ORD-001-14.txt'\n",
    "with open(outFile, 'r', encoding='utf-8') as f:\n",
    "    essay = f.read()\n",
    "\n",
    "single_sentences_list = re.split(r'(?<=[.?!])\\s+', essay)\n",
    "print(f\"{len(single_sentences_list)} sentences were found\")\n",
    "\n",
    "sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]\n",
    "\n",
    "\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    return sentences\n",
    "\n",
    "\n",
    "sentences = combine_sentences(sentences)\n",
    "embeddings = model.encode([x['combined_sentence'] for x in sentences])\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "\n",
    "\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences\n",
    "\n",
    "\n",
    "distances, sentences = calculate_cosine_distances(sentences)\n",
    "\n",
    "y_upper_bound = 0.2\n",
    "\n",
    "breakpoint_percentile_threshold = 95  # 95\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)  # If you want more chunks, lower the percentile cutoff\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]  # The indices of those breakpoints on your list\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "chunks = []\n",
    "for index in indices_above_thresh:\n",
    "    end_index = index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    start_index = index + 1\n",
    "\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "# Semantic Clustering\n",
    "label_embeddings = model.encode(labels)  # Get embeddings for labels\n",
    "\n",
    "# Initialize FAISS index with label embeddings\n",
    "dimension = label_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(label_embeddings)\n",
    "\n",
    "\n",
    "def segment_text_with_faiss_label_assignment(semantic_chunks):\n",
    "    labeled_segments = defaultdict(list)\n",
    "    for chunk in semantic_chunks:\n",
    "        if chunk.strip():  \n",
    "            paragraph_embedding = model.encode(chunk).reshape(1, -1)\n",
    "            _, closest_label_index = faiss_index.search(paragraph_embedding, 1)\n",
    "            closest_label = labels[closest_label_index[0][0]]\n",
    "            labeled_segments[closest_label].append(chunk)\n",
    "    return labeled_segments\n",
    "\n",
    "\n",
    "RELEVANCE_THRESHOLD = -0.7  # Adjust based on experimentation\n",
    "def segment_text_with_faiss_label_assignment(semantic_chunks, threshold=RELEVANCE_THRESHOLD):\n",
    "    labeled_segments = defaultdict(list)\n",
    "    for chunk in semantic_chunks:\n",
    "        if chunk.strip():  \n",
    "            paragraph_embedding = model.encode(chunk).reshape(1, -1)\n",
    "            distances, label_indices = faiss_index.search(paragraph_embedding, len(labels))\n",
    "            similarities = 1 - distances  \n",
    "            assigned_labels = [labels[i] for i, sim in enumerate(similarities[0]) if sim >= threshold]\n",
    "\n",
    "            if assigned_labels:\n",
    "                print(\"Assigned labels : \")\n",
    "                print(assigned_labels)\n",
    "                for label in assigned_labels:\n",
    "                    labeled_segments[label].append(chunk)\n",
    "            else:\n",
    "                labeled_segments[\"Other\"].append(chunk)\n",
    "    \n",
    "    for label in labels:\n",
    "        print(label, len(labeled_segments[label]))\n",
    "    return labeled_segments\n",
    "\n",
    "\n",
    "segmented_result = segment_text_with_faiss_label_assignment(chunks)\n",
    "out_file_path = r'C:\\Users\\hetvi.solanki\\Desktop\\AIProjects\\ragllm\\ragTechniques\\RAG_Techniques\\data\\out.json'\n",
    "with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(segmented_result, out_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/QAAPI/qaVenv2/lib64/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/data/QAAPI/qaVenv2/lib64/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7ff2715b0cc0> >\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Get embeddings from external API\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "\n",
    "# Load text from files\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "\n",
    "# Sentence processing and chunk creation\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Prepare sentence embeddings\n",
    "def process_text(texts):\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', combined_text)\n",
    "    sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]\n",
    "    \n",
    "    sentences = combine_sentences(sentences)\n",
    "    embeddings = model.encode([x['combined_sentence'] for x in sentences])\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Calculate cosine distances between consecutive sentences\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences\n",
    "\n",
    "\n",
    "# FAISS semantic clustering\n",
    "def segment_text_with_faiss_label_assignment(semantic_chunks, threshold=-0.7):\n",
    "    labeled_segments = defaultdict(list)\n",
    "    for chunk in semantic_chunks:\n",
    "        if chunk.strip():\n",
    "            paragraph_embedding = model.encode(chunk).reshape(1, -1)\n",
    "            distances, label_indices = faiss_index.search(paragraph_embedding, len(labels))\n",
    "            similarities = 1 - distances\n",
    "            assigned_labels = [labels[i] for i, sim in enumerate(similarities[0]) if sim >= threshold]\n",
    "\n",
    "            if assigned_labels:\n",
    "                for label in assigned_labels:\n",
    "                    labeled_segments[label].append(chunk)\n",
    "            else:\n",
    "                labeled_segments[\"Other\"].append(chunk)\n",
    "    return labeled_segments\n",
    "\n",
    "\n",
    "# Main processing\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "labels = [\"Important Dates\", \"Eligibility or Prequalification Criteria\", \"Scope of Work\", \"Contact Details\"]\n",
    "\n",
    "folder_path = \"/data/tendergpt/testing/77153810\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "sentences = process_text(all_docs_text)\n",
    "\n",
    "distances, sentences = calculate_cosine_distances(sentences)\n",
    "\n",
    "y_upper_bound = 0.2\n",
    "breakpoint_percentile_threshold = 95\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "\n",
    "start_index = 0\n",
    "chunks = []\n",
    "for index in indices_above_thresh:\n",
    "    end_index = index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    start_index = index + 1\n",
    "\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize FAISS knowledge base\n",
    "knowledge_base = FAISS.from_texts(chunks, embedding=CustomEmbeddings())\n",
    "\n",
    "# FAISS Label Assignment\n",
    "label_embeddings = model.encode(labels)\n",
    "dimension = label_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "print(faiss_index)\n",
    "faiss_index.add(label_embeddings)\n",
    "\n",
    "segmented_result = segment_text_with_faiss_label_assignment(chunks)\n",
    "\n",
    "# Save result to JSON\n",
    "out_file_path = r'/data/QAAPI/stored_files/out.json'\n",
    "with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(segmented_result, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# # FAISS Label Assignment\n",
    "# label_embeddings = model.encode(labels)\n",
    "# dimension = label_embeddings.shape[1]\n",
    "# faiss_index = faiss.IndexFlatL2(dimension)\n",
    "# faiss_index.add(label_embeddings)\n",
    "\n",
    "# segmented_result = segment_text_with_faiss_label_assignment(chunks)\n",
    "\n",
    "# # Save result to JSON\n",
    "# out_file_path = r'/data/QAAPI/stored_files\\out.json'\n",
    "# with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#     json.dump(segmented_result, out_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<faiss.swigfaiss_avx512.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7ff0386e0cc0> >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query processing failed for title PQ: 'IndexFlatL2' object has no attribute 'similarity_search'\n",
      "Responses have been saved to '75927775.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "queries = {\n",
    "\n",
    "       \"Extract all the point of Eligibilty criteria from the tender document\":\"PQ\"    \n",
    "}\n",
    "\n",
    "\n",
    "def process_query(query, title):\n",
    "    # Simulate retrieving documents from the knowledge base\n",
    "    docs = faiss_index.similarity_search(query)\n",
    "    \n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "        # model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Load the QA chain\n",
    "    chain = load_qa_chain(llm,verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Run the chain and capture the response\n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        # Strip unnecessary context or text\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        # Ensure each title entry starts with an empty string if not already in aggregated\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        # Concatenate the response to the existing entry\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists to store results\n",
    "titles = []\n",
    "responses = []\n",
    "\n",
    "# Use ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses for each point\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to '75927775.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load Sentence Transformer model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Labels and FAISS index setup\n",
    "labels = [\"Important Date\", \"Eligibility or Prequalification Criteria\", \"scope of work\", \"Contact Details\"]\n",
    "label_embeddings = model.encode(labels)\n",
    "dimension = label_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(label_embeddings)\n",
    "\n",
    "# Step 1: Load text files from directory\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                all_text.append(file.read())\n",
    "    return all_text\n",
    "\n",
    "# Step 2: Split text into semantic chunks\n",
    "def split_text_into_chunks(texts: List[str], chunk_size=4096, chunk_overlap=200):\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(combined_text)\n",
    "\n",
    "# Step 3: Generate embeddings and label assignment\n",
    "RELEVANCE_THRESHOLD = -0.7\n",
    "\n",
    "def assign_labels_to_chunks(semantic_chunks):\n",
    "    labeled_segments = defaultdict(list)\n",
    "    for chunk in semantic_chunks:\n",
    "        if chunk.strip():\n",
    "            paragraph_embedding = model.encode(chunk).reshape(1, -1)\n",
    "            distances, label_indices = faiss_index.search(paragraph_embedding, len(labels))\n",
    "            similarities = 1 - distances  # Convert distances to cosine similarity\n",
    "            assigned_labels = [labels[i] for i, sim in enumerate(similarities[0]) if sim >= RELEVANCE_THRESHOLD]\n",
    "            \n",
    "            if assigned_labels:\n",
    "                for label in assigned_labels:\n",
    "                    labeled_segments[label].append(chunk)\n",
    "            else:\n",
    "                labeled_segments[\"Other\"].append(chunk)\n",
    "    return labeled_segments\n",
    "\n",
    "# Step 4: Visualize labeled chunks\n",
    "def visualize_clusters(segmented_result):\n",
    "    scatter_points = []\n",
    "    labels_for_plot = []\n",
    "    colors = ['red', 'blue', 'green', 'yellow']\n",
    "\n",
    "    for label_index, label in enumerate(labels):\n",
    "        paragraphs = segmented_result[label]\n",
    "        paragraph_embeddings = model.encode(paragraphs)\n",
    "        scatter_points.append(paragraph_embeddings)\n",
    "        labels_for_plot.extend([label] * len(paragraphs))\n",
    "\n",
    "    all_embeddings = np.vstack(scatter_points)\n",
    "    kmeans = KMeans(n_clusters=len(labels), random_state=42)\n",
    "    kmeans.fit(all_embeddings)\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    reduced_embeddings = PCA(n_components=2).fit_transform(all_embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, label in enumerate(labels):\n",
    "        indices = np.where(cluster_labels == i)[0]\n",
    "        plt.scatter(reduced_embeddings[indices, 0], reduced_embeddings[indices, 1],\n",
    "                    label=label, color=colors[i], alpha=0.7)\n",
    "    plt.title('Text Segmentation Clusters with K-Means')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Step 5: Save results to JSON\n",
    "def save_segmented_results(segmented_result, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(segmented_result, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Main workflow\n",
    "folder_path = \"/data/tendergpt/testing/77153810\"  # Update to your folder path\n",
    "all_text = load_text_files_from_directory(folder_path)\n",
    "chunks = split_text_into_chunks(all_text)\n",
    "segmented_result = assign_labels_to_chunks(chunks)\n",
    "\n",
    "# Save and visualize results\n",
    "output_path = \"segmented_results.json\"\n",
    "save_segmented_results(segmented_result, output_path)\n",
    "visualize_clusters(segmented_result)\n",
    "\n",
    "# Optional: Search for a specific labeled chunk\n",
    "eligibility_criteria_chunk = segmented_result[\"Eligibility or Prequalification Criteria\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses have been saved to 'responses.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define queries\n",
    "queries = {\n",
    "    \"Extract all points related to bidder eligibility criteria, pre-qualification requirements, blacklisting criteria, \"\n",
    "    \"and technical capability for bid submission as mentioned in this tender document. Include specific requirements such as: \"\n",
    "    \"experience, financial criteria, certifications, technical qualifications, equipment or resource requirements, technical \"\n",
    "    \"capability specifications, prior project experience, legal compliance, mandatory documents or certificates, and any conditions \"\n",
    "    \"related to blacklisting or prior performance. Ensure all technical qualifications, minimum standards, and capability-related \"\n",
    "    \"conditions are accurately extracted without including irrelevant information.\": \"PQ\"\n",
    "}\n",
    "\n",
    "# Initialize Sentence Transformer and FAISS index\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Simulate document retrieval for the knowledge base\n",
    "knowledge_base = ...  # Replace with your document retrieval or embedding logic\n",
    "\n",
    "# Function to preprocess text and create embeddings\n",
    "def preprocess_and_embed(text):\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    # Chunk sentences\n",
    "    buffer_size = 2\n",
    "    chunks = []\n",
    "    for i in range(len(sentences)):\n",
    "        chunk = ' '.join(sentences[max(0, i - buffer_size): min(len(sentences), i + buffer_size + 1)])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # Create embeddings\n",
    "    chunk_embeddings = model.encode(chunks)\n",
    "    return chunks, chunk_embeddings\n",
    "\n",
    "# Function to add chunks to FAISS index\n",
    "def add_chunks_to_index(chunks, embeddings):\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        faiss_index.add(embeddings[i].reshape(1, -1))\n",
    "\n",
    "# Function to process a query\n",
    "def process_query(query, title):\n",
    "    try:\n",
    "        # Search similar chunks in FAISS\n",
    "        query_embedding = model.encode([query])\n",
    "        distances, indices = faiss_index.search(query_embedding, k=5)  # Top 5 results\n",
    "\n",
    "        # Retrieve documents based on FAISS results\n",
    "        docs = [knowledge_base[idx] for idx in indices[0] if idx < len(knowledge_base)]  # Replace with actual retrieval logic\n",
    "\n",
    "        # Initialize LLM and QA chain\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",  # Replace with actual key\n",
    "            max_tokens=4096,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "\n",
    "        # Run the chain\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        return title, response.strip()\n",
    "    except Exception as e:\n",
    "        return title, f\"Error processing query: {e}\"\n",
    "\n",
    "# Function to aggregate responses\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        aggregated[title] = aggregated.get(title, \"\") + \"\\n\" + response\n",
    "    return aggregated\n",
    "\n",
    "# Process queries in parallel\n",
    "responses = []\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            responses.append(result)\n",
    "        except Exception as e:\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "# Save results to Excel\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "df.to_excel('responses.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to 'responses.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List, Dict\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.get_embedding(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_embedding(text: str) -> List[float]:\n",
    "        response = requests.post(\n",
    "            \"http://0.0.0.0:5002/embeddings\",\n",
    "            json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data['data'][0]['embedding']\n",
    "        else:\n",
    "            raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class TenderDocumentProcessor:\n",
    "    def __init__(self, folder_path: str):\n",
    "        self.folder_path = folder_path\n",
    "        self.knowledge_base = None\n",
    "        self.queries = {\n",
    "            \"eligibility\": \"\"\"\n",
    "            Extract comprehensive eligibility criteria for bidders including:\n",
    "            1. Basic qualification requirements\n",
    "            2. Financial criteria (turnover, net worth, etc.)\n",
    "            3. Technical qualifications and certifications\n",
    "            4. Past experience requirements\n",
    "            5. Legal compliance requirements\n",
    "            6. Mandatory documents/certificates\n",
    "            7. Blacklisting conditions\n",
    "            8. Technical Capability\n",
    "            Please provide specific details and numbers where mentioned.\n",
    "            \"\"\",\n",
    "            \n",
    "            \"dates\": \"\"\"\n",
    "            Extract all important dates and deadlines including:\n",
    "            1. Tender publication date\n",
    "            2. Pre-bid meeting date and venue\n",
    "            3. Bid submission start and end dates\n",
    "            4. Technical bid opening date\n",
    "            5. Financial bid opening date\n",
    "            6. Project timeline/completion period\n",
    "            List all dates in DD/MM/YYYY format where possible.\n",
    "            \"\"\",\n",
    "            \n",
    "            \"amounts\": \"\"\"\n",
    "            Extract all financial details including:\n",
    "            1. Estimated project cost\n",
    "            2. EMD/Bid security amount\n",
    "            3. Performance security amount\n",
    "            4. Tender fee\n",
    "            5. Minimum turnover requirement\n",
    "            6. Any other significant financial figures\n",
    "            Please specify the currency and provide exact figures.\n",
    "            \"\"\",\n",
    "            \n",
    "            \"scope\": \"\"\"\n",
    "            Extract detailed scope of work including:\n",
    "            1. Project overview and objectives\n",
    "            2. Detailed deliverables\n",
    "            3. Technical specifications\n",
    "            4. Quality requirements\n",
    "            5. Location details\n",
    "            6. Timeline requirements\n",
    "            7. Any specific conditions or constraints\n",
    "            Please provide comprehensive details without omitting critical information.\n",
    "            \"\"\",\n",
    "            \n",
    "            \"contact\": \"\"\"\n",
    "            Extract all contact information including:\n",
    "            1. Tender inviting authority details\n",
    "            2. Contact person name and designation\n",
    "            3. Office address\n",
    "            4. Phone numbers\n",
    "            5. Email addresses\n",
    "            6. Website details\n",
    "            7. Helpdesk information\n",
    "            Please include complete contact details for all mentioned points of contact.\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "    def load_documents(self) -> List[str]:\n",
    "        all_text = []\n",
    "        for filename in os.listdir(self.folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(self.folder_path, filename)\n",
    "                try:\n",
    "                    loader = TextLoader(file_path)\n",
    "                    docs = loader.load()\n",
    "                    all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading file {filename}: {e}\")\n",
    "        return all_text\n",
    "\n",
    "    def process_text(self, texts: List[str]):\n",
    "        combined_text = \"\\n\".join(texts)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=4096,\n",
    "            chunk_overlap=200,  # Increased overlap for better context\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "        )\n",
    "        chunks = text_splitter.split_text(combined_text)\n",
    "        embeddings = CustomEmbeddings()\n",
    "        self.knowledge_base = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "    def process_query(self, query: str, title: str) -> tuple:\n",
    "        try:\n",
    "            docs = self.knowledge_base.similarity_search(query, k=4)  # Increased k for better context\n",
    "            \n",
    "            llm = ChatOpenAI(\n",
    "                model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "                openai_api_base=\"http://localhost:8000/v1\",\n",
    "                openai_api_key=\"FAKE\",\n",
    "                max_tokens=4096,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "            \n",
    "            with get_openai_callback() as cost:\n",
    "                response = chain.run(input_documents=docs, question=query)\n",
    "            \n",
    "            return title, response.strip()\n",
    "        except Exception as e:\n",
    "            return title, f\"Error processing query: {e}\"\n",
    "\n",
    "    def process_tender_document(self, output_filename: str):\n",
    "        # Load and process documents\n",
    "        all_docs_text = self.load_documents()\n",
    "        self.process_text(all_docs_text)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        responses = []\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, title): title \n",
    "                for title, query in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                try:\n",
    "                    title, response = future.result()\n",
    "                    responses.append((title, response))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in processing: {e}\")\n",
    "        \n",
    "        # Create DataFrame and save to Excel\n",
    "        df = pd.DataFrame(responses, columns=['Category', 'Information'])\n",
    "        df.to_excel(output_filename, index=False)\n",
    "        print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/data/tendergpt/testing/77774640\"\n",
    "    processor = TenderDocumentProcessor(folder_path)\n",
    "    processor.process_tender_document(\"tender_analysis.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]})\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "def process_text(text):\n",
    "    text = \"\\n\".join([doc.page_content for doc in text])\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=2048,\n",
    "        chunk_overlap=32,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    embeddings = CustomEmbeddings()\n",
    "    \n",
    "    knowledgeBase = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    return knowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcno  = 77562718 \n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            all_docs.extend(loader.load())\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# folder_path=f\"/data/tendergpt/testing/77302344\"\n",
    "folder_path = f\"/data/tendergpt/livetender_txt/77562718\"\n",
    "all_docs = load_text_files_from_directory(folder_path)\n",
    "knowledge_base = process_text(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define a more open-ended query to fetch all relevant information\n",
    "queries = {\n",
    "           \"\"\" Locate and extract the contact information for the officer associated with this tender. This includes:\n",
    "\n",
    "The officers full name, precisely as listed.\n",
    "The contact phone number in any format.\n",
    "The official email address.\n",
    "Each of these details is already present in the tender document. Search thoroughly across all sections to capture this information accurately. For any detail that cannot be found, return 'None.' \"\"\":\"Contact details\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a  Question Answering assistant. Your primary task is to answer questions based STRICTLY on the provided context. \n",
    "\n",
    "RULES:\n",
    "- ONLY answer if the question relates directly to the provided context.\n",
    "- Do NOT provide information that is not explicitly mentioned in the context. Avoid speculating or adding details from outside the context.\n",
    "- If the question does NOT directly match with the context, respond with  I don't know.\n",
    "- If no context is provided, always respond with I don't know.\n",
    "- Always use more text to elaborate the answer. However, ensure the elaboration is strictly based on the context.\n",
    "\n",
    "Remember: Stick to the context. If uncertain, respond with I don't know.\n",
    "\n",
    "Documents: {context}\n",
    "\n",
    "Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Function to create and run the chain with the given query and documents\n",
    "def process_query(query, title):\n",
    "    # Simulate retrieving documents from the FAISS knowledge base\n",
    "    docs = knowledge_base.similarity_search(query)  # List of document objects\n",
    "    \n",
    "    # Initialize the language model (Llama 3)\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://10.0.0.19:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "     \n",
    "    )\n",
    "    \n",
    "    # Define the template using `PromptTemplate`\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"query\", \"context\"]  # Define input variables\n",
    "    )\n",
    "    \n",
    "    # Create the LLM chain using the prompt and the Llama 3 model\n",
    "    chain = LLMChain(llm=llm, verbose=True,prompt=prompt)\n",
    "    \n",
    "    # Prepare the documents to be passed to the model\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Run the chain and capture the response\n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(query=query, context=context)  # Pass inputs directly\n",
    "        # Strip unnecessary context or text\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "# Aggregate responses function\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = response\n",
    "        else:\n",
    "            aggregated[title] += \"\\n\" + response\n",
    "    return aggregated\n",
    "\n",
    "# Initialize lists to store results\n",
    "titles = []\n",
    "responses = []\n",
    "\n",
    "# Use ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses for each point\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to '77326167.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            # Extract text content from Document objects\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "\n",
    "def process_text(texts):\n",
    "    # Join all document texts into a single string to preserve context\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    \n",
    "    # Text splitting based on semantic boundaries to keep context intact\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2048,\n",
    "        chunk_overlap=32,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    \n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(combined_text)\n",
    "    \n",
    "    # Create embeddings for the knowledge base\n",
    "    embeddings = CustomEmbeddings()\n",
    "    knowledge_base = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    return knowledge_base\n",
    "\n",
    "\n",
    "# Specify folder path and load documents\n",
    "folder_path =f\"/data/tendergpt/livetender_txt/71484890\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "\n",
    "# Process texts and create knowledge base\n",
    "knowledge_base = process_text(all_docs_text)\n",
    "\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "queries = {\n",
    "\n",
    "    \"Extract the contact details of the officer from this document, including their name, email ID, and contact number. Search thoroughly across relevant sections, such as 'Contact Information,' 'Officer Details,' 'Authorized Contact,' or similar headings. If any detail is not found, return 'None' for that field.\": \"Contact Details\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_query(query, title):\n",
    "    # Simulate retrieving documents from the knowledge base\n",
    "    docs = knowledge_base.similarity_search(query)\n",
    "    \n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Load the QA chain\n",
    "    chain = load_qa_chain(llm,verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Run the chain and capture the response\n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        # Strip unnecessary context or text\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        # Ensure each title entry starts with an empty string if not already in aggregated\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        # Concatenate the response to the existing entry\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists to store results\n",
    "titles = []\n",
    "responses = []\n",
    "\n",
    "# Use ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses for each point\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to '75927775.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "queries = {\n",
    "\n",
    "    \"Extract the contact details of the officer from this document, including their name, email ID, and contact number. Search thoroughly across relevant sections, such as 'Contact Information,' 'Officer Details,' 'Authorized Contact,' or similar headings. If any detail is not found, return 'None' for that field.\": \"Contact Details\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_query(query, title):\n",
    "    # Simulate retrieving documents from the knowledge base\n",
    "    docs = knowledge_base.similarity_search(query)\n",
    "    \n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Load the QA chain\n",
    "    chain = load_qa_chain(llm,verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Run the chain and capture the response\n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        # Strip unnecessary context or text\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        # Ensure each title entry starts with an empty string if not already in aggregated\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        # Concatenate the response to the existing entry\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists to store results\n",
    "titles = []\n",
    "responses = []\n",
    "\n",
    "# Use ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses for each point\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to '75927775.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Custom embedding class for connecting to external embedding API\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "# Load text files from directory\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "# Process text into hierarchical knowledge base\n",
    "def process_text(texts):\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=32, separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "    chunks = text_splitter.split_text(combined_text)\n",
    "    \n",
    "    embeddings = CustomEmbeddings()\n",
    "    knowledge_base = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    return knowledge_base\n",
    "\n",
    "\n",
    "queries = {\n",
    "    \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "    \"Clauses specifying  Pre-Qualification Criteria  or eligibility criteria\": \"Prequalification Criteria\",\n",
    "    \"List all mandatory qualification criteria, including Blacklisting and required certifications\": \"Mandatory Qualification Criteria\",\n",
    "     \"Performance criteria including work experience,experience and past performance criteria, emphasizing the need for prior similar project experience, references, and the successful completion of similar contracts\": \"Performance Criteria\",\n",
    "    \"Financial criteria including turnover, Networth\": \"Financial Criteria\",\n",
    "    \"Technical requirements\": \"Technical Requirements\",\n",
    "     \"Work Specifications that bidders must meet to deliver tender requirements\": \"Specifications\",\n",
    "     \"Supporting documents\": \"Supporting Documents\",\n",
    "    #  \"List of all the dates mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD date\":\"Importants Date\",\n",
    "    \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document. This includes but is not limited to the following fields: bid submission end date, tender due date, bid validity, opening date, closing date, pre-bid meeting date, EMD date, tender value, and tender fee. Group all extracted items under the label 'Important Dates and Amounts,' clearly specifying each date, time, or amount and its description as stated in the document.\":\"Important date\",\n",
    "    # \"Extract all key important dates, times, and amounts mentioned in this document. Do not extract any irrelevant information.\":\"Importants Date\",\n",
    "    \"Extract the contact details, including phone number, email address, and officer name. If the details are unavailable, return 'None' for the missing fields.\":\"Contact details\"\n",
    "  \n",
    "       \n",
    "}\n",
    "\n",
    "def hierarchical_retrieve_and_process(query, title):\n",
    "    # First-level retrieval to get relevant chunks\n",
    "    initial_docs = knowledge_base.similarity_search(query, top_k=10)\n",
    "    \n",
    "    # Concatenate initial docs for second-level retrieval\n",
    "    initial_text = \"\\n\".join(doc.page_content for doc in initial_docs)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=32)\n",
    "    fine_grained_chunks = text_splitter.split_text(initial_text)\n",
    "    \n",
    "    # Create sub-knowledge base for second-level retrieval\n",
    "    sub_knowledge_base = FAISS.from_texts(fine_grained_chunks, embedding=CustomEmbeddings())\n",
    "    final_docs = sub_knowledge_base.similarity_search(query, top_k=5)\n",
    "    \n",
    "    # Initialize LLM and QA chain for answering\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Generate response with hierarchical context\n",
    "    with get_openai_callback() as cost:\n",
    "        response = chain.run(input_documents=final_docs, question=query)\n",
    "    \n",
    "    return title, response.strip()\n",
    "\n",
    "# Run queries in parallel\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "# Execute hierarchical RAG and save to Excel\n",
    "# folder_path = f\"/data/tendergpt/livetender_txt/76542577\"\n",
    "folder_path = \"/data/QAAPI/doc111_txt/74512478\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "knowledge_base = process_text(all_docs_text)\n",
    "\n",
    "titles, responses = [], []\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(hierarchical_retrieve_and_process, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses and save to Excel\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "df = pd.DataFrame({'Title': [title for title in queries.values()],\n",
    "                   'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]})\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to '77326167.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Custom embedding with topic-specific handling\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str], topic: str = None) -> List[List[float]]:\n",
    "        modified_texts = [f\"{topic}: {text}\" for text in texts] if topic else texts\n",
    "        return [get_embedding(text) for text in modified_texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "# Load text files from directory\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "# Process text with hierarchical RAG and topic weighting\n",
    "def process_text(texts, important_topics):\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=32, separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "    chunks = text_splitter.split_text(combined_text)\n",
    "    \n",
    "    # Apply weighted embeddings for key topics\n",
    "    embeddings = CustomEmbeddings()\n",
    "    weighted_chunks = []\n",
    "    for topic in important_topics:\n",
    "        weighted_chunks += [f\"{topic}: {chunk}\" for chunk in chunks]\n",
    "    \n",
    "    knowledge_base = FAISS.from_texts(weighted_chunks, embedding=embeddings)\n",
    "    return knowledge_base\n",
    "\n",
    "# Define queries with higher emphasis for key topics\n",
    "queries = {\n",
    "    \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "    \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "    \"Provide a detailed list of all dates, times, and monetary values in the document, including bid submission end date, opening date, pre-bid meeting date, and other specified dates. Label this list 'Important Dates.'\": \"Important Dates\",\n",
    "    \"Extract contact details of the officer, including name, phone number, and email ID. For any details not available, return 'None' for missing fields.\": \"Contact Details\"\n",
    "}\n",
    "\n",
    "important_topics = [\"eligibility criteria\", \"scope of work\", \"important dates\", \"contact details\"]\n",
    "\n",
    "def hierarchical_retrieve_and_process(query, title):\n",
    "    # Initial retrieval step with topic emphasis\n",
    "    initial_docs = knowledge_base.similarity_search(f\"{title}: {query}\", top_k=10)\n",
    "    initial_text = \"\\n\".join(doc.page_content for doc in initial_docs)\n",
    "    \n",
    "    # Second-level retrieval with focused chunks for high-importance topics\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32)\n",
    "    fine_grained_chunks = text_splitter.split_text(initial_text)\n",
    "    \n",
    "    sub_knowledge_base = FAISS.from_texts(fine_grained_chunks, embedding=CustomEmbeddings())\n",
    "    final_docs = sub_knowledge_base.similarity_search(query, top_k=5)\n",
    "    \n",
    "    # Initialize LLM and QA chain\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Run QA chain and retrieve response\n",
    "    with get_openai_callback() as cost:\n",
    "        response = chain.run(input_documents=final_docs, question=query)\n",
    "    \n",
    "    return title, response.strip()\n",
    "\n",
    "# Aggregate responses\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "# Execution\n",
    "folder_path = \"/data/tendergpt/testing/77326167\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "knowledge_base = process_text(all_docs_text, important_topics)\n",
    "\n",
    "titles, responses = [], []\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(hierarchical_retrieve_and_process, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses and save to Excel\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "df = pd.DataFrame({'Title': [title for title in queries.values()],\n",
    "                   'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]})\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to '77326167.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# Custom embeddings setup\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "# Text loading and processing\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "def process_text(texts):\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,  # Smaller chunks to retain topic accuracy\n",
    "        chunk_overlap=128,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(combined_text)\n",
    "    embeddings = CustomEmbeddings()\n",
    "    knowledge_base = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    return knowledge_base\n",
    "\n",
    "# Specify folder path and load documents\n",
    "folder_path = \"/data/QAAPI/doc111_txt/74512478\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "knowledge_base = process_text(all_docs_text)\n",
    "\n",
    "# Priority queries with detailed and specific prompts\n",
    "queries = {\n",
    "    \"Detailed eligibility criteria for this tender, including pre-qualification conditions and any specific requirements\": \"Eligibility Criteria\",\n",
    "    \"Full scope of work and functional requirements specified for bidders\": \"Scope of Work\",\n",
    "    \"All important dates and times, including bid submission end date, opening date, bid validity, and pre-bid meeting date\": \"Important Dates\",\n",
    "    \"Contact details of the officer, including officers name, email ID, and phone number\": \"Contact Details\"\n",
    "}\n",
    "\n",
    "# Process query with hierarchical response generation\n",
    "def process_query(query, title):\n",
    "    docs = knowledge_base.similarity_search(query, k=5)  # Retrieve more relevant documents\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",\n",
    "        max_tokens=2048,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    chain = load_qa_chain(llm, chain_type=\"map_reduce\", verbose=True)  # Use map_reduce for hierarchical processing\n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    return title, response\n",
    "\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "# Retrieve and process all queries\n",
    "titles = []\n",
    "responses = []\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate and store responses\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses saved to '77326167.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m all_docs_text \u001b[38;5;241m=\u001b[39m load_text_files_from_directory(folder_path)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Process texts to create knowledge base with weighted embeddings\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m knowledge_base \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_docs_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Define queries\u001b[39;00m\n\u001b[1;32m     84\u001b[0m queries \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentify the functional requirements, also referred to as the scope of work, specified in the document.\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScope of Work\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract clauses that specify Pre-Qualification Criteria or eligibility criteria.\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrequalification Criteria\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract the contact details of the officer, including name, email, and phone number, from the document.\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContact Details\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m }\n",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m, in \u001b[0;36mprocess_text\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     70\u001b[0m     weighted_embedding \u001b[38;5;241m=\u001b[39m embedding \u001b[38;5;241m*\u001b[39m weight\n\u001b[1;32m     71\u001b[0m     weighted_embeddings\u001b[38;5;241m.\u001b[39mappend(weighted_embedding\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 73\u001b[0m knowledge_base \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweighted_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m knowledge_base\n",
      "File \u001b[0;32m/data/QAAPI/qaVenv2/lib64/python3.11/site-packages/langchain_community/vectorstores/faiss.py:1117\u001b[0m, in \u001b[0;36mFAISS.from_embeddings\u001b[0;34m(cls, text_embeddings, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_embeddings\u001b[39m(\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1096\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     texts, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtext_embeddings)\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;28mlist\u001b[39m(texts),\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;28mlist\u001b[39m(embeddings),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1125\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import requests\n",
    "\n",
    "# Define Custom Embeddings to get embeddings from external API\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "# Function to assign weights based on chunk content\n",
    "def calculate_chunk_weight(chunk: str) -> float:\n",
    "    keywords = [\"scope of work\", \"eligibility\", \"performance criteria\", \"important dates\", \"contact details\"]\n",
    "    weight = 1.0  # Default weight\n",
    "    if any(keyword in chunk.lower() for keyword in keywords):\n",
    "        weight = 1.5  # Higher weight for specific keywords\n",
    "    return weight\n",
    "\n",
    "# Function to load text files from a directory\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_text = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text.extend(doc.page_content for doc in docs if hasattr(doc, 'page_content'))\n",
    "    return all_text\n",
    "\n",
    "# Process text and create weighted embeddings for the knowledge base\n",
    "def process_text(texts):\n",
    "    combined_text = \"\\n\".join(texts)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2048,\n",
    "        chunk_overlap=32,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(combined_text)\n",
    "    \n",
    "    chunk_weights = [calculate_chunk_weight(chunk) for chunk in chunks]\n",
    "    embeddings = CustomEmbeddings()\n",
    "    weighted_embeddings = []\n",
    "    \n",
    "    for chunk, weight in zip(chunks, chunk_weights):\n",
    "        embedding = np.array(embeddings.embed_documents([chunk])[0])\n",
    "        weighted_embedding = embedding * weight\n",
    "        weighted_embeddings.append(weighted_embedding.tolist())\n",
    "    \n",
    "    knowledge_base = FAISS.from_embeddings(weighted_embeddings, embedding=embeddings)\n",
    "    return knowledge_base\n",
    "\n",
    "# Initialize folder path and load documents\n",
    "folder_path = \"/data/QAAPI/doc111_txt/74512478\"\n",
    "all_docs_text = load_text_files_from_directory(folder_path)\n",
    "\n",
    "# Process texts to create knowledge base with weighted embeddings\n",
    "knowledge_base = process_text(all_docs_text)\n",
    "\n",
    "# Define queries\n",
    "queries = {\n",
    "    \"Identify the functional requirements, also referred to as the scope of work, specified in the document.\": \"Scope of Work\",\n",
    "    \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "    \"List all mandatory qualification criteria, including blacklisting status and required certifications.\": \"Mandatory Qualification Criteria\",\n",
    "    \"Detail performance criteria, such as work experience, experience, and past performance requirements, focusing on similar project experience, references, and successful completion of similar contracts.\": \"Performance Criteria\",\n",
    "    \"Provide the financial criteria outlined in the document, including turnover and net worth requirements.\": \"Financial Criteria\",\n",
    "    \"Outline the technical requirements mentioned in the document.\": \"Technical Requirements\",\n",
    "    \"Summarize the work specifications that bidders must meet to fulfill the tender requirements.\": \"Specifications\",\n",
    "    \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "    \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "    \"Extract the contact details of the officer, including name, email, and phone number, from the document.\": \"Contact Details\"\n",
    "}\n",
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# (Assume all imports and other function definitions above this point are correct)\n",
    "\n",
    "# Adjusted process_query function\n",
    "def process_query(query, title):\n",
    "    docs = knowledge_base.similarity_search(query)\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "    \n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response  # Ensure both title and response are returned as a tuple\n",
    "\n",
    "# Parallel query processing with debugging\n",
    "responses = []\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(f\"Debug: Future result = {result}\")  # Debug print to check the result structure\n",
    "            if isinstance(result, tuple) and len(result) == 2:\n",
    "                responses.append(result)\n",
    "            else:\n",
    "                print(f\"Unexpected result format for title {title}: {result}\")\n",
    "                responses.append((title, \"Unexpected result format\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses and save to Excel\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "# Aggregate and write to DataFrame\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "print(\"Responses have been saved to '77326167.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize the language model (Llama 3)\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://10.0.0.19:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain =RetrievalQA.from_chain_type(\n",
    "llm,\n",
    "retriever =knowledge_base.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain =RetrievalQA.from_chain_type(\n",
    "llm,chain_type=\"stuff\",return_source_documents=True,\n",
    "retriever =knowledge_base.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"extract the entire pre qualification criteria mentioned in this document\"\n",
    "response =qa_chain.invoke({\"query\":query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'extract the entire pre qualification criteria mentioned in this document',\n",
       " 'result': \"Here is the pre-qualification criteria mentioned in the document:\\n\\n1. Individual Bidders:\\n\\t* 1.1 Constitution or legal status of Bidder (attach copy)\\n\\t* 1.2 Total annual volume of civil engineering construction work executed and payments received in the last five years preceding the year in which bids are invited (attach certificate from Chartered Accountant)\\n\\t* 1.5 Qualifications of technical personnel proposed for the Contract (refer also to Clause 4.2(e) of the Instructions to Bidders and Clause 9.1 of Part-1 General Conditions of Contract)\\n\\t* 1.6 Financial reports for the last five years: balance sheets, profit and loss statements, auditors' reports, etc. (list below and attach copies)\\n\\t* 1.7 Evidence of access to financial resources to meet the qualification requirements: cash in hand, lines of credit, etc. (list below and attach copies of support documents)\\n1.8 Name, address, and telephone, telex, and facsimile numbers of banks that may provide references if contacted by the Employer.\\n1.9 Information on current litigation in which the Bidder is involved:\\n\\t* Name of Other party(s)\\n\\t* Cause of dispute\\n\\t* Litigation where\\n\\t* Amount involved (Court/arbitration)\\n\\nNote that the document also mentions that even though the bidders meet the above qualifying criteria, they are subject to be disqualified if they have made misleading or false representations in the forms, statements, affidavits and attachments submitted in proof of the qualification requirements.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     return_source_documents=True,\n",
    "#     retriever=knowledge_base.as_retriever()\n",
    "# )\n",
    "\n",
    "# query = (\n",
    "#     \"Locate the 'Pre-Qualification Criteria' section in this document and extract each criterion listed. \"\n",
    "#     \"If the criteria are in a bulleted or numbered format, extract each item separately. Include any criteria \"\n",
    "#     \"under alternative section titles, such as 'Qualification Requirements' or 'Eligibility Criteria'.\"\n",
    "# )\n",
    "# ponse = qa_chain.invoke({\"query\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"Please extract all details from the 'Pre-Qualification Criteria' section of this document. Include each criterion in a list or structured format, and ensure that no points are omitted.\",\n",
       " 'result': 'Here are the pre-qualification criteria extracted from the document:\\n\\n**Pre-Qualification Criteria**\\n\\n**Individual Bidders**\\n\\n1. **Constitution or legal status of Bidder**\\n\\t* Attach copy\\n\\t* Place of registration: ________________________\\n\\t* Principal place of business: ________________________\\n\\t* Power of attorney of signatory ______________________ of Bid [Attach]\\n2. **Total annual volume of civil engineering construction work executed and payments received in the last five years preceding the year in which bids are invited**\\n\\t* (Attach certificate from Chartered Accountant)\\n\\t* Rs. In lakhs: ______________________________________\\n3. **Evidence of access to financial resources to meet the qualification requirements**\\n\\t* Cash in hand, lines of credit, etc.\\n\\t* List below and attach copies of support documents\\n\\t* (Sample format attached)\\n4. **Name, address, and telephone, telex, and facsimile numbers of banks that may provide references if contacted by the Employer**\\n5. **Information on current litigation in which the Bidder is involved**\\n\\t* Name of Other party(s): ________________________\\n\\t* Cause of dispute: ________________________\\n\\t* Litigation where: ________________________\\n\\t* Amount involved: ________________________\\n\\n**Common Criteria for All Bidders**\\n\\n1. **Copies of original documents defining the constitution or legal status, place of registration, and principal place of business**\\n2. **Written power of attorney of the signatory of the Bid to commit the Bidder**\\n3. **Total monetary value of civil construction works performed for each of the last five years**\\n4. **Experience in works of a similar nature and size for each of the last five years, and details of works in progress or contractually committed with certificates from the concerned officer of the rank of Executive Engineer or equivalent**\\n5. **Evidence of ownership of major items of construction equipment named in Clause 4.4 B (b) (i) of ITB or evidence of arrangement of possessing them on hire/lease/buying as defined therein**\\n\\nNote: These criteria are applicable for bids costing more than Rs. 10.00 Lacs.',\n",
       " 'source_documents': [Document(page_content='Qualification Information\\n( Applicable for bids costing more than Rs. 10.00 Lacs)\\n( Following informations shall be furnished by the contractor on a non-judicial stamp paper\\nof Rs. 100/- only.)\\nNotes on Form of Qualification Information\\nThe information to be filled in by bidders in the following pages will be used for purposes of post-\\nqualification as provided for in Clause 4 of the Instructions to Bidders. This information will not be\\nincorporated in the Contract. Attach additional pages as necessary.\\n1. Individual Bidders\\n1.1 Constitution or legal status of [attach copy]\\nBidder\\nPlace of registration:\\n______________________\\nPrincipal place of business:\\nPower of attorney of signatory ______________________\\nof Bid\\n[Attach]\\n1.2 Total annual volume of (Rs. In lakhs)\\ncivil engineering\\nconstruction work executed\\nand payments received in\\nthe last five years preceding\\nthe year in which bids are\\ninvited. (Attach certificate\\nfrom Chartered\\nAccountant)'),\n",
       "  Document(page_content='________________________________________________________________________________________________________\\n__________________________________________________\\n1.7 Evidence of access to financial resources to meet the qualification requirements: cash in hand, lines of credit, etc. List below\\nand attach copies of support documents. (Sample format attached).\\n________________________________________________________________________________________________________\\n________________________________________________________________________________________________________\\n__________________________________________________\\n1.8 Name, address, and telephone, telex, and facsimile numbers of banks that may provide references if contacted by\\nthe Employer.\\n1.9 Information on current litigation in which the Bidder is involved.\\nName of Other party(s) Cause of dispute Litigation where Amount involved\\n(Court/arbitration)'),\n",
       "  Document(page_content='3.2 All bidders shall include the following information and documents with their bids in Section 3, Qualification\\nInformation unless otherwise stated in the Appendix to ITB:\\n(a) copies of original documents defining the constitution or legal status, place of registration, and principal place of business;\\nwritten power of attorney of the signatory of the Bid to commit the Bidder;\\n(b) total monetary value of civil construction works performed for each of the last five years;\\n(c) experience in works of a similar nature and size for each of the last five years, and details of works in progress or\\ncontractually committed with certificates from the concerned officer of the rank of Executive Engineer or equivalent;\\n(d) evidence of ownership of major items of construction equipment named in Clause 4.4 B (b) (i) of ITB or evidence of\\narrangement of possessing them on hire/lease/buying as defined therein.'),\n",
       "  Document(page_content='N = Number of years prescribed for completion of the works for which bids are invited (period up to 6 months to be taken\\nas half-year and more than 6 months as one year).\\nM = M is taken 2.5\\nB = Value, at the current price level, of existing commitments and on-going works to be completed during the period of\\ncompletion of the works for which bids are invited.\\nNote: The statements showing the value of existing commitments and on-going works as well as the stipulated period of\\ncompletion remaining for each of the works listed should be countersigned by the Engineer in charge, not below the\\nrank of an Executive Engineer or equivalent.\\n3.7 Even though the bidders meet the above qualifying criteria, they are subject to be disqualified if they have:\\n(i) Made misleading or false representations in the forms, statements, affidavits and attachments submitted in proof of\\nthe qualification requirements; and/or')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define a more open-ended query to fetch all relevant information\n",
    "queries = {\n",
    "           \"\"\" Locate and extract the contact information for the officer associated with this tender. This includes:\n",
    "\n",
    "The officers full name, precisely as listed.\n",
    "The contact phone number in any format.\n",
    "The official email address.\n",
    "Each of these details is already present in the tender document. Search thoroughly across all sections to capture this information accurately. For any detail that cannot be found, return 'None.' \"\"\":\"Contact details\"       \n",
    "                                        \n",
    "                                        \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a  Question Answering assistant. Your primary task is to answer questions based STRICTLY on the provided context. \n",
    "\n",
    "RULES:\n",
    "- ONLY answer if the question relates directly to the provided context.\n",
    "- Do NOT provide information that is not explicitly mentioned in the context. Avoid speculating or adding details from outside the context.\n",
    "- If the question does NOT directly match with the context, respond with  I don't know.\n",
    "- If no context is provided, always respond with I don't know.\n",
    "- Always use more text to elaborate the answer. However, ensure the elaboration is strictly based on the context.\n",
    "\n",
    "Remember: Stick to the context. If uncertain, respond with I don't know.\n",
    "\n",
    "Documents: {context}\n",
    "\n",
    "Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Function to create and run the chain with the given query and documents\n",
    "def process_query(query, title):\n",
    "    # Simulate retrieving documents from the FAISS knowledge base\n",
    "    docs = knowledge_base.similarity_search(query)  # List of document objects\n",
    "    \n",
    "    # Initialize the language model (Llama 3)\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://10.0.0.19:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "     \n",
    "    )\n",
    "    \n",
    "    # Define the template using `PromptTemplate`\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"query\", \"context\"]  # Define input variables\n",
    "    )\n",
    "    \n",
    "    # Create the LLM chain using the prompt and the Llama 3 model\n",
    "    chain = LLMChain(llm=llm, verbose=True,prompt=prompt)\n",
    "    \n",
    "    # Prepare the documents to be passed to the model\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Run the chain and capture the response\n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(query=query, context=context)  # Pass inputs directly\n",
    "        # Strip unnecessary context or text\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "# Aggregate responses function\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = response\n",
    "        else:\n",
    "            aggregated[title] += \"\\n\" + response\n",
    "    return aggregated\n",
    "\n",
    "# Initialize lists to store results\n",
    "titles = []\n",
    "responses = []\n",
    "\n",
    "# Use ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses for each point\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to '77326167.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# # Define queries with more context for accuracy\n",
    "# queries = {\n",
    "#     \"Identify the functional requirements, also referred to as the scope of work, specified in the document.\": \"Scope of Work\",\n",
    "#     \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "#     \"List all mandatory qualification criteria, including blacklisting status and required certifications.\": \"Mandatory Qualification Criteria\",\n",
    "#     \"Detail performance criteria, such as work experience, experience, and past performance requirements, focusing on similar project experience, references, and successful completion of similar contracts.\": \"Performance Criteria\",\n",
    "#     \"Provide the financial criteria outlined in the document, including turnover and net worth requirements.\": \"Financial Criteria\",\n",
    "#     \"Outline the technical requirements mentioned in the document.\": \"Technical Requirements\",\n",
    "#     \"Summarize the work specifications that bidders must meet to fulfill the tender requirements.\": \"Specifications\",\n",
    "#     \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "#     \"Identify and extract all dates mentioned throughout this tender document, regardless of context. List each date along with any associated description or event if provided (e.g., Bid submission end date, Opening date, Bid validity, pre-bid meeting date, etc.). Label this list as 'All Dates in Document.'\": \"Important Dates\",\n",
    "#     \"Extract the contact details of the officer from this document, including their name, email ID, and contact number. Search thoroughly across relevant sections, such as 'Contact Information,' 'Officer Details,' 'Authorized Contact,' or similar headings. If any detail is not found, return 'None' for that field.\": \"Contact Details\"\n",
    "# }\n",
    "\n",
    "\n",
    "# # Retrieve main context or summary from the document\n",
    "# main_context = \" \".join(doc.page_content for doc in knowledge_base.similarity_search(\"tender document summary\"))\n",
    "\n",
    "\n",
    "# def process_query(query, title, main_context):\n",
    "#     # Incorporate main context into each query for clarity\n",
    "#     query_with_context = f\"{main_context}\\n\\n{query}\"\n",
    "    \n",
    "#     # Initialize the language model\n",
    "#     llm = ChatOpenAI(\n",
    "#         model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#         openai_api_base=\"http://localhost:8000/v1\",\n",
    "#         openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "#         max_tokens=4096,\n",
    "#         temperature=0.1\n",
    "#     )\n",
    "    \n",
    "#     # Load the QA chain\n",
    "#     chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "    \n",
    "#     # Run the chain and capture the response\n",
    "#     try:\n",
    "#         with get_openai_callback() as cost:\n",
    "#             response = chain.run(input_documents=knowledge_base.similarity_search(query_with_context), question=query)\n",
    "#         # Strip unnecessary context or text\n",
    "#         response = response.strip()\n",
    "#     except Exception as e:\n",
    "#         response = f\"Error processing query: {e}\"\n",
    "    \n",
    "#     return title, response\n",
    "\n",
    "# def aggregate_responses(responses):\n",
    "#     aggregated = {}\n",
    "#     for title, response in responses:\n",
    "#         if title not in aggregated:\n",
    "#             aggregated[title] = \"\"\n",
    "#         aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "#     return aggregated\n",
    "\n",
    "# # Initialize lists to store results\n",
    "# titles = []\n",
    "# responses = []\n",
    "\n",
    "# # Use ThreadPoolExecutor to run queries in parallel\n",
    "# with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "#     future_to_query = {executor.submit(process_query, query, title, main_context): title for query, title in queries.items()}\n",
    "    \n",
    "#     for future in as_completed(future_to_query):\n",
    "#         title = future_to_query[future]\n",
    "#         try:\n",
    "#             result_title, result_response = future.result()\n",
    "#             titles.append(result_title)\n",
    "#             responses.append((result_title, result_response))\n",
    "#         except Exception as e:\n",
    "#             print(f\"Query processing failed for title {title}: {e}\")\n",
    "#             titles.append(title)\n",
    "#             responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# # Aggregate responses for each point\n",
    "# aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'Title': [title for title in queries.values()],\n",
    "#     'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "# })\n",
    "\n",
    "# # Save the DataFrame to an Excel file\n",
    "# df.to_excel('responses.xlsx', index=False)\n",
    "\n",
    "# print(\"Responses have been saved to 'responses.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "queries = {\n",
    "    \"Identify the functional requirements, also referred to as the scope of work, specified in the document.\": \"Scope of Work\",\n",
    "    \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "    \"List all mandatory qualification criteria, including blacklisting status and required certifications.\": \"Mandatory Qualification Criteria\",,\n",
    "    \"Summarize the work specifications that bidders must meet to fulfill the tender requirements.\": \"Specifications\",\n",
    "    \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "    \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document. This includes but is not limited to the following fields: bid submission end date, tender due date, bid validity, opening date, closing date, pre-bid meeting date, EMD date, tender value, and tender fee. Group all extracted items under the label 'Important Dates and Amounts,' clearly specifying each date, time, or amount and its description as stated in the document.\":\"Important date\",\n",
    "    \"Extract the contact details of the officer from this document, including their name, email ID, and contact number. Search thoroughly across relevant sections, such as 'Contact Information,' 'Officer Details,' 'Authorized Contact,' or similar headings. If any detail is not found, return 'None' for that field.\": \"Contact Details\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def process_query(query, title):\n",
    "    # Simulate retrieving documents from the knowledge base\n",
    "    docs = knowledge_base.similarity_search(query)\n",
    "    \n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Load the QA chain\n",
    "    chain = load_qa_chain(llm,verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Run the chain and capture the response\n",
    "    try:\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        # Strip unnecessary context or text\n",
    "        response = response.strip()\n",
    "    except Exception as e:\n",
    "        response = f\"Error processing query: {e}\"\n",
    "    \n",
    "    return title, response\n",
    "\n",
    "def aggregate_responses(responses):\n",
    "    aggregated = {}\n",
    "    for title, response in responses:\n",
    "        # Ensure each title entry starts with an empty string if not already in aggregated\n",
    "        if title not in aggregated:\n",
    "            aggregated[title] = \"\"\n",
    "        # Concatenate the response to the existing entry\n",
    "        aggregated[title] += \"\\n\" + response if response else \"\"\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists to store results\n",
    "titles = []\n",
    "responses = []\n",
    "\n",
    "# Use ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(queries)) as executor:\n",
    "    future_to_query = {executor.submit(process_query, query, title): title for query, title in queries.items()}\n",
    "    \n",
    "    for future in as_completed(future_to_query):\n",
    "        title = future_to_query[future]\n",
    "        try:\n",
    "            result_title, result_response = future.result()\n",
    "            titles.append(result_title)\n",
    "            responses.append((result_title, result_response))\n",
    "        except Exception as e:\n",
    "            print(f\"Query processing failed for title {title}: {e}\")\n",
    "            titles.append(title)\n",
    "            responses.append((title, f\"Error: {e}\"))\n",
    "\n",
    "# Aggregate responses for each point\n",
    "aggregated_responses = aggregate_responses(responses)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': [title for title in queries.values()],\n",
    "    'Response': [aggregated_responses.get(title, 'No response') for title in queries.values()]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('77326167.xlsx', index=False)\n",
    "\n",
    "print(\"Responses have been saved to '75927775.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import shutil\n",
    "\n",
    "def remove_all_files_in_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.remove(file_path)  # Remove the file\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove the directory and its contents\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        print(f\"All files and subfolders have been removed from {folder_path}.\")\n",
    "    else:\n",
    "        print(f\"The folder {folder_path} does not exist.\")\n",
    "\n",
    "def extract_images_and_text_from_pdf(pdf_path, output_folder):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    filename = os.path.basename(pdf_path).split('.')[0]\n",
    "    output_folder = os.path.join(output_folder, filename)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Variable to store extracted text\n",
    "    pdf_text = \"\"\n",
    "\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "\n",
    "        # Extract text from the page\n",
    "        page_text = page.get_text(\"text\")\n",
    "        pdf_text += page_text  # Append text from each page\n",
    "\n",
    "        images = page.get_images(full=True)\n",
    "        for image_index, image in enumerate(images):\n",
    "            xref = image[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_filename = f\"{filename}_page_{page_number + 1}_image_{image_index + 1}.{image_ext}\"\n",
    "            image_filepath = os.path.join(output_folder, image_filename)\n",
    "            with open(image_filepath, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "            print(f\"Saved image {image_filename}\")\n",
    "    \n",
    "    print(f\"Extraction complete. Images saved to {output_folder}\")\n",
    "    \n",
    "    return pdf_text  # Return the extracted text\n",
    "\n",
    "def process_pdfs_in_folder(input_folder_path, image_folder_path):\n",
    "    all_pdf_text = \"\"  # Variable to store text from all PDFs\n",
    "\n",
    "    for filename in os.listdir(input_folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(input_folder_path, filename)\n",
    "            # Extract images and text from each PDF\n",
    "            pdf_text = extract_images_and_text_from_pdf(file_path, image_folder_path)\n",
    "            all_pdf_text += pdf_text  # Append text from each PDF\n",
    "\n",
    "    # Return the combined text from all PDFs\n",
    "    return all_pdf_text\n",
    "\n",
    "# Example usage\n",
    "\n",
    "input_folder_path = r'/data/QAAPI/extract_hetvi11'  # Folder where PDFs are stored\n",
    "image_folder_path = r'/data/QAAPI/doc111F'  # Folder to save images\n",
    "\n",
    "# Process PDFs and get the combined text\n",
    "combined_pdf_text = process_pdfs_in_folder(input_folder_path, image_folder_path)\n",
    "\n",
    "# Now `combined_pdf_text` contains the extracted text from all PDFs in the folder\n",
    "print(combined_pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image Certificate of Incorporation _page_1_image_1.jpeg\n",
      "Saved image Certificate of Incorporation _page_2_image_1.jpeg\n",
      "Extraction complete. Images saved to /data/QAAPI/doc111/Certificate of Incorporation \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import shutil\n",
    "\n",
    "def remove_all_files_in_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.remove(file_path)  # Remove the file\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove the directory and its contents\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        print(f\"All files and subfolders have been removed from {folder_path}.\")\n",
    "    else:\n",
    "        print(f\"The folder {folder_path} does not exist.\")\n",
    "\n",
    "def extract_images_and_text_from_pdf(pdf_path, output_folder):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    filename = os.path.basename(pdf_path).split('.')[0]\n",
    "    output_folder = os.path.join(output_folder, filename)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Variable to store extracted text\n",
    "    pdf_text = \"\"\n",
    "\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "\n",
    "        # Extract text from the page\n",
    "        page_text = page.get_text(\"text\")\n",
    "        pdf_text += page_text  # Append text from each page\n",
    "\n",
    "        images = page.get_images(full=True)\n",
    "        for image_index, image in enumerate(images):\n",
    "            xref = image[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_filename = f\"{filename}_page_{page_number + 1}_image_{image_index + 1}.{image_ext}\"\n",
    "            image_filepath = os.path.join(output_folder, image_filename)\n",
    "            with open(image_filepath, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "            print(f\"Saved image {image_filename}\")\n",
    "    \n",
    "    print(f\"Extraction complete. Images saved to {output_folder}\")\n",
    "    \n",
    "    return pdf_text  # Return the extracted text\n",
    "\n",
    "# Function to process a single PDF file by passing its path\n",
    "def process_single_pdf(pdf_file_path, image_folder_path):\n",
    "    # Extract images and text from the single PDF\n",
    "    pdf_text = extract_images_and_text_from_pdf(pdf_file_path, image_folder_path)\n",
    "\n",
    "    # Return the extracted text from the PDF\n",
    "    return pdf_text\n",
    "\n",
    "# Example usage\n",
    "pdf_file_path = r'/data/QAAPI/extract_hetvi11/Certificate of Incorporation .pdf'  # Path to a specific PDF\n",
    "image_folder_path = r'/data/QAAPI/doc111'  # Folder to save images\n",
    "\n",
    "# Process the specific PDF and get the extracted text\n",
    "extracted_pdf_text = process_single_pdf(pdf_file_path, image_folder_path)\n",
    "\n",
    "# Now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from apiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "import io\n",
    "import concurrent.futures\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from tabulate import tabulate\n",
    "from multiprocessing import Pool\n",
    "import xlrd\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "import string\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from apiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "import io\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def googleOcr(file_path):\n",
    "    try:\n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            r\"/data/imageExtraction/GoogleAPICred/projectoct-436907-a6e51afb9d49.json\",\n",
    "            scopes=['https://www.googleapis.com/auth/drive']\n",
    "        )\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "        \n",
    "        # Handle PDF files by converting them to images first\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            images = convert_from_path(file_path)\n",
    "            extracted_text = \"\"\n",
    "            for i, img in enumerate(images):\n",
    "                img_path = f\"page_{i}.png\"\n",
    "                print(\"image:::::\",img_path)\n",
    "                img.save(img_path, 'PNG')  # Save each page as an image\n",
    "                page_text = googleOcr(img_path)  # Perform OCR on each page\n",
    "                extracted_text += (page_text or \"\") + \"\\n\"  # Handle None return case\n",
    "            return extracted_text.strip()\n",
    "\n",
    "        mime = 'application/vnd.google-apps.document'\n",
    "        res = service.files().create(\n",
    "            body={'name': os.path.basename(file_path), 'mimeType': mime},\n",
    "            media_body=MediaFileUpload(file_path, mimetype=mime, resumable=True)\n",
    "        ).execute()\n",
    "\n",
    "        text_output = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(text_output, service.files().export_media(fileId=res['id'], mimeType=\"text/plain\"))\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "\n",
    "        text_output.seek(0)\n",
    "        extracted_text = text_output.read().decode('utf-8')\n",
    "        service.files().delete(fileId=res['id']).execute()\n",
    "        \n",
    "        return extracted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error in OCR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    # Walk through the folder and process each file\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Only process supported file types\n",
    "            if file.lower().endswith(('.pdf', '.png', '.jpg', '.jpeg')):\n",
    "                logger.info(f\"Processing file: {file_path}\")\n",
    "                file_text = googleOcr(file_path)\n",
    "                extracted_text += f\"Text from {file}:\\n\" + (file_text or \"No text found\") + \"\\n\\n\"\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = r\"/data/QAAPI/extract_hetvi/11111113\"\n",
    "result = process_folder(folder_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "# # Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Step 1: Load the Data\n",
    "file_path = '/data/QAAPI/PQ.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Step 2: Encode the Labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_tag'])\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['pq'], data['label_encoded'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
    "val_dataset = TextDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n",
    "\n",
    "# Step 5: Model Initialization\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "# Step 6: Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Step 7: Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Step 8: Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save the Model and Label Encoder\n",
    "model.save_pretrained(\"./model\")\n",
    "tokenizer.save_pretrained(\"./model\")\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    import pickle\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Step 10: Inference Function\n",
    "def predict_label(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return label_encoder.inverse_transform([pred_label])[0]\n",
    "\n",
    "# Example Usage\n",
    "example_text = \"The bidder must have a valid CMMI Level 3 certification.\"\n",
    "predicted_label = predict_label(example_text)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 24s 970ms/step - loss: 2.6443 - accuracy: 0.0488 - val_loss: 2.5499 - val_accuracy: 0.0909\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 3s 540ms/step - loss: 2.5190 - accuracy: 0.1951 - val_loss: 2.5033 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 3s 517ms/step - loss: 2.3459 - accuracy: 0.3415 - val_loss: 2.4329 - val_accuracy: 0.1818\n",
      "Predicted Label: Certificates \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Load the Data\n",
    "file_path = '/data/QAAPI/PQ.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Step 2: Encode the Labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label_tag'])\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['pq'], data['label_encoded'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode_texts(texts, tokenizer, max_len=128):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"tf\",\n",
    "    )\n",
    "\n",
    "train_encodings = encode_texts(train_texts, tokenizer)\n",
    "val_encodings = encode_texts(val_texts, tokenizer)\n",
    "\n",
    "# Step 5: Prepare the Data for TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"input_ids\": train_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    },\n",
    "    train_labels\n",
    ")).batch(8)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"input_ids\": val_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "    },\n",
    "    val_labels\n",
    ")).batch(8)\n",
    "\n",
    "# Step 6: Model Initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "# Step 7: Compile the Model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Step 8: Train the Model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Step 9: Save the Model and Label Encoder\n",
    "model.save_pretrained(\"./model\")\n",
    "tokenizer.save_pretrained(\"./model\")\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    import pickle\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Step 10: Inference Function\n",
    "def predict_label(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"tf\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    outputs = model(inputs)\n",
    "    pred_label = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "    return label_encoder.inverse_transform([pred_label])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Client Ref. Letter \n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "example_text = \"\"\"\n",
    "\n",
    "The software being given to MMTC should have been used for processing a total no. of at least 500 e-Tenders and 200 e-Auction during the last three (3) financial years ending on 31.3.2021.\n",
    "\"\"\"\n",
    "predicted_label = predict_label(example_text)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('/data/QAAPI/PQ.xlsx')\n",
    "\n",
    "# Prepare data\n",
    "X = df['pq']\n",
    "y = df['label_tag']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create pipeline with TF-IDF and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', LinearSVC())  # You can replace with MultinomialNB()\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Function for new predictions\n",
    "def predict_label(text):\n",
    "    return pipeline.predict([text])[0]\n",
    "\n",
    "# Example usage\n",
    "new_text = \"\"\"\n",
    "\n",
    "Bidders should have successfully managed and executed e-Auctioning of immovable properties of and and building of worth Rs. 300.00 crores or more in last three (3) financial years. (2018-19, 2019-20, 2020-21)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(f\"Predicted Label: {predict_label(new_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_excel('/data/QAAPI/PQ.xlsx')\n",
    "df = df.dropna(subset=['pq', 'label_tag'])  # Remove rows with missing values\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "df['processed_text'] = df['pq'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['encoded_label'] = le.fit_transform(df['label_tag'])\n",
    "\n",
    "# Split data\n",
    "X = df['processed_text']\n",
    "y = df['encoded_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Advanced pipeline with hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english', \n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        max_features=5000\n",
    "    )),\n",
    "    ('classifier', LinearSVC())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [3000, 5000, 7000],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__max_iter': [5000]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Detailed evaluation\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Prediction function\n",
    "def predict_label(text):\n",
    "    processed_text = preprocess_text(text)\n",
    "    pred_encoded = best_model.predict([processed_text])[0]\n",
    "    return le.inverse_transform([pred_encoded])[0]\n",
    "\n",
    "# Example\n",
    "test_texts = [\n",
    "    \"E-procurement solution for government tender\",\n",
    "    \"Annual financial statements for the company\",\n",
    "    \"ISO certification details\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"Text: {text}\\nPredicted Label: {predict_label(text)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
