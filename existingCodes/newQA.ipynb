{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.chains import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example of labels you want to use\n",
    "labels = [\"Important Dates\", \"Eligibility or Prequalification Criteria\", \"Scope of Work\", \"Contact Details\"]\n",
    "\n",
    "outFile = r'/data/tendergpt/testing/77774640/77774640.txt'\n",
    "with open(outFile, 'r', encoding='utf-8') as f:\n",
    "    essay = f.read()\n",
    "\n",
    "# Split the essay into sentences\n",
    "single_sentences_list = re.split(r'(?<=[.?!])\\s+', essay)\n",
    "\n",
    "# Create sentence objects with index\n",
    "sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]\n",
    "\n",
    "# Function to combine sentences into chunks\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    return sentences\n",
    "\n",
    "# Combine sentences and create embeddings\n",
    "sentences = combine_sentences(sentences)\n",
    "print(\"sentences:::\",sentences)\n",
    "embeddings = model.encode([x['combined_sentence'] for x in sentences])\n",
    "\n",
    "# Add embeddings to sentences\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "\n",
    "# Function to calculate cosine distances between sentence embeddings\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        similarity = np.dot(embedding_current, embedding_next) / (np.linalg.norm(embedding_current) * np.linalg.norm(embedding_next))\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences\n",
    "\n",
    "# Calculate distances between sentences\n",
    "distances, sentences = calculate_cosine_distances(sentences)\n",
    "\n",
    "# Define a threshold for splitting chunks\n",
    "breakpoint_percentile_threshold = 95\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "\n",
    "# Split the document into chunks based on distance threshold\n",
    "start_index = 0\n",
    "chunks = []\n",
    "for index in indices_above_thresh:\n",
    "    end_index = index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    start_index = index + 1\n",
    "\n",
    "if start_index < len(sentences):    \n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "# Function to chunk large text into smaller parts based on token count\n",
    "def chunk_text(text, max_tokens=3500):\n",
    "    max_chars = max_tokens * 2  # Approximate token-to-character conversion\n",
    "    chunks = [text[i:i + max_chars] for i in range(0, len(text), max_chars)]\n",
    "    return chunks\n",
    "\n",
    "# Combine the document chunks into a single large text if needed\n",
    "large_text = \" \".join(chunks)\n",
    "print(\"large_text:::\",large_text)\n",
    "# Split large text into smaller chunks that fit within the token limit\n",
    "chunked_texts = chunk_text(large_text)\n",
    "print(\"chunked_texts:::\",chunked_texts)\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    openai_api_base=\"http://localhost:8000/v1\",\n",
    "    openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "    max_tokens=500,  # Adjust this based on your use case\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Load QA chain\n",
    "chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "\n",
    "# Function to process chunks and run LLM\n",
    "def paramExtractionUsingLLM(chunks, label=\"\"):\n",
    "    print(\" -- inside llm -- \")\n",
    "    all_results = []\n",
    "    max_chars = 23520  # Token size limit per chunk\n",
    "    finalText = \"\"\n",
    "    for c in chunks:\n",
    "        finalText += c\n",
    "    print(\"Total length of chunk: \", len(finalText))\n",
    "\n",
    "    if len(finalText) > max_chars:\n",
    "        split_texts = [finalText[i:i + max_chars] for i in range(0, len(finalText), max_chars)]\n",
    "    else:\n",
    "        split_texts = None\n",
    "\n",
    "    results = []\n",
    "    if split_texts:\n",
    "        print(\" -- inside if -- \")\n",
    "        for text in split_texts:\n",
    "            print(\"-- in for loop --\")\n",
    "            f\"\"\"Extract only the relevant details for the label '{label}' from the below text.\n",
    "Do not include any irrelevant information or summaries. Provide a direct response to the question.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Output (only relevant details): \n",
    "\"\"\"\n",
    "            try:\n",
    "                with get_openai_callback() as cost:\n",
    "                    response = chain.run(input_documents=[Document(page_content=text)], question=label)\n",
    "                results.append(response.strip())\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "                results.append(\"Error processing chunk\")\n",
    "        \n",
    "        combined_result = \" \".join(results)\n",
    "        all_results.append(combined_result)\n",
    "    else:\n",
    "        print(\" -- inside else -- \")\n",
    "        f\"\"\"Extract only the relevant details for the label '{label}' from the below text.\n",
    "Do not include any irrelevant information or summaries. Provide a direct response to the question.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Output (only relevant details): \n",
    "\"\"\"\n",
    "        try:\n",
    "            with get_openai_callback() as cost:\n",
    "                response = chain.run(input_documents=[Document(page_content=finalText)], question=label)\n",
    "            results.append(response.strip())\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            results.append(\"Error processing chunk\")\n",
    "    \n",
    "        combined_result = \" \".join(results)\n",
    "        all_results.append(combined_result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Main function to process all labels\n",
    "def main(label):\n",
    "    eligibility_criteria_chunk = chunks  # Adjust according to the chunking\n",
    "    eligibility_results = paramExtractionUsingLLM(eligibility_criteria_chunk, label=label)\n",
    "    print(\" -- LLM Response -- \")\n",
    "    print(eligibility_results)\n",
    "    return eligibility_results\n",
    "\n",
    "# Create a dictionary to hold the results for all labels\n",
    "labeled_ans = defaultdict(list)\n",
    "for label in labels:\n",
    "    result = main(label)\n",
    "    labeled_ans[label].append(result)\n",
    "\n",
    "# Save LLM output\n",
    "ll_response_out_file_path = r'/data/QAAPI/stored_files/lllm_out.json'\n",
    "with open(ll_response_out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(labeled_ans, out_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subfolders: 1935875\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_subfolders(main_folder_path):\n",
    "    return len([f for f in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, f))])\n",
    "\n",
    "# Example usage\n",
    "main_folder =\"/data/tendergpt/livetender_txt\"\n",
    "# main_folder = \"/data/dailydocument\"\n",
    "# main_folder = \"/data/tendergpt/livetender_txt\"\n",
    "# main_folder= \"/data/tendergpt/03_txt_testing\"\n",
    "# main_folder = \"/data/txtfolder/dailydocument_29-11-24_txt\"\n",
    "# main_folder = \"/data/unzipdocument/dailydocument_02-12-24\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of subfolders:\", count_subfolders(main_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Langchain imports\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('/data/tendergpt/processing_log.txt'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class TenderDocumentProcessor:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize the Tender Document Processor\n",
    "        \n",
    "        :param config: Configuration dictionary with processing parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize models\n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(config['embedding_model'])\n",
    "            self.llm = ChatOpenAI(\n",
    "                model_name=config['llm_model'],\n",
    "                openai_api_base=config['api_base'],\n",
    "                openai_api_key=config['api_key'],\n",
    "                max_tokens=config['max_tokens'],\n",
    "                temperature=config['temperature']\n",
    "            )\n",
    "            self.qa_chain = load_qa_chain(self.llm, chain_type='stuff', verbose=False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model initialization error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_document(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Load document content safely\n",
    "        \n",
    "        :param file_path: Path to the input file\n",
    "        :return: Document text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Preprocess text into sentences\n",
    "        \n",
    "        :param text: Input document text\n",
    "        :return: List of cleaned sentences\n",
    "        \"\"\"\n",
    "        # Use more robust sentence splitting\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        return [sent.strip() for sent in sentences if sent.strip()]\n",
    "\n",
    "    def generate_embeddings(self, sentences: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate sentence embeddings\n",
    "        \n",
    "        :param sentences: List of sentences\n",
    "        :return: Numpy array of embeddings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.embedding_model.encode(sentences, show_progress_bar=False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Embedding generation error: {e}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def calculate_sentence_distances(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate pairwise cosine distances between sentence embeddings\n",
    "        \n",
    "        :param embeddings: Numpy array of sentence embeddings\n",
    "        :return: Array of pairwise distances\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Compute cosine distances\n",
    "            distances = cosine_distances(embeddings)\n",
    "            \n",
    "            # Extract distances between consecutive sentences\n",
    "            # Add padding to handle edge cases\n",
    "            pairwise_distances = np.diagonal(distances, offset=1)\n",
    "            pairwise_distances = np.pad(\n",
    "                pairwise_distances, \n",
    "                (0, 1), \n",
    "                mode='constant', \n",
    "                constant_values=0\n",
    "            )\n",
    "            \n",
    "            return pairwise_distances\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Distance calculation error: {e}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def chunk_document(\n",
    "        self, \n",
    "        sentences: List[str], \n",
    "        distances: np.ndarray, \n",
    "        threshold_percentile: float = 95\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Chunk document based on semantic distances\n",
    "        \n",
    "        :param sentences: List of sentences\n",
    "        :param distances: Pairwise sentence distances\n",
    "        :param threshold_percentile: Percentile for chunk breaking\n",
    "        :return: List of document chunks\n",
    "        \"\"\"\n",
    "        # Handle edge cases\n",
    "        if not sentences or len(distances) == 0:\n",
    "            return [' '.join(sentences)]\n",
    "\n",
    "        # Calculate distance threshold\n",
    "        try:\n",
    "            distance_threshold = np.percentile(distances, threshold_percentile)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Percentile calculation error, using default: {e}\")\n",
    "            distance_threshold = np.max(distances) * 0.8\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "\n",
    "        for sentence, distance in zip(sentences, distances):\n",
    "            current_chunk.append(sentence)\n",
    "            \n",
    "            # Break chunk if distance exceeds threshold\n",
    "            if distance > distance_threshold:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = []\n",
    "\n",
    "        # Add remaining sentences\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def extract_information(self, chunk: str, label: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract specific information using LLM\n",
    "        \n",
    "        :param chunk: Text chunk to process\n",
    "        :param label: Information label to extract\n",
    "        :return: Extracted information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract only the relevant details for the label '{label}' from the text.\n",
    "Do not include irrelevant information or summaries. Provide a direct, concise response.\n",
    "\n",
    "Text: {chunk}\n",
    "\n",
    "Output (only relevant details):\"\"\"\n",
    "            \n",
    "            # Use QA chain for extraction\n",
    "            response = self.qa_chain.run(\n",
    "                input_documents=[Document(page_content=chunk)], \n",
    "                question=prompt\n",
    "            )\n",
    "            \n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Information extraction error for {label}: {e}\")\n",
    "            return \"Could not extract information\"\n",
    "\n",
    "    def process_document(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Main document processing method\n",
    "        \n",
    "        :return: Dictionary of extracted information by label\n",
    "        \"\"\"\n",
    "        # Load document\n",
    "        document_text = self.load_document(self.config['input_file'])\n",
    "        if not document_text:\n",
    "            logger.error(\"Empty document. Processing cannot continue.\")\n",
    "            return {}\n",
    "\n",
    "        # Preprocess sentences\n",
    "        sentences = self.preprocess_text(document_text)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings(sentences)\n",
    "        if len(embeddings) == 0:\n",
    "            logger.error(\"Embedding generation failed.\")\n",
    "            return {}\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = self.calculate_sentence_distances(embeddings)\n",
    "        \n",
    "        # Chunk document\n",
    "        chunks = self.chunk_document(\n",
    "            sentences, \n",
    "            distances, \n",
    "            self.config['distance_percentile']\n",
    "        )\n",
    "        \n",
    "        # Extract information for each label\n",
    "        labeled_results = {}\n",
    "        for label in self.config['labels']:\n",
    "            results = [\n",
    "                self.extract_information(chunk, label) \n",
    "                for chunk in chunks\n",
    "            ]\n",
    "            # Filter out empty results\n",
    "            labeled_results[label] = [r for r in results if r]\n",
    "\n",
    "        return labeled_results\n",
    "\n",
    "    def save_results(self, results: Dict[str, List[str]]):\n",
    "        \"\"\"\n",
    "        Save processing results to a JSON file\n",
    "        \n",
    "        :param results: Extracted information dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.config['output_file'], 'w', encoding='utf-8') as out_file:\n",
    "                json.dump(results, out_file, indent=4, ensure_ascii=False)\n",
    "            logger.info(f\"Results saved to {self.config['output_file']}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving results: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Centralized configuration\n",
    "    CONFIG = {\n",
    "        'embedding_model': 'all-MiniLM-L6-v2',\n",
    "        'llm_model': 'meta-llama/Llama-3.1-8B-Instruct',\n",
    "        'api_base': 'http://localhost:8000/v1',\n",
    "        'api_key': 'FAKE',\n",
    "        'max_tokens': 500,\n",
    "        'temperature': 0.1,\n",
    "        'labels': [\n",
    "            \"Important Dates\", \n",
    "            \"Eligibility or Prequalification Criteria\", \n",
    "            \"Scope of Work\", \n",
    "            \"Contact Details\"\n",
    "        ],\n",
    "        'input_file': '/data/tendergpt/testing/77408673/77408673.txt',\n",
    "        'output_file': '/data/QAAPI/stored_files/lllm_out.json',\n",
    "        'distance_percentile': 95\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Initialize and process document\n",
    "        processor = TenderDocumentProcessor(CONFIG)\n",
    "        results = processor.process_document()\n",
    "        \n",
    "        # Save results\n",
    "        processor.save_results(results)\n",
    "        \n",
    "        logger.info(\"Document processing completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error in document processing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMCHAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.chains import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example of labels you want to use\n",
    "labels = [\"Important Dates\", \"Eligibility or Prequalification Criteria\", \"Scope of Work\", \"Contact Details\"]\n",
    "\n",
    "outFile = r'/data/tendergpt/testing/77408673/77408673.txt'\n",
    "with open(outFile, 'r', encoding='utf-8') as f:\n",
    "    essay = f.read()\n",
    "\n",
    "# Split the essay into sentences\n",
    "single_sentences_list = re.split(r'(?<=[.?!])\\s+', essay)\n",
    "\n",
    "# Create sentence objects with index\n",
    "sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]\n",
    "\n",
    "# Function to combine sentences into chunks\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    return sentences\n",
    "\n",
    "# Combine sentences and create embeddings\n",
    "sentences = combine_sentences(sentences)\n",
    "embeddings = model.encode([x['combined_sentence'] for x in sentences])\n",
    "\n",
    "# Add embeddings to sentences\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "\n",
    "# Function to calculate cosine distances between sentence embeddings\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        similarity = np.dot(embedding_current, embedding_next) / (np.linalg.norm(embedding_current) * np.linalg.norm(embedding_next))\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences\n",
    "\n",
    "# Calculate distances between sentences\n",
    "distances, sentences = calculate_cosine_distances(sentences)\n",
    "\n",
    "# Define a threshold for splitting chunks\n",
    "breakpoint_percentile_threshold = 95\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "\n",
    "# Split the document into chunks based on distance threshold\n",
    "start_index = 0\n",
    "chunks = []\n",
    "for index in indices_above_thresh:\n",
    "    end_index = index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    start_index = index + 1\n",
    "\n",
    "if start_index < len(sentences):    \n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "# Function to chunk large text into smaller parts based on token count\n",
    "def chunk_text(text, max_tokens=3500):\n",
    "    max_chars = max_tokens * 2  # Approximate token-to-character conversion\n",
    "    chunks = [text[i:i + max_chars] for i in range(0, len(text), max_chars)]\n",
    "    return chunks\n",
    "\n",
    "# Combine the document chunks into a single large text if needed\n",
    "large_text = \" \".join(chunks)\n",
    "\n",
    "# Split large text into smaller chunks that fit within the token limit\n",
    "chunked_texts = chunk_text(large_text)\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    openai_api_base=\"http://localhost:8000/v1\",\n",
    "    openai_api_key=\"FAKE\",  # Replace with your actual key if needed\n",
    "    max_tokens=500,  # Adjust this based on your use case\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Define a prompt template for the chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\", \"label\"],\n",
    "    template=(\n",
    "        \"Extract only the relevant details for the label '{label}' from the text below.\\n\"\n",
    "        \"Do not include any irrelevant information or summaries. Provide a direct response to the question.\\n\\n\"\n",
    "        \"Text: {text}\\n\\n\"\n",
    "        \"Output (only relevant details):\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the LLM chain using the prompt and the Llama 3 model\n",
    "chain = LLMChain(llm=llm, verbose=True, prompt=prompt_template)\n",
    "\n",
    "# Function to process chunks and run LLM using LLMChain\n",
    "def paramExtractionUsingLLM(chunks, label=\"\"):\n",
    "    print(\" -- inside llm -- \")\n",
    "    all_results = []\n",
    "    max_chars = 23520  # Token size limit per chunk\n",
    "\n",
    "    final_text = \" \".join(chunks)\n",
    "    print(\"Total length of chunk: \", len(final_text))\n",
    "\n",
    "    if len(final_text) > max_chars:\n",
    "        split_texts = [final_text[i:i + max_chars] for i in range(0, len(final_text), max_chars)]\n",
    "    else:\n",
    "        split_texts = [final_text]\n",
    "\n",
    "    results = []\n",
    "    for text in split_texts:\n",
    "        try:\n",
    "            response = chain.run(text=text, label=label)\n",
    "            results.append(response.strip())\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            results.append(\"Error processing chunk\")\n",
    "        \n",
    "    combined_result = \" \".join(results)\n",
    "    all_results.append(combined_result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Main function to process all labels\n",
    "def main(label):\n",
    "    eligibility_criteria_chunk = chunks  # Adjust according to the chunking\n",
    "    eligibility_results = paramExtractionUsingLLM(eligibility_criteria_chunk, label=label)\n",
    "    print(\" -- LLM Response -- \")\n",
    "    print(eligibility_results)\n",
    "    return eligibility_results\n",
    "\n",
    "# Create a dictionary to hold the results for all labels\n",
    "labeled_ans = defaultdict(list)\n",
    "for label in labels:\n",
    "    result = main(label)\n",
    "    labeled_ans[label].append(result)\n",
    "\n",
    "# Save LLM output\n",
    "ll_response_out_file_path = r'/data/QAAPI/stored_files/lllm2_out.json'\n",
    "with open(ll_response_out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(labeled_ans, out_file, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set dynamic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Embedding and ML libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# LangChain libraries\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', \n",
    "                 breakpoint_percentile=95, \n",
    "                 buffer_size=1):\n",
    "        \"\"\"\n",
    "        Initialize Semantic Chunker with embedding model and chunking parameters\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Sentence transformer model name\n",
    "            breakpoint_percentile (int): Percentile for distance threshold\n",
    "            buffer_size (int): Number of surrounding sentences to include in context\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.breakpoint_percentile = breakpoint_percentile\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text by splitting into sentences\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "        \n",
    "        Returns:\n",
    "            list: List of sentences with indices\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "        return [{'sentence': x, 'index': i} for i, x in enumerate(sentences)]\n",
    "\n",
    "    def combine_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Combine sentences with surrounding context\n",
    "        \n",
    "        Args:\n",
    "            sentences (list): List of sentence dictionaries\n",
    "        \n",
    "        Returns:\n",
    "            list: Enhanced sentence dictionaries with combined context\n",
    "        \"\"\"\n",
    "        for i in range(len(sentences)):\n",
    "            combined_sentence = ''\n",
    "            \n",
    "            # Add preceding context\n",
    "            for j in range(max(0, i - self.buffer_size), i):\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "            \n",
    "            # Add current sentence\n",
    "            combined_sentence += sentences[i]['sentence']\n",
    "            \n",
    "            # Add following context\n",
    "            for j in range(i + 1, min(len(sentences), i + 1 + self.buffer_size)):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "            \n",
    "            sentences[i]['combined_sentence'] = combined_sentence\n",
    "        return sentences\n",
    "\n",
    "    def calculate_cosine_distances(self, sentences):\n",
    "        \"\"\"\n",
    "        Calculate cosine distances between sentence embeddings\n",
    "        \n",
    "        Args:\n",
    "            sentences (list): List of sentence dictionaries\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Distances and updated sentences\n",
    "        \"\"\"\n",
    "        # Encode combined sentences\n",
    "        embeddings = self.model.encode([x['combined_sentence'] for x in sentences])\n",
    "        \n",
    "        # Add embeddings to sentences\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "        \n",
    "        # Calculate inter-sentence distances\n",
    "        distances = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "            embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "            \n",
    "            similarity = np.dot(embedding_current, embedding_next) / (\n",
    "                np.linalg.norm(embedding_current) * np.linalg.norm(embedding_next)\n",
    "            )\n",
    "            distance = 1 - similarity\n",
    "            distances.append(distance)\n",
    "            sentences[i]['distance_to_next'] = distance\n",
    "        \n",
    "        return distances, sentences\n",
    "\n",
    "    def chunk_text(self, text, max_tokens=3500):\n",
    "        \"\"\"\n",
    "        Dynamic chunking based on semantic distances\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            max_tokens (int): Maximum tokens per chunk\n",
    "        \n",
    "        Returns:\n",
    "            list: Semantic chunks\n",
    "        \"\"\"\n",
    "        # Preprocess and combine sentences\n",
    "        sentences = self.preprocess_text(text)\n",
    "        sentences = self.combine_sentences(sentences)\n",
    "        \n",
    "        # Calculate cosine distances\n",
    "        distances, sentences = self.calculate_cosine_distances(sentences)\n",
    "        \n",
    "        # Determine breakpoint threshold\n",
    "        breakpoint_distance_threshold = np.percentile(distances, self.breakpoint_percentile)\n",
    "        \n",
    "        # Split document into chunks\n",
    "        chunks = []\n",
    "        start_index = 0\n",
    "        indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "        \n",
    "        for index in indices_above_thresh:\n",
    "            end_index = index\n",
    "            group = sentences[start_index:end_index + 1]\n",
    "            combined_text = ' '.join([d['sentence'] for d in group])\n",
    "            chunks.append(combined_text)\n",
    "            start_index = index + 1\n",
    "        \n",
    "        # Add remaining text\n",
    "        if start_index < len(sentences):    \n",
    "            combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "            chunks.append(combined_text)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "def paramExtractionUsingLLM(chunks, label, llm, chain):\n",
    "    \"\"\"\n",
    "    Extract parameters from text chunks using Language Model\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): Text chunks\n",
    "        label (str): Label for extraction\n",
    "        llm: Language model\n",
    "        chain: QA chain\n",
    "    \n",
    "    Returns:\n",
    "        list: Extracted results\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    max_chars = 23520  # Token size limit per chunk\n",
    "\n",
    "    finalText = \" \".join(chunks)\n",
    "    \n",
    "    if len(finalText) > max_chars:\n",
    "        split_texts = [finalText[i:i + max_chars] for i in range(0, len(finalText), max_chars)]\n",
    "    else:\n",
    "        split_texts = [finalText]\n",
    "\n",
    "    results = []\n",
    "    for text in split_texts:\n",
    "        prompt = f\"\"\"Extract only the relevant details for the label '{label}' \n",
    "        from the text. Do not include irrelevant information or summaries.\n",
    "\n",
    "        Text: {text}\n",
    "\n",
    "        Output (relevant details): \"\"\"\n",
    "        \n",
    "        try:\n",
    "            with get_openai_callback() as cost:\n",
    "                response = chain.run(input_documents=[Document(page_content=text)], question=prompt)\n",
    "            results.append(response.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk for {label}: {e}\")\n",
    "            results.append(\"Error processing chunk\")\n",
    "    \n",
    "    return [\" \".join(results)]\n",
    "\n",
    "def main(file_path, labels):\n",
    "    \"\"\"\n",
    "    Main processing function\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to input text file\n",
    "        labels (list): Labels for extraction\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted labeled information\n",
    "    \"\"\"\n",
    "    # Read input text\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        essay = f.read()\n",
    "    \n",
    "    # Initialize semantic chunker\n",
    "    chunker = SemanticChunker()\n",
    "    chunks = chunker.chunk_text(essay)\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Load QA chain\n",
    "    chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Process labels\n",
    "    labeled_ans = defaultdict(list)\n",
    "    for label in labels:\n",
    "        result = paramExtractionUsingLLM(chunks, label, llm, chain)\n",
    "        labeled_ans[label].append(result)\n",
    "    \n",
    "    return labeled_ans\n",
    "\n",
    "# Example usage\n",
    "labels = [\"Important Dates\", \"Eligibility Criteria\", \"Scope of Work\", \"Contact Details\"]\n",
    "input_file = '/data/tendergpt/testing/77774640/77774640.txt'\n",
    "output_file = '/data/QAAPI/stored_files/llm_out.json'\n",
    "\n",
    "# Run extraction\n",
    "results = main(input_file, labels)\n",
    "\n",
    "# Save results\n",
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(results, out_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error with extract max_model_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Embedding and ML libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# LangChain libraries\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', \n",
    "                 breakpoint_percentile=95, \n",
    "                 buffer_size=1,\n",
    "                 max_tokens=7000):\n",
    "        \"\"\"\n",
    "        Initialize Semantic Chunker with embedding model and chunking parameters\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Sentence transformer model name\n",
    "            breakpoint_percentile (int): Percentile for distance threshold\n",
    "            buffer_size (int): Number of surrounding sentences to include in context\n",
    "            max_tokens (int): Maximum tokens per chunk\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.breakpoint_percentile = breakpoint_percentile\n",
    "        self.buffer_size = buffer_size\n",
    "        self.max_tokens = max_tokens\n",
    "        self.token_splitter = TokenTextSplitter(chunk_size=max_tokens, chunk_overlap=200)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text by splitting into sentences\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "        \n",
    "        Returns:\n",
    "            list: List of sentences with indices\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "        return [{'sentence': x, 'index': i} for i, x in enumerate(sentences) if x.strip()]\n",
    "\n",
    "    def combine_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Combine sentences with surrounding context\n",
    "        \n",
    "        Args:\n",
    "            sentences (list): List of sentence dictionaries\n",
    "        \n",
    "        Returns:\n",
    "            list: Enhanced sentence dictionaries with combined context\n",
    "        \"\"\"\n",
    "        for i in range(len(sentences)):\n",
    "            combined_sentence = ''\n",
    "            \n",
    "            # Add preceding context\n",
    "            for j in range(max(0, i - self.buffer_size), i):\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "            \n",
    "            # Add current sentence\n",
    "            combined_sentence += sentences[i]['sentence']\n",
    "            \n",
    "            # Add following context\n",
    "            for j in range(i + 1, min(len(sentences), i + 1 + self.buffer_size)):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "            \n",
    "            sentences[i]['combined_sentence'] = combined_sentence\n",
    "        return sentences\n",
    "\n",
    "    def calculate_cosine_distances(self, sentences):\n",
    "        \"\"\"\n",
    "        Calculate cosine distances between sentence embeddings\n",
    "        \n",
    "        Args:\n",
    "            sentences (list): List of sentence dictionaries\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Distances and updated sentences\n",
    "        \"\"\"\n",
    "        # Encode combined sentences\n",
    "        embeddings = self.model.encode([x['combined_sentence'] for x in sentences])\n",
    "        \n",
    "        # Add embeddings to sentences\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "        \n",
    "        # Calculate inter-sentence distances\n",
    "        distances = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "            embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "            \n",
    "            similarity = np.dot(embedding_current, embedding_next) / (\n",
    "                np.linalg.norm(embedding_current) * np.linalg.norm(embedding_next)\n",
    "            )\n",
    "            distance = 1 - similarity\n",
    "            distances.append(distance)\n",
    "            sentences[i]['distance_to_next'] = distance\n",
    "        \n",
    "        return distances, sentences\n",
    "\n",
    "    def chunk_text(self, text):\n",
    "        \"\"\"\n",
    "        Dynamic chunking based on semantic distances and token management\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "        \n",
    "        Returns:\n",
    "            list: Semantic chunks\n",
    "        \"\"\"\n",
    "        # First, use token-based splitting as a fallback\n",
    "        if len(text) > self.max_tokens * 2:  # Rough token estimation\n",
    "            return self.token_splitter.split_text(text)\n",
    "        \n",
    "        # Preprocess and combine sentences\n",
    "        sentences = self.preprocess_text(text)\n",
    "        sentences = self.combine_sentences(sentences)\n",
    "        \n",
    "        # Calculate cosine distances\n",
    "        distances, sentences = self.calculate_cosine_distances(sentences)\n",
    "        \n",
    "        # Determine breakpoint threshold\n",
    "        breakpoint_distance_threshold = np.percentile(distances, self.breakpoint_percentile)\n",
    "        \n",
    "        # Split document into chunks\n",
    "        chunks = []\n",
    "        start_index = 0\n",
    "        indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "        \n",
    "        for index in indices_above_thresh:\n",
    "            end_index = index\n",
    "            group = sentences[start_index:end_index + 1]\n",
    "            combined_text = ' '.join([d['sentence'] for d in group])\n",
    "            chunks.append(combined_text)\n",
    "            start_index = index + 1\n",
    "        \n",
    "        # Add remaining text\n",
    "        if start_index < len(sentences):    \n",
    "            combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "            chunks.append(combined_text)\n",
    "        \n",
    "        # Fallback to token-based splitting if chunks are too large\n",
    "        final_chunks = []\n",
    "        for chunk in chunks:\n",
    "            if len(chunk) > self.max_tokens * 2:\n",
    "                final_chunks.extend(self.token_splitter.split_text(chunk))\n",
    "            else:\n",
    "                final_chunks.append(chunk)\n",
    "        \n",
    "        return final_chunks\n",
    "\n",
    "def paramExtractionUsingLLM(chunks, label, llm, chain):\n",
    "    \"\"\"\n",
    "    Extract parameters from text chunks using Language Model\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): Text chunks\n",
    "        label (str): Label for extraction\n",
    "        llm: Language model\n",
    "        chain: QA chain\n",
    "    \n",
    "    Returns:\n",
    "        list: Extracted results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        prompt = f\"\"\"Extract only the precise and most relevant details for the label '{label}'. \n",
    "        Focus on key information. Avoid redundancy and summaries.\n",
    "\n",
    "        Context: {chunk}\n",
    "\n",
    "        Extraction: \"\"\"\n",
    "        \n",
    "        try:\n",
    "            with get_openai_callback() as cost:\n",
    "                response = chain.run(input_documents=[Document(page_content=chunk)], question=prompt)\n",
    "            \n",
    "            # Only append non-empty results\n",
    "            if response and response.strip():\n",
    "                results.append(response.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk for {label}: {e}\")\n",
    "    \n",
    "    return [\" \".join(results)] if results else [\"No relevant information found\"]\n",
    "\n",
    "def main(file_path, labels):\n",
    "    \"\"\"\n",
    "    Main processing function\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to input text file\n",
    "        labels (list): Labels for extraction\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted labeled information\n",
    "    \"\"\"\n",
    "    # Read input text\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        essay = f.read()\n",
    "    \n",
    "    # Initialize semantic chunker\n",
    "    chunker = SemanticChunker(max_tokens=7000)\n",
    "    chunks = chunker.chunk_text(essay)\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        openai_api_base=\"http://localhost:8000/v1\",\n",
    "        openai_api_key=\"FAKE\",\n",
    "        max_tokens=700,  # Reduced max tokens\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Load QA chain\n",
    "    chain = load_qa_chain(llm, verbose=True, chain_type='stuff')\n",
    "    \n",
    "    # Process labels\n",
    "    labeled_ans = defaultdict(list)\n",
    "    for label in labels:\n",
    "        result = paramExtractionUsingLLM(chunks, label, llm, chain)\n",
    "        labeled_ans[label].append(result)\n",
    "    \n",
    "    return labeled_ans\n",
    "\n",
    "# Example usage\n",
    "labels = [\"Important Dates\", \"Eligibility Criteria\", \"Scope of Work\", \"Contact Details\"]\n",
    "input_file = '/data/tendergpt/testing/77774640/77774640.txt'\n",
    "output_file = '/data/QAAPI/stored_files/llm_out.json'\n",
    "\n",
    "# Run extraction\n",
    "results = main(input_file, labels)\n",
    "\n",
    "# Save results\n",
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(results, out_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
