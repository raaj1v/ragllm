{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /data/QAAPI/qavenv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fuzzywuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pdfplumber\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import requests\n",
    "\n",
    "# from Classification_Api import *\n",
    "from pdf_boq_extraction2 import *\n",
    "from doc_boq_extraction import get_most_relevant_table_doc\n",
    "\n",
    "\n",
    "def should_drop(value):\n",
    "    if isinstance(value, str):\n",
    "        return value.startswith(\"number #\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join(i for i in text if ord(i) < 128)\n",
    "\n",
    "def normalize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return ' '.join(text.lower().split())\n",
    "    return text  # Return numeric values or other types as-is\n",
    "\n",
    "def save_all_dfs(dfs, base_path, base_name):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    file_number = 0\n",
    "    for df in dfs:\n",
    "        # # Clean the DataFrame\n",
    "        # df = set_header_from_keywords(df, keywords)\n",
    "        # df = df.dropna(how='all')\n",
    "        # df = df.dropna(axis=1, how='all')\n",
    "        while True:\n",
    "            output_file = os.path.join(base_path, f\"{base_name}_BOQ{file_number if file_number > 0 else ''}.csv\")\n",
    "            if not os.path.exists(output_file):\n",
    "                df.to_csv(output_file, index=False)\n",
    "                print(f\"Saved extracted data to: {output_file}\")\n",
    "                break\n",
    "            file_number += 1\n",
    "\n",
    "def find_header_row_1608(df, header_keywords, min_matches=2):\n",
    "    for i, row in df.iterrows():\n",
    "        row_str = ' '.join(row.dropna().astype(str).tolist()).lower()\n",
    "\n",
    "        # Check individual keyword matches\n",
    "        keyword_matches = [any(fuzz.partial_ratio(keyword, word) > 40 for word in row_str.split()) for keyword in header_keywords]\n",
    "        if sum(keyword_matches) >= min_matches:  # Heuristic: At least 2-3 keywords should match\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def name_check_1609_doc(folder_path, base_path):\n",
    "    file_names = os.listdir(folder_path)\n",
    "    valid_extensions = ('.xlsx', '.csv', '.pdf', '.xls','.doc','.docx')\n",
    "    matched_files = [file for file in file_names if file.lower().endswith(valid_extensions)]\n",
    "    print(\"Matched files:\", matched_files)\n",
    "    \n",
    "    all_dfs = []  # List to store DataFrames\n",
    "    for file_name in matched_files:\n",
    "        try:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = None\n",
    "\n",
    "            if file_name.lower().endswith('.xlsx') or file_name.lower().endswith('.xls'):\n",
    "                if file_name.lower().endswith('.xls'):\n",
    "                    df = pd.read_excel(file_path, engine='xlrd')\n",
    "                else:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                print(\"XLSX:\", file_name)\n",
    "                \n",
    "            elif file_name.lower().endswith('.csv'):\n",
    "                df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "                print(\"CSV:\", file_name)\n",
    "                \n",
    "            elif file_name.lower().endswith('.pdf'):\n",
    "                df = get_most_relevant_table(file_path)\n",
    "                if df is not None:\n",
    "                    print(f\"Processed PDF: {file_name}\")\n",
    "                    if len(all_dfs) > 1:  # If all_dfs is empty\n",
    "                        if 'RA Details' in df.columns:\n",
    "                            print(f\"Skipping file with 'RA Details' column: {file_name}\")\n",
    "                            continue  # Skip appending this DataFrame\n",
    "                    else:                         \n",
    "                        all_dfs.append(df)  # Append the extracted DataFrame to all_dfs\n",
    "                else:\n",
    "                    print(f\"Failed to extract table from PDF: {file_name}\")\n",
    "                    continue\n",
    "                \n",
    "            else:\n",
    "                print(\"Other file type:\", file_name)\n",
    "                continue\n",
    "\n",
    "            if df is not None and not file_name.lower().endswith('.pdf'):\n",
    "                header_keywords = [\n",
    "                    'bill of quantity', 'qty', 'quantity', \"inr\", \"item/activity\", 'total price', \"item description\", \"weight\",\n",
    "                    'quantity', 'qty', 'quintal', 'quantity in quintal', 'total quantity', 'rate', 'price', 'base rate',\n",
    "                    'offer rate', 'rate per quintal', 'rate in rs.', 'rate per unit', 'item rate', 'rate per item',\n",
    "                    'price per unit', 'price', 'schedule of rates', 'charges', 'total cost', 'cost', 'total base cost',\n",
    "                    'total offer cost', 'price in rs.', 'overall rate', 'aggregate rate', 'extended amount', 'landed cost',\n",
    "                    'final rate', 'total charges', 'lumpsum cost', 'lump-sum', 'item', 'description', 'particulars',\n",
    "                    'activity', 'item/activity', 'deliverables', 'item description', 'item name', 'item title', 'item description'\n",
    "                ]\n",
    "                \n",
    "                # header_row_idx = find_header_row_1608(df, header_keywords)\n",
    "                \n",
    "                if header_row_idx is not None:\n",
    "                    if header_row_idx != 0:\n",
    "                        # Only modify the DataFrame if the header is not in the first row\n",
    "                        df.columns = df.iloc[header_row_idx]\n",
    "                        df = df[header_row_idx + 1:].reset_index(drop=True)\n",
    "                    \n",
    "                    df = df.apply(lambda x: x.map(normalize_text) if x.dtype == 'object' else x)\n",
    "                    \n",
    "                    # Ensure specific columns are within the range of DataFrame columns\n",
    "                    specific_cols = [0, 1, 3, 4, 5, 12, 52, 53, 54]\n",
    "                    max_index = df.shape[1] - 1\n",
    "                    valid_cols = [col for col in specific_cols if col <= max_index]\n",
    "\n",
    "                    if valid_cols:\n",
    "                        df = df.iloc[:, valid_cols]\n",
    "                        rows_to_drop = df.apply(lambda x: x.map(should_drop).any(), axis=1)\n",
    "\n",
    "\n",
    "                        # Get the index of the first row that meets the condition\n",
    "                        first_row_to_drop = rows_to_drop.idxmax() if rows_to_drop.any() else None\n",
    "\n",
    "                        # Drop the row and all rows above it if a matching row is found\n",
    "                        if first_row_to_drop is not None:\n",
    "                            df = df.drop(index=df.index[:first_row_to_drop + 1])\n",
    "                            print(f\"Dropped rows up to {first_row_to_drop} in file: {file_name}\")\n",
    "                        else:\n",
    "                            df = df.copy()\n",
    "                \n",
    "                        # Replace 'Unnamed' columns with '-'\n",
    "                        df.columns = [str(col) for col in df.columns] \n",
    "                        df.columns = ['-' if 'Unnamed' in col else col for col in df.columns]\n",
    "                        df.dropna(axis=1, how='all', inplace=True)  # Drop empty columns\n",
    "                        df.dropna(axis=0, how='all', inplace=True)  # Drop empty rows\n",
    "                        all_dfs.append(df)\n",
    "                    \n",
    "                    else:\n",
    "                        print(f\"No valid columns found in file: {file_name}\")\n",
    "                \n",
    "                else:\n",
    "                    print(f\"Header row not found in file: {file_name}\")\n",
    "\n",
    "        except pd.errors.ParserError:\n",
    "            print(f\"Error reading file: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {file_name} - {e}\")\n",
    "    # Check if all_dfs is empty and process DOC/DOCX files if present\n",
    "    if not all_dfs:\n",
    "        doc_files = [file for file in file_names if file.lower().endswith(('.doc', '.docx'))]\n",
    "        for doc_file in doc_files:\n",
    "            file_path = os.path.join(folder_path, doc_file)\n",
    "            df = get_most_relevant_table_doc(file_path)\n",
    "            if df is not None:\n",
    "                all_dfs.append(df)\n",
    "                print(f\"Extracted table from DOC/DOCX: {doc_file}\")\n",
    "            else:\n",
    "                print(f\"Failed to extract table from DOC/DOCX: {doc_file}\")\n",
    "    save_all_dfs(all_dfs, base_path, os.path.basename(folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/data/unzipdocument/dailydocument_23-10-24/73342080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
