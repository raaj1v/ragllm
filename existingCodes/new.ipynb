{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "import pandas as pd\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': 'localhost', 'port': 9200}],\n",
    "    http_auth=('admin', '4Z*lwtz,,2T:0TGu'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False  # Set to True if you have proper certificates\n",
    ")\n",
    "\n",
    "\n",
    "# Query OpenSearch to fetch all document IDs\n",
    "index_name = \"tprocanswers\"\n",
    "query = {\n",
    "    \"_source\": False,\n",
    "    \"query\": {\"match_all\": {}}\n",
    "}\n",
    "\n",
    "# Scroll API for fetching large datasets\n",
    "result = client.search(index=index_name, body=query, scroll=\"2m\", size=1000)\n",
    "scroll_id = result[\"_scroll_id\"]\n",
    "hits = result[\"hits\"][\"hits\"]\n",
    "\n",
    "# Collect IDs\n",
    "ids = [hit[\"_id\"] for hit in hits]\n",
    "\n",
    "# Scroll through all results\n",
    "while len(hits) > 0:\n",
    "    result = client.scroll(scroll_id=scroll_id, scroll=\"2m\")\n",
    "    scroll_id = result[\"_scroll_id\"]\n",
    "    hits = result[\"hits\"][\"hits\"]\n",
    "    ids.extend(hit[\"_id\"] for hit in hits)\n",
    "\n",
    "# Split data if it exceeds Excel's row limit\n",
    "max_rows = 1000000\n",
    "file_count = 1\n",
    "\n",
    "for i in range(0, len(ids), max_rows):\n",
    "    # Create a DataFrame for the current chunk\n",
    "    chunk = ids[i:i + max_rows]\n",
    "    df = pd.DataFrame(chunk, columns=[\"tcno\"])\n",
    "    \n",
    "    # Save to an Excel file\n",
    "    file_name = f\"document_ids_part{file_count}.xlsx\"\n",
    "    df.to_excel(file_name, index=False)\n",
    "    print(f\"Saved: {file_name}\")\n",
    "    file_count += 1\n",
    "\n",
    "print(\"All document IDs have been saved in multiple Excel files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, ast\n",
    "sys.path.append(r'/data/imageExtraction/')\n",
    "sys.path.append(r'/data/imageExtraction/imgEnv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib64/python311.zip', '/usr/lib64/python3.11', '/usr/lib64/python3.11/lib-dynload', '', '/home/rajeev/.local/lib/python3.11/site-packages', '/usr/local/lib64/python3.11/site-packages', '/usr/local/lib/python3.11/site-packages', '/usr/lib64/python3.11/site-packages', '/usr/lib/python3.11/site-packages', '/data/imageExtraction/', '/data/imageExtraction/imgEnv/']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload\n",
    "import imageAPI_live as imgProcessingCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajeev/.local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import re\n",
    "from sentence_transformers import util\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, re, warnings, io, time, ast, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "from apiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'/data/imageExtraction/')\n",
    "sys.path.append(r'/data/imageExtraction/imgEnv/')\n",
    "\n",
    "try:\n",
    "    from deep_translator import GoogleTranslator\n",
    "except ModuleNotFoundError:\n",
    "    print(\"deep_translator module not found. Please check installation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n",
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "\n",
    "# Attempt to connect to the database\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "            user=\"boq\",\n",
    "            password=\"boq\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\",\n",
    "            database=\"tbl_docboq\"\n",
    "        )\n",
    "    # If connected, print success message\n",
    "    print(\"Connection successful\")\n",
    "except OperationalError as e:\n",
    "    print(f\"Error: Unable to connect to the database: {e}\")\n",
    "finally:\n",
    "    # Closing the connection\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Connection closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile=r'C:\\Users\\hetvi.solanki\\Downloads\\77153810.txt'\n",
    "with open(outFile, 'r', encoding='utf-8') as f:\n",
    "    essay = f.read()\n",
    "\n",
    " \n",
    "import re\n",
    "\n",
    "# Splitting the essay on '.', '?', and '!'\n",
    "single_sentences_list = re.split(r'(?<=[.?!])\\s+', essay)\n",
    "print (f\"{len(single_sentences_list)} senteneces were found\")\n",
    "\n",
    " \n",
    "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
    "sentences[:3]\n",
    "\n",
    " \n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences\n",
    "\n",
    "sentences = combine_sentences(sentences)\n",
    "\n",
    " \n",
    "sentences[:3]\n",
    "\n",
    " \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "oaiembeds = OpenAIEmbeddings()\n",
    "\n",
    " \n",
    "embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "\n",
    " \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "\n",
    " \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances, sentences\n",
    "\n",
    " \n",
    "distances, sentences = calculate_cosine_distances(sentences)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances);\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "plt.plot(distances);\n",
    "\n",
    "y_upper_bound = .2\n",
    "plt.ylim(0, y_upper_bound)\n",
    "plt.xlim(0, len(distances))\n",
    "\n",
    "# We need to get the distance threshold that we'll consider an outlier\n",
    "# We'll use numpy .percentile() for this\n",
    "breakpoint_percentile_threshold = 95 #95\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-');\n",
    "\n",
    "# Then we'll see how many distances are actually above this one\n",
    "num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f\"{num_distances_above_theshold + 1} Chunks\");\n",
    "\n",
    "# Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "\n",
    "# Start of the shading and text\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "for i, breakpoint_index in enumerate(indices_above_thresh):\n",
    "    start_index = 0 if i == 0 else indices_above_thresh[i - 1]\n",
    "    end_index = breakpoint_index if i < len(indices_above_thresh) - 1 else len(distances)\n",
    "\n",
    "    plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)\n",
    "    plt.text(x=np.average([start_index, end_index]),\n",
    "             y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "             s=f\"Chunk #{i}\", horizontalalignment='center',\n",
    "             rotation='vertical')\n",
    "\n",
    "# # Additional step to shade from the last breakpoint to the end of the dataset\n",
    "if indices_above_thresh:\n",
    "    last_breakpoint = indices_above_thresh[-1]\n",
    "    if last_breakpoint < len(distances):\n",
    "        plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)\n",
    "        plt.text(x=np.average([last_breakpoint, len(distances)]),\n",
    "                 y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "                 s=f\"Chunk #{i+1}\",\n",
    "                 rotation='vertical')\n",
    "\n",
    "plt.title(\"PG Essay Chunks Based On Embedding Breakpoints\")\n",
    "plt.xlabel(\"Index of sentences in essay (Sentence Position)\")\n",
    "plt.ylabel(\"Cosine distance between sequential sentences\")\n",
    "plt.show()\n",
    "\n",
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Create a list to hold the grouped sentences\n",
    "chunks = []\n",
    "\n",
    "# Iterate through the breakpoints to slice the sentences\n",
    "for index in indices_above_thresh:\n",
    "    # The end index is the current breakpoint\n",
    "    end_index = index\n",
    "\n",
    "    # Slice the sentence_dicts from the current start index to the end index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    \n",
    "    # Update the start index for the next group\n",
    "    start_index = index + 1\n",
    "\n",
    "# The last group, if any sentences remain\n",
    "if start_index < len(sentences):    \n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "# grouped_sentences now contains the chunked sentences\n",
    "\n",
    " \n",
    "# for i, chunk in enumerate(chunks[:20]):\n",
    "#     buffer = 200\n",
    "    \n",
    "#     print(f\"Chunk #{i}\")\n",
    "#     print(chunk.strip())\n",
    "#     # print (\"...\")\n",
    "#     # print (chunk[-buffer:].strip())\n",
    "#     print(\"\\n\")\n",
    "\n",
    "   \n",
    "# ## Semantic Clustering\n",
    "\n",
    " \n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the model and initialize labels\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "labels = [\"Important Date\", \"Eligibility or Prequalification Criteria\", \"Technical Requirements\", \"Contact Details\"]\n",
    "label_embeddings = model.encode(labels)  # Get embeddings for labels\n",
    "\n",
    "# Initialize FAISS index with label embeddings\n",
    "dimension = label_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "faiss_index.add(label_embeddings)  # Add label embeddings to FAISS index\n",
    "\n",
    "def segment_text_with_faiss_label_assignment(semantic_chunks):\n",
    "    labeled_segments = defaultdict(list)\n",
    "    \n",
    "    for chunk in semantic_chunks:\n",
    "        if chunk.strip():  # Skip empty paragraphs\n",
    "            paragraph_embedding = model.encode(chunk).reshape(1, -1)\n",
    "            _, closest_label_index = faiss_index.search(paragraph_embedding, 1)\n",
    "            closest_label = labels[closest_label_index[0][0]]\n",
    "            labeled_segments[closest_label].append(chunk)\n",
    "     \n",
    "    return labeled_segments\n",
    "\n",
    "RELEVANCE_THRESHOLD = -0.7  # Adjust based on experimentation\n",
    "\n",
    "def segment_text_with_faiss_label_assignment(semantic_chunks, threshold=RELEVANCE_THRESHOLD):\n",
    "    labeled_segments = defaultdict(list)\n",
    "    irrelevant_segments = []  # For storing irrelevant chunks\n",
    "\n",
    "    for chunk in semantic_chunks:\n",
    "        if chunk.strip():  # Skip empty paragraphs\n",
    "            paragraph_embedding = model.encode(chunk).reshape(1, -1)\n",
    "            \n",
    "            # Get similarity scores with all labels\n",
    "            distances, label_indices = faiss_index.search(paragraph_embedding, len(labels))\n",
    "            similarities = 1 - distances  # Convert distances to cosine similarity\n",
    "            # for i, sim in enumerate(similarities[0]):\n",
    "            #     #print(f\"Label: {labels[i]}, Similarity: {sim}\")\n",
    "            # Assign to multiple labels if similarity exceeds threshold\n",
    "            assigned_labels = [labels[i] for i, sim in enumerate(similarities[0]) if sim >= threshold]\n",
    "\n",
    "            if assigned_labels:\n",
    "                print(\"Assigned labels : \")\n",
    "                print(assigned_labels)\n",
    "                # Add chunk to all relevant labels\n",
    "                for label in assigned_labels:\n",
    "                    labeled_segments[label].append(chunk)\n",
    "            else:\n",
    "                # If no label has sufficient similarity, mark it as irrelevant\n",
    "                labeled_segments[\"Other\"].append(chunk)\n",
    "    \n",
    "    for label in labels:\n",
    "        print(label , len(labeled_segments[label]))\n",
    "    # Add the irrelevant chunks to an \"Other\" category\n",
    "    # labeled_segments[\"Other\"] = irrelevant_segments\n",
    "    return labeled_segments\n",
    " \n",
    "segmented_result = segment_text_with_faiss_label_assignment(chunks)\n",
    "#print(\" == Segmented Results == \")\n",
    "#print(segmented_result)\n",
    "# Save the result to JSON\n",
    "out_file_path = r'C:\\Users\\hetvi.solanki\\Desktop\\AIProjects\\ragllm\\ragTechniques\\RAG_Techniques\\data\\out.json'\n",
    "with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(segmented_result, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    " \n",
    "# Prepare data for visualization\n",
    "colors = ['red', 'blue', 'green', 'yellow']  # Define colors for each cluster\n",
    "scatter_points = []\n",
    "labels_for_plot = []\n",
    "\n",
    "for label_index, label in enumerate(labels):\n",
    "    paragraphs = segmented_result[label]\n",
    "    #print(\"--para--\", label)\n",
    "    #print(paragraphs)\n",
    "    paragraph_embeddings = model.encode(paragraphs)\n",
    "    scatter_points.append(paragraph_embeddings)\n",
    "    labels_for_plot.extend([label] * len(paragraphs))\n",
    "\n",
    "# Concatenate all paragraph embeddings for plotting\n",
    "all_embeddings = np.vstack(scatter_points)\n",
    "\n",
    "# Use KMeans to cluster embeddings\n",
    "n_clusters = len(labels)  # Number of clusters based on labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(all_embeddings)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(all_embeddings)\n",
    " \n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# labeled_final_chunk=[]\n",
    "# #print(len(chunks))\n",
    "for i, label in enumerate(labels):\n",
    "    indices = np.where(cluster_labels == i)[0]  # Get indices of paragraphs in this cluster\n",
    "    # para=\"\"\n",
    "    # #print(indices)\n",
    "    # for index in indices:\n",
    "    #     para += chunks[index]\n",
    "    # labeled_final_chunk.append((label, para))\n",
    "    ##print(label, \" \", colors[i])\n",
    "    ##print(indices)\n",
    "    plt.scatter(reduced_embeddings[indices, 0], reduced_embeddings[indices, 1], \n",
    "                label=label, color=colors[i], alpha=0.7)\n",
    "\n",
    "plt.title('Text Segmentation Clusters with K-Means')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    " \n",
    "##print(\"labeled_final_chunk : \", labeled_final_chunk)\n",
    "\n",
    " \n",
    "def search_chunk(label_name, labeled_final_chunk):\n",
    "    for label, para in labeled_final_chunk:\n",
    "            if label == label_name:  # Check if the label matches\n",
    "                return para \n",
    "\n",
    "eligibility_criteria_chunk = segmented_result[\"Eligibility or Prequalification Criteria\"]\n",
    "# ## LLM \n",
    "\n",
    "from openai import OpenAI\n",
    "import concurrent.futures\n",
    "client = OpenAI(\n",
    "    base_url=\"http://129.154.248.128:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "model=client.models.list().data[0].id\n",
    "\n",
    "# MAX_TOKENS = 8000\n",
    "\n",
    "# def split_chunk_for_llm(chunk, max_tokens=MAX_TOKENS):\n",
    "#     # Assuming 1 token ~ 4 characters on average\n",
    "#     avg_chars_per_token = 4\n",
    "#     max_chars = max_tokens * avg_chars_per_token\n",
    "    \n",
    "#     if len(chunk) > max_chars:\n",
    "#         # Split chun..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
