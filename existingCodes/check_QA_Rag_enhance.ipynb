{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append(r\"/data/QAAPI/qa/lib/python3.10/site-packages/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pyodbc\n",
    "import warnings\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from opensearchpy import OpenSearch\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Query Titles\n",
    "query_titles = {\n",
    "    \"List of all the dates mentioned in the tender document...\": \"Important Dates\",\n",
    "    \"Clauses specifying prequalification requirements...\": \"Prequalification Criteria\",\n",
    "    \"Technical Eligibility criteria\": \"Technical Requirements\",\n",
    "    \"Performance criteria\": \"Performance Criteria\",\n",
    "    \"Financial criteria\": \"Financial Criteria\",\n",
    "    \"List all mandatory qualification criteria...\": \"Mandatory Qualification Criteria\",\n",
    "    \"Specifications that bidders must meet\": \"Specifications\",\n",
    "    \"What are the functional requirements...\": \"Scope of Work\"\n",
    "}\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
    "\n",
    "# LLM model initialization (using locally hosted LLaMA 3)\n",
    "llm_model = ChatOpenAI(\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    openai_api_base=\"http://localhost:8000/v1\",\n",
    "    openai_api_key=\"FAKE\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# # OpenSearch and Elasticsearch clients\n",
    "# index_name = 'tprocanswers'\n",
    "# opensearch_client = OpenSearch(\n",
    "#     hosts=['https://10.0.0.109:9200'],\n",
    "#     http_auth=(\"admin\", \"4Z*lwtz,,2T:0TGu\"),\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=False,\n",
    "#     ssl_show_warn=False\n",
    "# )\n",
    "# es_client = Elasticsearch(['http://10.0.0.200:9200'])\n",
    "\n",
    "# # Ensure index exists\n",
    "# if not opensearch_client.indices.exists(index=index_name):\n",
    "#     opensearch_client.indices.create(index=index_name)\n",
    "\n",
    "# Custom Embeddings API request\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"BAAI/bge-small-en-v1.5\", \"input\": [text]})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class CustomEmbeddings(HuggingFaceEmbeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "# Load and process text\n",
    "def process_text(text_docs):\n",
    "    text = \"\\n\".join([doc.page_content for doc in text_docs])\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=2048, chunk_overlap=32)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Use custom embeddings\n",
    "    embeddings = CustomEmbeddings()\n",
    "    knowledge_base = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    \n",
    "    return knowledge_base\n",
    "\n",
    "# Load text files\n",
    "def load_text_files_from_directory(folder_path):\n",
    "    all_docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = TextLoader(file_path)\n",
    "            all_docs.extend(loader.load())\n",
    "    return all_docs\n",
    "\n",
    "# Process query\n",
    "def process_query(query, knowledge_base):\n",
    "    try:\n",
    "        docs = knowledge_base.similarity_search(query)\n",
    "        chain = load_qa_chain(llm_model, chain_type='stuff')\n",
    "        with get_openai_callback() as cost:\n",
    "            response = chain.run(input_documents=docs, question=query)\n",
    "        return {'title': query_titles[query], 'response': response}\n",
    "    except Exception as e:\n",
    "        return {'title': query_titles[query], 'error': str(e)}\n",
    "\n",
    "# Update Elasticsearch\n",
    "def update_elasticsearch(tcno):\n",
    "    search_query = {\"query\": {\"term\": {\"tcno\": tcno}}}\n",
    "    indices = ['tbl_tendersadvance_migration_embedding', 'tbl_tendersadvance_migration']\n",
    "    \n",
    "    for index in indices:\n",
    "        es_response = es_client.search(index=index, body=search_query)\n",
    "        if es_response['hits']['total']['value'] > 0:\n",
    "            for hit in es_response['hits']['hits']:\n",
    "                es_client.update(index=index, id=hit['_id'], body={\"doc\": {\"ispq\": 1}})\n",
    "                print(f\"Updated tcno {tcno} with ispq = 1 in {index}\")\n",
    "\n",
    "# Update database\n",
    "def update_database(tcno):\n",
    "    conn_string = (\n",
    "        'DRIVER=/opt/microsoft/msodbcsql18/lib64/libmsodbcsql-18.3.so.3.1;'\n",
    "        'SERVER=10.0.0.63;DATABASE=ttneo;UID=aimlpq;PWD=aimlpq;'\n",
    "        'TrustServerCertificate=yes;'\n",
    "    )\n",
    "    with pyodbc.connect(conn_string) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"UPDATE apptender.tbl_tender SET ispq = 1 WHERE tcno = ?\", (tcno,))\n",
    "        conn.commit()\n",
    "        print(f\"Database updated successfully for tcno {tcno}\")\n",
    "\n",
    "# Folder processing\n",
    "def process_folder(tcno):\n",
    "    try:\n",
    "        folder_path = f\"/data/txtfolder/dailydocument_11-10-24_txt/{tcno}\"\n",
    "        all_docs = load_text_files_from_directory(folder_path)\n",
    "        knowledge_base = process_text(all_docs)\n",
    "\n",
    "        results = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_query, query, knowledge_base) for query in query_titles.keys()]\n",
    "            for future in as_completed(futures):\n",
    "                results.append(future.result())\n",
    "\n",
    "        json_response = {\"results\": [{\"title\": result[\"title\"], \"response\": result.get(\"response\", result.get(\"error\"))} for result in results]}\n",
    "        opensearch_client.index(index=index_name, id=tcno, body=json_response)\n",
    "\n",
    "        update_elasticsearch(tcno)\n",
    "        update_database(tcno)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process folder {tcno}: {str(e)}\")\n",
    "\n",
    "# Process all folders in parallel\n",
    "def process_folders_in_parallel():\n",
    "    base_folder_path = \"/data/txtfolder/dailydocument_11-10-24_txt\"\n",
    "    tcno_folders = [tcno for tcno in os.listdir(base_folder_path) if os.path.isdir(os.path.join(base_folder_path, tcno))]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        executor.map(process_folder, tcno_folders)\n",
    "\n",
    "# Main function\n",
    "if __name__ == '__main__':\n",
    "    process_folders_in_parallel()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
