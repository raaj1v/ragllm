{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1:\n",
      "Query: Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\n",
      "{\n",
      "    \"Prequalification Criteria\": \"Here are the extracted clauses that specify Pre-Qualification Criteria or eligibility criteria:\\n\\n**Section 3 - Evaluation and Qualification Criteria**\\n\\n1. **Eligibility** (Criteria Compliance Requirements Documents)\\n\\t* 2.1.1 Nationality: The Bidder must meet the requirement of nationality in accordance with ITB Sub-Clause 4.2.\\n\\t* 2.1.2 Conflict of Interest: The Bidder must meet the requirement of no conflicts of interest in accordance with ITB Sub-Clause 4.3.\\n\\t* 2.1.3 Government-owned Entity: The Bidder must meet the requirements of ITB Sub-Clause 4.5.\\n\\t* 2.1.4 Government-owned Entity: The Bidder must meet the requirements of ITB Sub-Clause 4.5.\\n\\t* 2.1.5 UN Eligibility: The Bidder must meet the requirement of not being declared ineligible by the UN.\\n2. **Pending Litigation** (Criteria Compliance Requirements Documents)\\n\\t* 2.2.1 Pending Litigation: All pending litigation shall be treated as resolved against the Bidder and shall not represent more than 50 percent of the Bidder's net worth.\\n3. **Financial Situation** (Criteria Compliance Requirements Documents)\\n\\t* 2.3.1 Historical Financial Performance: The Bidder must submit audited balance sheets or other financial statements acceptable to the Employer for the last 5 years.\\n\\t* 2.3.2 Average Annual Construction Turnover: The Bidder must demonstrate a minimum average annual construction turnover of INR 183.06 Lakhs (Rupees One Crore Eighty Three Lakhs & Six Thousand Only).\\n\\t* 2.3.3 Financial Resources: The Bidder must demonstrate access to or availability of liquid assets, lines of credit, or other financial resources to meet the Bidder's financial resources requirement indicated in Form FIN-4.\\n4. **Experience** (Criteria Compliance Requirements Documents)\\n\\t* 2.4.1 General Construction Experience: The Bidder must have experience under construction contracts in the role of contractor, subcontractor, or management contractor for at least the last 5 years prior to the bid submission deadline.\\n\\t* 2.4.2 Specific Construction Experience: The Bidder must have experience of executing the work of value of at least 25% of the estimated cost of the work in any one year during the last 5 financial years.\\n5. **Minimum Requirement of key Plant and Equipment** (Criteria Compliance Requirements Documents)\\n\\t* The Bidder must demonstrate clearly that it has the capability to meet the minimum requirement listed below:\\n\\t\\t+ Tractor Trolly (at least one owned)\\n\\t\\t+ Crash barrier installation Machine\\n\\t\\t+ Water tanker (owned/leased)\\n\\t\\t+ Needle Vibrator\\n\\t\\t+ Concrete mixer\\n6. **Requirement of Technical Staff** (Criteria Compliance Requirements Documents)\\n\\t* The Bidder must demonstrate availability of qualified key personnel listed below:\\n\\t\\t+ Deputy Project Manager cum Highway Engineer\\n\\t\\t+ Construction Safety Officer\\n\\t\\t+ Site Engineer\\n\\t\\t+ Work Supervisor\\n\\n**Section 4 - Bidding Forms**\\n\\n1. **Form ELI - 1: Bidder\\u2019s Information Sheet**\\n\\t* The Bidder must provide information about its legal name, country of constitution, year of constitution, and authorized representative.\\n2. **Form ELI - 2: JV Information Sheet**\\n\\t* Each member of a JV must provide information about its legal name, country of constitution, year of constitution, and authorized representative.\\n3. **Form LIT - Pending Litigation**\\n\\t* Each Bidder or member of a JV must provide information about any pending litigation.\\n4. **Form FIN - 1: Financial Situation**\\n\\t* Each Bidder or member of a JV must provide financial data for the last 3 years.\\n5. **Form FIN - 2: Average Annual Construction Turnover**\\n\\t* Each Bidder or member of a JV must provide information about its average annual construction turnover for the last 3 years.\\n6. **Form FIN - 3: Financial Resources**\\n\\t* Each Bidder or member of a JV must provide information about its proposed sources of financing.\\n7. **Form FIN - 4: Current Contract Commitments / Works in Progress**\\n\\t* Each Bidder or member of a JV must provide information about its current contract commitments and works in progress.\\n8. **Form EXP - 1: General Construction Experience**\\n\\t* Each Bidder or member of a JV must provide information about its general construction experience.\\n9. **Form EXP - 2: Specific Construction Experience**\\n\\t* Each Bidder or member of a JV must provide information about its specific construction experience.\\n\\n**Section 7 - Particular Conditions of Contract**\\n\\n1. **GCC 61.1**\\n\\t* The Contractor, subcontractors, or suppliers for any part of the contract, including related services, shall have the nationality of India.\\n2.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "             model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            # \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\"\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\"\n",
    "            # \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            # \"List of all the dates mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD amount,tender fee, tender value\": \"Important Dates\",\n",
    "            # \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.request_count = 0 \n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            # print(\"text:::::::\",text)\n",
    "        \n",
    "        # Split into sentences and create chunks\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        # print(\"sentences:::::::\",sentences)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        # print(\"chunks:::::::\",chunks)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 1000) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            self.request_count += 1  # Increment the request counter\n",
    "            \n",
    "            # Print the current request details\n",
    "            print(f\"Request {self.request_count}:\")\n",
    "            print(f\"Query: {query}\")\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document\n",
    "        chunks = self.process_document(file_path)\n",
    "        # print(\"chunks::::\",chunks)\n",
    "        combined_text = \" \".join(chunks)\n",
    "        # print(\"combined_text::::\",combined_text)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        results = {}\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, combined_text): title\n",
    "                for query, title in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] = response\n",
    "                except Exception as e:\n",
    "                    results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file = \"/data/Pqmatch/testing/78804029/78804029.txt\"\n",
    "    \n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1:\n",
      "Query: Extract the most critical pre-qualification or eligibility criteria mentioned in the document.\n",
      "{\n",
      "    \"Prequalification Criteria\": \"Based on the provided document, the most critical pre-qualification or eligibility criteria mentioned are:\\n\\n1. **Nationality**: The Bidder must meet the requirement of nationality as specified in ITB 4.2.\\n2. **Conflict of Interest**: The Bidder must not have any conflict of interest as specified in ITB 4.3.\\n3. **Government-owned Entity**: The Bidder must meet the requirements of ITB 4.5 if it is a government-owned entity.\\n4. **Eligibility**: The Bidder must not have been declared ineligible by Government of Uttarakhand as specified in ITB 4.4.\\n5. **Financial Situation**: The Bidder must demonstrate a sound financial position and prospective long-term profitability as specified in ITB 2.3.1.\\n6. **Average Annual Construction Turnover**: The Bidder must have a minimum average annual construction turnover of INR 183.06 Lakhs as specified in ITB 2.3.2.\\n7. **Financial Resources**: The Bidder must demonstrate access to or availability of liquid assets, lines of credit, or other financial resources as specified in ITB 2.3.3.\\n8. **Experience**: The Bidder must have experience under construction contracts in the role of contractor, subcontractor, or management contractor for at least the last 5 years as specified in ITB 2.4.1.\\n9. **Specific Construction Experience**: The Bidder must have experience of executing work of value of at least 25% of the estimated cost of the work in any one year during the last 5 financial years as specified in ITB 2.4.2.\\n10. **Minimum Requirement of Key Plant and Equipment**: The Bidder must demonstrate the capability to meet the minimum requirement of key plant and equipment as specified in ITB 2.5.\\n11. **Requirement of Technical Staff**: The Bidder must demonstrate the availability of qualified key personnel as specified in ITB 2.6.\\n\\nThese criteria are mentioned in Section 3: Evaluation and Qualification Criteria of the document.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Create a custom prompt template\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are an expert tender document analyzer. \n",
    "Carefully extract the exact relevant information based on the query from the given context. \n",
    "Follow these guidelines:\n",
    "1. Be precise and concise\n",
    "2. Avoid repeating sentences\n",
    "3. Focus on unique, key information\n",
    "4. If no relevant information is found, respond with \"No specific information found\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Query: {question}\n",
    "\n",
    "Extracted Information:\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=2048,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Update the chain to use the custom prompt template\n",
    "        self.chain = load_qa_chain(\n",
    "            self.llm, \n",
    "            chain_type='stuff', \n",
    "            prompt=self.prompt_template\n",
    "        )\n",
    "        \n",
    "        self.queries = {\n",
    "            \"Extract the most critical pre-qualification or eligibility criteria mentioned in the document.\": \"Prequalification Criteria\",\n",
    "            # Add more queries as needed\n",
    "        }\n",
    "        self.request_count = 0 \n",
    "        \n",
    "        \n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            # print(\"text:::::::\",text)\n",
    "        \n",
    "        # Split into sentences and create chunks\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        # print(\"sentences:::::::\",sentences)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        # print(\"chunks:::::::\",chunks)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 1000) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text with improved extraction\"\"\"\n",
    "        try:\n",
    "            self.request_count += 1  # Increment the request counter\n",
    "            \n",
    "            # Print the current request details\n",
    "            print(f\"Request {self.request_count}:\")\n",
    "            print(f\"Query: {query}\")\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "                \n",
    "                # Post-process response to remove duplicates and improve clarity\n",
    "                unique_sentences = list(dict.fromkeys(response.split('. ')))\n",
    "                processed_response = '. '.join(unique_sentences)\n",
    "                \n",
    "            return processed_response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        \n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "            \"\"\"Main analysis function\"\"\"\n",
    "            # Process document\n",
    "            chunks = self.process_document(file_path)\n",
    "            # print(\"chunks::::\",chunks)\n",
    "            combined_text = \" \".join(chunks)\n",
    "            # print(\"combined_text::::\",combined_text)\n",
    "            \n",
    "            # Process queries in parallel\n",
    "            results = {}\n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, combined_text): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results[title] = response\n",
    "                    except Exception as e:\n",
    "                        results[title] = f\"Error: {str(e)}\"\n",
    "            \n",
    "            return results\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file = \"/data/Pqmatch/testing/78804029/78804029.txt\"\n",
    "    \n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1:\n",
      "Query: Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\n",
      "{\n",
      "    \"Prequalification Criteria\": \"Here are the clauses that specify Pre-Qualification Criteria or eligibility criteria:\\n\\n**Section 3 - Evaluation and Qualification Criteria**\\n\\n1. **Nationality** (Clause 2.1.1): The Bidder must meet the requirement of nationality as specified in ITB 4.2.\\n2. **Conflict of Interest** (Clause 2.1.2): The Bidder must not have any conflict of interest as specified in ITB 4.3.\\n3. **ADB Eligibility** (Clause 2.1.3): The Bidder must meet the eligibility criteria specified in ITB 4.4.\\n4. **Government-owned Entity** (Clause 2.1.4): The Bidder must meet the requirements specified in ITB 4.5.\\n5. **UN Eligibility** (Clause 2.1.5): The Bidder must meet the eligibility criteria specified in ITB 4.6.\\n6. **Pending Litigation** (Clause 2.2.1): The Bidder must not have any pending litigation as specified in ITB 4.7.\\n7. **Financial Situation** (Clause 2.3.1): The Bidder must submit audited balance sheets or other financial statements as specified in ITB 4.8.\\n8. **Average Annual Construction Turnover** (Clause 2.3.2): The Bidder must meet the minimum average annual construction turnover as specified in ITB 4.9.\\n9. **Financial Resources** (Clause 2.3.3): The Bidder must demonstrate access to or availability of liquid assets, lines of credit, or other financial resources as specified in ITB 4.10.\\n10. **Experience** (Clause 2.4.1): The Bidder must have experience under construction contracts as specified in ITB 4.11.\\n11. **Specific Construction Experience** (Clause 2.4.2): The Bidder must have experience in contracts of similar size and nature as specified in ITB 4.12.\\n12. **General Construction Experience** (Clause 2.4.1): The Bidder must have experience under construction contracts as specified in ITB 4.11.\\n13. **Minimum Requirement of Key Plant and Equipment** (Clause 2.5): The Bidder must demonstrate availability of minimum key equipment as specified in ITB 4.13.\\n14. **Technical Staff** (Clause 2.6): The Bidder must demonstrate availability of qualified key personnel as specified in ITB 4.14.\\n\\n**Section 4 - Bidding Forms**\\n\\n1. **Bidder\\u2019s Information Sheet** (Form ELI-1): The Bidder must provide information about its legal name, country of constitution, year of constitution, and authorized representative.\\n2. **JV Information Sheet** (Form ELI-2): The Bidder must provide information about its JV partners, including their legal name, country of constitution, year of constitution, and authorized representative.\\n3. **Pending Litigation** (Form LIT): The Bidder must provide information about any pending litigation.\\n4. **Financial Situation** (Form FIN-1): The Bidder must provide financial data for the last 3 years, including total assets, total liabilities, net worth, current assets, and current liabilities.\\n5. **Average Annual Construction Turnover** (Form FIN-2): The Bidder must provide information about its average annual construction turnover.\\n6. **Financial Resources** (Form FIN-3): The Bidder must provide information about its financial resources, including liquid assets, lines of credit, and other financial resources.\\n7. **Current Contract Commitments** (Form FIN-4): The Bidder must provide information about its current contract commitments.\\n8. **General Construction Experience** (Form EXP-1): The Bidder must provide information about its general construction experience.\\n9. **Specific Construction Experience** (Form EXP-2): The Bidder must provide information about its specific construction experience.\\n10. **Bank Certificate** (Form BANK CERTIFICATE): The Bidder must provide a certificate from its bank stating that it is a reputed company with a good financial standing.\\n\\n**Section 5 - Employer\\u2019s Requirements**\\n\\n1. **Project Location** (Clause 1.1): The project is located in the Pauri Garhwal district of Uttarakhand.\\n2. **Project Road Details** (Clause 1.2): The project road details are specified in Table 1.1.\\n3. **Land Use** (Clause 1.3): The land use is available.\\n4. **Climate** (Clause 1.4): The climate is pleasant in summer and monsoon.\\n5. **Terrain** (Clause 1.5): The terrain is hilly.\\n6. **Roadside Development and Villages** (Clause 1.6): The roadside development and villages are specified in the project documents.\\n7. **Right of Way (ROW\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "def get_embeddings_via_api(sentence):\n",
    "    \"\"\"Get embeddings from API (using all-mpnet-base-v2 model)\"\"\"\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"input\": [sentence]}\n",
    "    )\n",
    "    return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "             model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            # \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\"\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\"\n",
    "            # \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            # \"List of all the dates mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD amount,tender fee, tender value\": \"Important Dates\",\n",
    "            # \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.request_count = 0 \n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Split into sentences and create chunks\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        semantic_chunks = self._create_chunks(sentences)\n",
    "        chunked_texts, all_used_indices = self._chunk_by_tokens(semantic_chunks)\n",
    "        return chunked_texts\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        # Remove exact duplicates by using a set to track unique sentences\n",
    "        unique_sentences = []\n",
    "        seen_sentences = set()  # Set to track seen sentences (exact match)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # If the sentence hasn't been seen before, add it\n",
    "            if sentence['sentence'] not in seen_sentences:\n",
    "                unique_sentences.append(sentence)\n",
    "                seen_sentences.add(sentence['sentence'])\n",
    "        return self._combine_sentences(sentences)\n",
    "    \n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = [get_embeddings_via_api(s['combined_sentence']) for s in sentences]\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1])\n",
    "            )\n",
    "            distances.append(1 - similarity)\n",
    "\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = {\n",
    "                    'chunk': ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]]),\n",
    "                    'indices': [s['index'] for s in sentences[start_idx:i + 1]]  # Track indices\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        if start_idx < len(sentences):\n",
    "            chunk = {\n",
    "                'chunk': ' '.join([s['sentence'] for s in sentences[start_idx:]]),\n",
    "                'indices': [s['index'] for s in sentences[start_idx:]]  # Track indices\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "        return len(text.split()) * 1.3  # Rough estimate of tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _chunk_by_tokens(self, semantic_chunks: List[Dict[str, Any]], max_tokens: int = 1000) -> (List[str], set):\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        all_used_indices = set()  # To track used indices\n",
    "\n",
    "        for chunk in semantic_chunks:\n",
    "            estimated_tokens = self._estimate_tokens(chunk['chunk'])  # Use the proper function to estimate tokens\n",
    "\n",
    "            if current_tokens + estimated_tokens > max_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [chunk['chunk']]\n",
    "                current_tokens = estimated_tokens\n",
    "                all_used_indices.update(chunk['indices'])  # Keep track of indices in the current chunk\n",
    "            else:\n",
    "                current_chunk.append(chunk['chunk'])\n",
    "                current_tokens += estimated_tokens\n",
    "                all_used_indices.update(chunk['indices'])  # Add current chunk's indices to the set\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        # Return final chunks along with the set of all used indices\n",
    "        return chunks, all_used_indices\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 1000) -> List[str]:\n",
    "    #     \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "    #     chunks = []\n",
    "    #     current_chunk = []\n",
    "    #     current_tokens = 0\n",
    "    #     all_used_indices = set()  # To track used indices\n",
    "        \n",
    "    #     for chunk in semantic_chunks:\n",
    "    #         estimated_tokens = estimate_tokens(chunk['chunk'])\n",
    "            \n",
    "    #         if current_tokens + estimated_tokens > max_chunk_tokens:\n",
    "    #             if current_chunk:\n",
    "    #                 chunks.append(\" \".join(current_chunk))\n",
    "    #             current_chunk = [chunk['chunk']]\n",
    "    #             current_tokens = estimated_tokens\n",
    "    #             all_used_indices.update(chunk['indices'])  # Keep track of indices in the current chunk\n",
    "    #         else:\n",
    "    #             current_chunk.append(chunk['chunk'])\n",
    "    #             current_tokens += estimated_tokens\n",
    "    #             all_used_indices.update(chunk['indices'])  # Add current chunk's indices to the set\n",
    "        \n",
    "    #     if current_chunk:\n",
    "    #         chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "    #     # Return final chunks along with the set of all used indices\n",
    "    #     return chunks, all_used_indices\n",
    "    \n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            self.request_count += 1  # Increment the request counter\n",
    "            \n",
    "            # Print the current request details\n",
    "            print(f\"Request {self.request_count}:\")\n",
    "            print(f\"Query: {query}\")\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document\n",
    "        chunks = self.process_document(file_path)\n",
    "        # print(\"chunks::::\",chunks)\n",
    "        combined_text = \" \".join(chunks)\n",
    "        # print(\"combined_text::::\",combined_text)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        results = {}\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, combined_text): title\n",
    "                for query, title in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] = response\n",
    "                except Exception as e:\n",
    "                    results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file = \"/data/Pqmatch/testing/78804029/78804029.txt\"\n",
    "    \n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/QAAPI/qa/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1:\n",
      "Query: Analyze the document and extract ONLY the functional requirements exactly as they are mentioned in the tender document. Provide a precise and concise list of requirements without adding any external information or interpretation.\n",
      "Request 2:\n",
      "Query: Extract ONLY the pre-qualification or eligibility criteria directly stated in the tender document. Do not include any additional context or explanations beyond what is explicitly written.\n",
      "Request 3:\n",
      "Query: List ONLY the supporting documents that are explicitly required in the tender document. Include no additional commentary or external suggestions.\n",
      "Request 4:\n",
      "Query: Extract ALL dates mentioned in the tender document, including but not limited to: Bid submission end date, Opening date, Closing date, Pre-bid meeting date, EMD amount, Tender fee, and Tender value. Provide the exact dates as they appear in the document.\n",
      "Request 5:\n",
      "Query: Extract the EXACT contact details of the officer mentioned in the document. This should include only the name, email ID, and contact number as they are written in the original text.\n",
      "{\n",
      "    \"Scope of Work\": \"Here are the functional requirements extracted from the document:\\n\\n1. The work involves supplying, installation, testing, and commissioning of a 58.5 KVA DG set in an existing building of HUDCO Bhawan Rail Head Complex, P.O. Railway Station, Jammu.\\n2. The DG set shall be of silent type with prime power rating of 58.5 KVA, 415 volts at 1500 RPM, 0.8 lagging power factor at 415 V suitable for 50 Hz, 3-phase system & for 0.85 Load factor as per CPCB-IV Compliance.\\n3. The DG set shall consist of a diesel engine, alternator, base frame, foundation, fuel tank, exhaust system, starting system, acoustic and weatherproof enclosure, and automatic mains failure control panel.\\n4. The diesel engine shall be a multi-cylinder, vertical 4-stroke cycle, air/water cooled, diesel engine developing required B.H.P. as per BS 649/1958 with upto date amendments under SITE CONDITIONS for giving a continuous output of specified KVA at the load terminals of alternator, exclusive of the power requirements of auxiliaries while running at 1500 rpm.\\n5. The engine shall be with 10% overload capacity of one hour in any period of 12 hours continuous run.\\n6. The engine shall be fitted complete with all the required accessories.\\n7. The alternator shall be rated for a continuous output of specified KVA (NTP) in BOQ 0.8 P.F. or better, at 415 Volts, 3-phase, 50 cycles suitable for the 4-wire system exclusive of power requirement of auxiliaries.\\n8. The alternator shall be star connected and neutral shall be brought out through a separate terminal and will be solidly grounded.\\n9. The alternator shall have a speed of the engine and shall match the engine for a direct drive.\\n10. The alternator shall be self-excited, self-regulated, and preferably with static excitation facility.\\n11. The alternator shall have a rectifier suitable for operation at high ambient temperature at site.\\n12. The alternator shall be in accordance with the following standards as are applicable: IS:4722-68/BS-2613-1970, IS:4889-68/BS-269 rules for method of declaring efficiency of electrical machine, IS:4889-68/BS-269 rules for method of declaring efficiency of electrical machine.\\n13. The alternator shall have a voltage regulation from no load to rated load within a range of 5% of rated voltage.\\n14. The alternator shall have a frequency regulation from no load to full load within a band of 1% of rated frequency.\\n15. The alternator shall have a voltage dip for any addition of load up to and including 90% load not exceeding 20% of rated voltage and shall recover to and remain within the steady band within not more than 1.5 sec.\\n16. The alternator shall have a frequency recovery to the steady state frequency band within 5 seconds.\\n17. The windings shall not develop hot spots/pockets exceeding safe limits due to an imbalance of 25% between any two phases from no load to full load.\\n18. The alternator shall have an automatic voltage regulator system compatible with excitation system described above to furnish a performance as defined herein under all condition of load.\\n19. The alternator shall have a manual rheostatic control or an equivalent alternative to vary the set point from 400 to 433 volts may be incorporated on the regulator panel or control panel.\\n20. The alternator shall have a terminal box suitable for necessary PVC 1.1 KV grade underground cable confirming specification to IS1554 part-I with sufficient space for bifurcation inside the box, necessary removable gland plate, cable lugs, connections shall all be included.\\n21. The alternator shall have a terminal box suitable for withstanding the mechanical and thermal stresses developed due to any short circuit at the terminals.\\n22. The alternator shall have two earth terminals on opposite side with vibration proof connections, non-ferrous hardware etc. with galvanized or plate and passivated washers of minimum size 12 mm dia.\\n23. The alternator shall have a class of insulation as per standard manufacturer product.\\n24. The control panel shall be fabricated out of sheet steel, totally enclosed, dust, damp and vermin proof free standing floor mounted type and front of board operated.\\n25. The control panel shall have a degree of protection required will be IP 42 confirming to IS 2147-1962.\\n26. The control panel shall have one No. 25 mm x 5 mm copper strip run at the rear of the board/connecting all the sections suitable bonding to earth.\\n27. The control panel shall have earth terminals shall be vibration proof with all hardware of non-ferrous or galvanized/plated and passivated in case of ferrous hardware.\\n28\",\n",
      "    \"Prequalification Criteria\": \"1. Contractors who fulfill the following requirements shall be eligible to apply:\\n   a) CPWD Registered contractors:\\n      i) Should have satisfactorily completed the works as mentioned below during the last Seven years ending previous day of last date of submission of tenders:\\n         Three similar works each costing not less than Rs. 4.78 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n         OR\\n         Two similar works each costing not less than Rs. 7.17 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n         OR\\n         One similar work costing not less than Rs. 9.56 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n      ii) The Building & Road category contractor is also be eligible to carry out himself/herself without associating any specialized agency provided:\\n         a) He fulfills the prescribed eligibility criteria respectively for these work(s).\\n         OR\\n         b) He directly procures the equipment of approved make from manufacturer and gets it installed from authorized agency/service provider of the manufacturer.\\n         OR\\n         c) Through Specialized agency who fulfills the eligible criteria as mentioned above.\\n   b) Non CPWD / Specialized agencies:\\n      i) Should have had average annual financial turnover of Rs. 3.59 Lakh on construction works during the last three consecutive years ending March-2023.\\n      ii) Should not have incurred any loss (profit after tax should be positive) in more than two years during available last five consecutive balance sheets ending March-2024.\\n      iii) Should have a solvency (Banker\\u2019s certificates from commercial bank) of Rs. 4.78 Lakh.\\n      OR\\n      Networth certificate of minimum of Rs. 1.20 Lakh issued by certified CA with UDIN.\\n\\n2. The bidder should have bidding capacity equal to or more than the estimated cost of the work put to tender.\\n\\n3. The bidder should have experience in executing similar works of magnitude specified below:\\n   a) Should have satisfactorily completed the works as mentioned below during the last Seven years ending previous day of last date of submission of tenders:\\n      Three similar works each costing not less than Rs. 4.78 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n      OR\\n      Two similar works each costing not less than Rs. 7.17 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n      OR\\n      One similar work costing not less than Rs. 9.56 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n\\n4. The bidder should have experience in executing similar works of magnitude specified below:\\n   a) Should have satisfactorily completed the works as mentioned below during the last Seven years ending previous day of last date of submission of tenders:\\n      Three similar works each costing not less than Rs. 4.78 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n      OR\\n      Two similar works each costing not less than Rs. 7.17 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n      OR\\n      One similar work costing not less than Rs. 9.56 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n\\n5. The bidder should have experience in executing similar works of magnitude specified below:\\n   a) Should have satisfactorily completed the works as mentioned below during the last Seven years ending previous day of last date of submission of tenders:\\n      Three similar works each costing not less than Rs. 4.78 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\\n      OR\\n      Two similar works each costing not less than Rs. 7.17 lakh. put to tender with capacity of individual DG Set being 80% of individual capacity (rounded off to next available higher capacity) of the equipment i.e. DG Set.\",\n",
      "    \"Supporting Documents\": \"1. Form CPWD 6\\n2. Letter of Transmittal\\n3. Financial Information (Form-A)\\n4. Banker\\u2019s Certificate From A Scheduled Bank (Form \\u2018B\\u2019)\\n5. Form for certificate of Networth From Chartered Accountant (Form \\u2018B-1\\u2019)\\n6. Details of eligible similar nature of works completed during the last seven years ending previous day of last submission of tenders (Form \\u2018C\\u2019)\\n7. Calculation of Bidding Capacity (Form-C-3)\\n8. Performance report of work (Form-D)\\n9. Assessment of quality (Form D-1)\\n10. Structure and Organization (Form-E)\\n11. Form-F\\n12. Receipt of original EMD\\n13. Section-1: Brief Particular of work\\n14. CPWD-7\\n15. Form Of Bank Guarantee for Earnest Money Deposit/Performance Guarantee/ Security Deposit/Mobilization Advance/Refund of milestone with bank\\n16. ELIGIBILTY TO EXECUTE SPECIALISED COMPONENT OF THE WORK\\n17. Form AA: - Consent letter from eligible associate agency\\n18. Form BB: - Affidavit of Memorandum of Understanding\\n19. Form CC: - WILLINGNESS CERTIFICATE\\n20. Form DD: - Details of all works of similar class\\n21. Undertaking regarding Electrical License\\n22. List of Field Tests\\n23. Field Testing Equipment and Instruments\\n24. Minimum required T&P\\n25. PROFORMA FOR TESTS CARRIED OUT\\n26. CEMENT/PAINT REGISTER\\n27. List of Preferred Makes for Civil Works\\n28. Schedule of Quantities\\n29. Form of Performance Security\\n30. Guarantee for Water Proofing Treatment\\n31. Guarantee Bond for Water Supply, Sanitary Installations and drainage\\n32. Guarantee Bond for removal of defects in respect of stone/tile work\\n33. Guarantee Bond for Aluminum Work\\n34. Authority letter\\n35. List of Field Tests, Field Testing equipments & instruments, List of T&P.\\n36. Performa for Test carried out, Performa for Cement/Paint registers.\\n37. List of approved makes of Materials, Fixtures & Fittings etc.\\n38. Circular GCC - 2023\\n39. Schedule of Quantities\",\n",
      "    \"Important Dates\": \"Here are the dates mentioned in the tender document:\\n\\n1. 10-12-2024 (Pre-bid conference date)\\n2. 24-12-2024 (Bid submission end date)\\n3. 15:00 Hrs (Bid submission end time)\\n4. 15:30 Hrs (Bid opening date and time)\\n5. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n6. 10-12-2024 (Pre-bid conference date)\\n7. 24-12-2024 (Bid submission end date)\\n8. 15:00 Hrs (Bid submission end time)\\n9. 15:30 Hrs (Bid opening date and time)\\n10. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n11. 24-12-2024 (Bid submission end date)\\n12. 15:00 Hrs (Bid submission end time)\\n13. 15:30 Hrs (Bid opening date and time)\\n14. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n15. 24-12-2024 (Bid submission end date)\\n16. 15:00 Hrs (Bid submission end time)\\n17. 15:30 Hrs (Bid opening date and time)\\n18. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n19. 24-12-2024 (Bid submission end date)\\n20. 15:00 Hrs (Bid submission end time)\\n21. 15:30 Hrs (Bid opening date and time)\\n22. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n23. 24-12-2024 (Bid submission end date)\\n24. 15:00 Hrs (Bid submission end time)\\n25. 15:30 Hrs (Bid opening date and time)\\n26. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n27. 24-12-2024 (Bid submission end date)\\n28. 15:00 Hrs (Bid submission end time)\\n29. 15:30 Hrs (Bid opening date and time)\\n30. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n31. 24-12-2024 (Bid submission end date)\\n32. 15:00 Hrs (Bid submission end time)\\n33. 15:30 Hrs (Bid opening date and time)\\n34. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n35. 24-12-2024 (Bid submission end date)\\n36. 15:00 Hrs (Bid submission end time)\\n37. 15:30 Hrs (Bid opening date and time)\\n38. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n39. 24-12-2024 (Bid submission end date)\\n40. 15:00 Hrs (Bid submission end time)\\n41. 15:30 Hrs (Bid opening date and time)\\n42. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n43. 24-12-2024 (Bid submission end date)\\n44. 15:00 Hrs (Bid submission end time)\\n45. 15:30 Hrs (Bid opening date and time)\\n46. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n47. 24-12-2024 (Bid submission end date)\\n48. 15:00 Hrs (Bid submission end time)\\n49. 15:30 Hrs (Bid opening date and time)\\n50. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n51. 24-12-2024 (Bid submission end date)\\n52. 15:00 Hrs (Bid submission end time)\\n53. 15:30 Hrs (Bid opening date and time)\\n54. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n55. 24-12-2024 (Bid submission end date)\\n56. 15:00 Hrs (Bid submission end time)\\n57. 15:30 Hrs (Bid opening date and time)\\n58. 21/EE(E)/JED/2024-25/2nd Call (Tender ID)\\n59. 24-12-2024 (Bid submission end date)\\n60. 15:00 Hrs (Bid submission end time)\\n61\",\n",
      "    \"Contact Details\": \"The officer mentioned in the document is:\\n\\nExecutive Engineer (Electrical), Jammu Electrical Division, CPWD, Satwari, Jammu.\\n\\nThe exact contact details of the officer are not explicitly mentioned in the document.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"Analyze the document and extract ONLY the functional requirements exactly as they are mentioned in the tender document. Provide a precise and concise list of requirements without adding any external information or interpretation.\": \"Scope of Work\",\n",
    "            \"Extract ONLY the pre-qualification or eligibility criteria directly stated in the tender document. Do not include any additional context or explanations beyond what is explicitly written.\": \"Prequalification Criteria\",\n",
    "            \"List ONLY the supporting documents that are explicitly required in the tender document. Include no additional commentary or external suggestions.\": \"Supporting Documents\",\n",
    "            \"Extract ALL dates mentioned in the tender document, including but not limited to: Bid submission end date, Opening date, Closing date, Pre-bid meeting date, EMD amount, Tender fee, and Tender value. Provide the exact dates as they appear in the document.\": \"Important Dates\",\n",
    "            \"Extract the EXACT contact details of the officer mentioned in the document. This should include only the name, email ID, and contact number as they are written in the original text.\": \"Contact Details\"\n",
    "        }\n",
    "        self.request_count = 0 \n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Split into sentences \n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences\"\"\"\n",
    "        # Use regex to split sentences, but handle common abbreviations\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "        \n",
    "        # Remove empty and whitespace-only sentences\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        return [{'sentence': s, 'index': i} for i, s in enumerate(sentences)]\n",
    "    \n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 2) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current sentence\n",
    "            context.append(sent['sentence'])\n",
    "            # Add next sentences\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            \n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "    \n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        if not sentences:\n",
    "            return []\n",
    "\n",
    "        # Combine sentences with context\n",
    "        sentences_with_context = self._combine_sentences(sentences)\n",
    "\n",
    "        # Encode sentences with context\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences_with_context])\n",
    "        \n",
    "        # Calculate pairwise distances\n",
    "        distances = [\n",
    "            1 - np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            for i in range(len(embeddings) - 1)\n",
    "        ]\n",
    "        \n",
    "        # Use adaptive thresholding\n",
    "        threshold = np.percentile(distances, 90)  # Slightly lower threshold for more granular chunking\n",
    "        \n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "\n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold or i == len(distances) - 1:\n",
    "                # Collect sentences for this chunk\n",
    "                chunk_sentences = [\n",
    "                    sent['sentence'] for sent in sentences_with_context[start_idx:i+1]\n",
    "                ]\n",
    "                \n",
    "                # Join sentences into a chunk\n",
    "                if chunk_sentences:\n",
    "                    chunks.append(' '.join(chunk_sentences))\n",
    "                \n",
    "                start_idx = i + 1\n",
    "\n",
    "        return chunks\n",
    "        \n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 1000) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            self.request_count += 1  # Increment the request counter\n",
    "            \n",
    "            # Print the current request details\n",
    "            print(f\"Request {self.request_count}:\")\n",
    "            print(f\"Query: {query}\")\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document\n",
    "        chunks = self.process_document(file_path)\n",
    "        combined_text = \" \".join(chunks)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        results = {}\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, combined_text): title\n",
    "                for query, title in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] = response\n",
    "                except Exception as e:\n",
    "                    results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file = \"/data/Pqmatch/testing/78804029/78804029.txt\"\n",
    "    \n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vaishnavi more code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Union\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os \n",
    "\n",
    "# # Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=512,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            # \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"List of all the  important dates and times mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD amount,tender fee, tender value\":\"Importants Date\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.max_chunk_tokens = 100000  # Safe limit below model's maximum\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        \n",
    "        # Remove exact duplicates by using a set to track unique sentences\n",
    "        unique_sentences = []\n",
    "        seen_sentences = set()  # Set to track seen sentences (exact match)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # If the sentence hasn't been seen before, add it\n",
    "            if sentence['sentence'] not in seen_sentences:\n",
    "                unique_sentences.append(sentence)\n",
    "                seen_sentences.add(sentence['sentence'])\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        embeddings = [get_embeddings_via_api(s['combined_sentence']) for s in sentences]\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1])\n",
    "            )\n",
    "            distances.append(1 - similarity)\n",
    "\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = {\n",
    "                    'chunk': ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]]),\n",
    "                    'indices': [s['index'] for s in sentences[start_idx:i + 1]]  # Track indices\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        if start_idx < len(sentences):\n",
    "            chunk = {\n",
    "                'chunk': ' '.join([s['sentence'] for s in sentences[start_idx:]]),\n",
    "                'indices': [s['index'] for s in sentences[start_idx:]]  # Track indices\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "        return len(text.split()) * 1.3  # Rough estimate of tokens\n",
    "\n",
    "    def _chunk_by_tokens(self, semantic_chunks: List[str]) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        all_used_indices = set()  # To track used indices\n",
    "        \n",
    "        for chunk in semantic_chunks:\n",
    "            estimated_tokens = estimate_tokens(chunk['chunk'])\n",
    "            \n",
    "            if current_tokens + estimated_tokens > max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [chunk['chunk']]\n",
    "                current_tokens = estimated_tokens\n",
    "                all_used_indices.update(chunk['indices'])  # Keep track of indices in the current chunk\n",
    "            else:\n",
    "                current_chunk.append(chunk['chunk'])\n",
    "                current_tokens += estimated_tokens\n",
    "                all_used_indices.update(chunk['indices'])  # Add current chunk's indices to the set\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        # Return final chunks along with the set of all used indices\n",
    "        return chunks, all_used_indices\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            sentences = self._split_into_sentences(text)\n",
    "            semantic_chunks = self._create_chunks(sentences)\n",
    "            chunked_texts, all_used_indices = self._chunk_by_tokens(semantic_chunks)\n",
    "            return chunked_texts\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # def _merge_responses(self, responses: List[str]) -> str:\n",
    "    #     \"\"\"Merge multiple responses into a coherent summary\"\"\"\n",
    "    #     # Remove duplicates while maintaining order\n",
    "    #     unique_lines = []\n",
    "    #     seen = set()\n",
    "    #     for response in responses:\n",
    "    #         for line in response.split('\\n'):\n",
    "    #             line = line.strip()\n",
    "    #             if line and line not in seen:\n",
    "    #                 seen.add(line)\n",
    "    #                 unique_lines.append(line)\n",
    "        \n",
    "    #     return '\\n'.join(unique_lines)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _merge_responses(self, responses: List[str]) -> str:\n",
    "                                                \"\"\"Merge multiple responses into a coherent summary and remove duplicates.\"\"\"\n",
    "        unique_lines = []\n",
    "        seen = set()\n",
    "\n",
    "        for response in responses:\n",
    "            for line in response.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and line not in seen:\n",
    "                    seen.add(line)\n",
    "                    unique_lines.append(line)\n",
    "        \n",
    "        # Additional deduplication for repeated sections within a single line\n",
    "        cleaned_lines = []\n",
    "        for line in unique_lines:\n",
    "            parts = line.split()  # Tokenize the line\n",
    "            deduped_line = \" \".join(dict.fromkeys(parts))  # Remove repeated words\n",
    "            cleaned_lines.append(deduped_line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "\n",
    "\n",
    "    def process_query(self, query: str, chunks: List[str]) -> str:\n",
    "        \"\"\"Process a single query against multiple text chunks\"\"\"\n",
    "        try:\n",
    "            responses = []\n",
    "            for chunk in chunks:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = self.chain.run(\n",
    "                        input_documents=[Document(page_content=chunk)],\n",
    "                        question=query\n",
    "                    )\n",
    "                    responses.append(response.strip())\n",
    "            \n",
    "            return self._merge_responses(responses)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        try:\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, chunks): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process all text files in the given folder and its subfolders, and index results into OpenSearch.\"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "\n",
    "    # Set up OpenSearch client\n",
    "    index_name = 'tprocanswers'\n",
    "    opensearch_client = OpenSearch(\n",
    "        hosts=['https://localhost:9200'],\n",
    "        http_auth=(\"admin\", \"4Z*lwtz,,2T:0TGu\"),\n",
    "        use_ssl=True,\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        future_to_file = {}\n",
    "        \n",
    "        for root, _, files in os.walk(base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    future_to_file[executor.submit(analyzer.analyze_tender, file_path)] = file_path\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                results = future.result()\n",
    "                tcno = os.path.basename(os.path.dirname(file_path))\n",
    "                all_results.append({\n",
    "                    \"tcno\": tcno,\n",
    "                    # \"file_path\": file_path,\n",
    "                    \"results\": results\n",
    "                })\n",
    "                \n",
    "                # Index results into OpenSearch\n",
    "                opensearch_client.index(index=index_name, id=tcno, body={\"file_path\": file_path, \"results\": results})\n",
    "                print(f\"Indexed results for {tcno} in OpenSearch.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                all_results.append({\n",
    "                    \"tcno\": \"Unknown\",\n",
    "                    # \"file_path\": file_path,\n",
    "                    \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "                })\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Base folder path\n",
    "    date_str = \"22-11-24\"  # You can modify this as needed\n",
    "    folder_path = f\"/data/txtfolder/dailydocument_23-11-24_txt\"\n",
    "    \n",
    "    # Process all documents in the folder\n",
    "    results = process_folder(folder_path)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "             model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff',verbose=True)\n",
    "        self.queries = {\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Split into sentences and create chunks\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 3500) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        unique_chunks = set()  # Track unique chunks\n",
    "        \n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            \n",
    "            # Add only unique chunks\n",
    "            for chunk in text_chunks:\n",
    "                if chunk not in unique_chunks:\n",
    "                    chunks.append(chunk)\n",
    "                    unique_chunks.add(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return self._remove_duplicates(response.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def _remove_duplicates(self, text: str) -> str:\n",
    "        \"\"\"Remove duplicate points from the response\"\"\"\n",
    "        # Split the text into points\n",
    "        points = [point.strip() for point in re.split(r'\\n|\\d+\\.', text) if point.strip()]\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_points = []\n",
    "        seen = set()\n",
    "        for point in points:\n",
    "            # Normalize point by lowercasing and removing extra whitespace\n",
    "            normalized_point = ' '.join(point.lower().split())\n",
    "            \n",
    "            if normalized_point not in seen:\n",
    "                unique_points.append(point)\n",
    "                seen.add(normalized_point)\n",
    "        \n",
    "        # Reconstruct the text with unique points\n",
    "        return '\\n'.join(f\"{i+1}. {point}\" for i, point in enumerate(unique_points))\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document\n",
    "        chunks = self.process_document(file_path)\n",
    "        combined_text = \" \".join(chunks)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        results = {}\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, combined_text): title\n",
    "                for query, title in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] = response\n",
    "                except Exception as e:\n",
    "                    results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file = \"/data/Pqmatch/testing/78804029/78804029.txt\"\n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store response in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     # Process tender document\n",
    "#     input_file = \"/data/txtfolder/dailydoc_test/70398187/70398187.txt\"\n",
    "    \n",
    "#     # Analyze and get results\n",
    "#     results = analyze_tender_document(input_file)\n",
    "    \n",
    "#     # Convert the results to a DataFrame\n",
    "#     results_df = pd.DataFrame(list(results.items()), columns=[\"Key\", \"Value\"])\n",
    "\n",
    "#     # Save the results to an Excel file\n",
    "#     output_file = \"tender_analysis_results.xlsx\"\n",
    "#     results_df.to_excel(output_file, index=False)\n",
    "\n",
    "#     # Print results (optional)\n",
    "#     print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    response = requests.post(\"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"sentence-transformers/all-mpnet-base-v2\", \"input\": [text]})\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['embedding']\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [get_embedding(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return get_embedding(text)\n",
    "\n",
    "\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model =self.model = CustomEmbeddings()\n",
    "        self.llm = ChatOpenAI(\n",
    "             model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Split into sentences and create chunks\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 3500) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        # Process document\n",
    "        chunks = self.process_document(file_path)\n",
    "        combined_text = \" \".join(chunks)\n",
    "        \n",
    "        # Process queries in parallel\n",
    "        results = {}\n",
    "        with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.process_query, query, combined_text): title\n",
    "                for query, title in self.queries.items()\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_query):\n",
    "                title = future_to_query[future]\n",
    "                try:\n",
    "                    response = future.result()\n",
    "                    results[title] = response\n",
    "                except Exception as e:\n",
    "                    results[title] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "def analyze_tender_document(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Top-level function to analyze a tender document\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the tender document\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary of analysis results\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    return analyzer.analyze_tender(file_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Process tender document\n",
    "    input_file =    input_file = \"/data/Pqmatch/testing/78804029/78804029.txt\"\n",
    "    \n",
    "    # Analyze and get results\n",
    "    results = analyze_tender_document(input_file)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import multiprocessing\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            # \"Identify the functional requirements, also referred to as the scope of work, specified in the document.\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\"\n",
    "        #     \"List all mandatory qualification criteria, including blacklisting status and required certifications.\": \"Mandatory Qualification Criteria\",\n",
    "        #     \"Summarize the work specifications that bidders must meet to fulfill the tender requirements.\": \"Specifications\",\n",
    "        #     \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "        #     \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "        #     \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        if not text:  # Skip empty files\n",
    "            print(f\"Empty file: {file_path}\")\n",
    "            return []\n",
    "\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        return self._chunk_by_tokens(chunks)\n",
    "\n",
    "\t\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i}\n",
    "                     for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "\n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "\n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 3500) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars]\n",
    "                           for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> Dict[str, str]:\n",
    "      \"\"\"Analyze a single tender document\"\"\"\n",
    "      chunks = self.process_document(file_path)\n",
    "      if not chunks:  # Skip empty documents\n",
    "        print(f\"No content to process in: {file_path}\")\n",
    "        return {title: \"No content\" for title in self.queries.values()}\n",
    "\n",
    "      results = {title: \"\" for title in self.queries.values()}\n",
    "      query_tasks = {}\n",
    "\n",
    "      with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for chunk in chunks:\n",
    "            for query, title in self.queries.items():\n",
    "                future = executor.submit(self.process_query, query, chunk)\n",
    "                query_tasks[future] = title\n",
    "\n",
    "        for future in as_completed(query_tasks):\n",
    "            title = query_tasks[future]\n",
    "            try:\n",
    "                response = future.result()\n",
    "                results[title] += f\"{response}\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query for {title}: {e}\")\n",
    "\n",
    "      return results\n",
    "\n",
    "\n",
    "\n",
    "def process_folder(tcno):\n",
    "    \"\"\"Process a single subfolder containing .txt files\"\"\"\n",
    "    try:\n",
    "        folder_path = f\"/data/txtfolder/dailydoc_test/{tcno}\"\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Folder not found: {folder_path}\")\n",
    "            return\n",
    "\n",
    "        analyzer = TenderAnalyzer()\n",
    "        folder_results = []\n",
    "\n",
    "        txt_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = {executor.submit(analyzer.analyze_tender, file): file for file in txt_files}\n",
    "            for future in as_completed(futures):\n",
    "                file_name = os.path.basename(futures[future])\n",
    "                try:\n",
    "                    analysis_result = future.result()\n",
    "                    folder_results.append({\"file_name\": file_name, \"analysis\": analysis_result})\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process file {file_name}: {e}\")\n",
    "\n",
    "        # Format response\n",
    "        json_response = {\n",
    "            \"tcno\": tcno,\n",
    "            \"results\": folder_results\n",
    "        }\n",
    "\n",
    "        print(f\"Processed folder: {tcno}\")\n",
    "        return json_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process folder {tcno}: {str(e)}\")\n",
    "\n",
    "\n",
    "def process_folders_in_parallel():\n",
    "    \"\"\"Process all subfolders in parallel\"\"\"\n",
    "    try:\n",
    "        base_folder_path = \"/data/txtfolder/dailydoc_test\"\n",
    "        if not os.path.exists(base_folder_path):\n",
    "            print(f\"Base folder not found: {base_folder_path}\")\n",
    "            return\n",
    "\n",
    "        tcno_folders = [tcno for tcno in os.listdir(base_folder_path) if os.path.isdir(os.path.join(base_folder_path, tcno))]\n",
    "\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            futures = {executor.submit(process_folder, tcno): tcno for tcno in tcno_folders}\n",
    "            for future in as_completed(futures):\n",
    "                tcno = futures[future]\n",
    "                try:\n",
    "                    folder_result = future.result()\n",
    "                    if folder_result:\n",
    "                        results.append(folder_result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process folder {tcno}: {e}\")\n",
    "\n",
    "        # Save results to a JSON file\n",
    "        output_path = f\"analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        print(f\"All results saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing folders: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_folders_in_parallel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Union\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            sentences = self._split_into_sentences(text)\n",
    "            chunks = self._create_chunks(sentences)\n",
    "            return self._chunk_by_tokens(chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str], max_tokens: int = 3500) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        max_chars = max_tokens * 2\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            text_chunks = [text[i:i + max_chars] \n",
    "                         for i in range(0, len(text), max_chars)]\n",
    "            chunks.extend(text_chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                response = self.chain.run(\n",
    "                    input_documents=[Document(page_content=text)],\n",
    "                    question=query\n",
    "                )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    # ... [Previous methods remain the same until analyze_tender] ...\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function with modified response format\"\"\"\n",
    "        try:\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            combined_text = \" \".join(chunks)\n",
    "            results = []\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, combined_text): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process all text files in the given folder and its subfolders in parallel.\n",
    "    \n",
    "    Args:\n",
    "        base_folder (str): Base folder path containing subfolders with text files\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing results for all processed files\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "    \n",
    "    def analyze_file(file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze a single file and return the result\"\"\"\n",
    "        try:\n",
    "            # Get tender number from folder name\n",
    "            tcno = os.path.basename(os.path.dirname(file_path))\n",
    "            \n",
    "            # Analyze the tender document\n",
    "            results = analyzer.analyze_tender(file_path)\n",
    "            \n",
    "            return {\n",
    "                \"tcno\": tcno,\n",
    "                \"file_path\": file_path,\n",
    "                \"results\": results\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"tcno\": \"Unknown\",\n",
    "                \"file_path\": file_path,\n",
    "                \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "            }\n",
    "    \n",
    "    # Create ThreadPoolExecutor to process each file in parallel\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:  # You can adjust max_workers based on your system's capacity\n",
    "        future_to_file = {}\n",
    "        \n",
    "        # Walk through all subfolders and submit tasks for files ending with .txt\n",
    "        for root, _, files in os.walk(base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    future_to_file[executor.submit(analyze_file, file_path)] = file_path\n",
    "        \n",
    "        # Collect the results from all futures as they complete\n",
    "        for future in as_completed(future_to_file):\n",
    "            result = future.result()\n",
    "            all_results.append(result)\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Base folder path\n",
    "    date_str = \"22-11-24\"  # You can modify this as needed\n",
    "    folder_path = f\"/data/txtfolder/dailydoc_test\"\n",
    "    \n",
    "    # Process all documents in the folder\n",
    "    results = process_folder(folder_path)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/QAAPI/qa/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 146481 tokens (145981 in the messages, 500 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 146495 tokens (145995 in the messages, 500 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 146487 tokens (145987 in the messages, 500 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 146477 tokens (145977 in the messages, 500 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 146490 tokens (145990 in the messages, 500 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tiktoken\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# Define the maximum token limit\n",
    "MAX_TOKENS = 131072\n",
    "\n",
    "# Function to calculate token count\n",
    "def calculate_token_count(text: str, model: str = 'meta-llama/Llama-3.1-8B-Instruct') -> int:\n",
    "    \"\"\"Calculate the number of tokens in a text.\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")  # For Llama models\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def chunk_by_tokens_with_limit(texts: List[str], max_tokens: int = MAX_TOKENS) -> List[str]:\n",
    "    \"\"\"Split texts into smaller chunks based on token count.\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for text in texts:\n",
    "        token_count = calculate_token_count(current_chunk + text)\n",
    "        \n",
    "        if token_count > max_tokens:\n",
    "            # If adding this text would exceed the limit, start a new chunk\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = text  # Start new chunk with current text\n",
    "        else:\n",
    "            current_chunk += text  # Add text to the current chunk\n",
    "    \n",
    "    if current_chunk:  # Add remaining chunk if any\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            sentences = self._split_into_sentences(text)\n",
    "            chunks = self._create_chunks(sentences)\n",
    "            \n",
    "            # Apply chunking based on token limits\n",
    "            return chunk_by_tokens_with_limit(chunks, max_tokens=MAX_TOKENS)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            # Add previous sentences\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            # Add current and next sentences\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        # Split into chunks\n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            # Split text into smaller chunks to avoid token limit errors\n",
    "            chunks = chunk_by_tokens_with_limit([text], max_tokens=MAX_TOKENS)\n",
    "            \n",
    "            full_response = \"\"\n",
    "            \n",
    "            # Process each chunk and combine the responses\n",
    "            for chunk in chunks:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = self.chain.run(\n",
    "                        input_documents=[Document(page_content=chunk)],\n",
    "                        question=query\n",
    "                    )\n",
    "                full_response += response.strip() + \" \"\n",
    "            \n",
    "            return full_response.strip()  # Combine all responses from chunks\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function with modified response format\"\"\"\n",
    "        try:\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            combined_text = \" \".join(chunks)\n",
    "            results = []\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, combined_text): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process all text files in the given folder and its subfolders\n",
    "    \n",
    "    Args:\n",
    "        base_folder (str): Base folder path containing subfolders with text files\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing results for all processed files\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "    \n",
    "    # Walk through all subfolders\n",
    "    for root, _, files in os.walk(base_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Get tender number from folder name\n",
    "                    tcno = os.path.basename(os.path.dirname(file_path))\n",
    "                    \n",
    "                    # Analyze the tender document\n",
    "                    results = analyzer.analyze_tender(file_path)\n",
    "                    \n",
    "                    # Add file information to results\n",
    "                    all_results.append({\n",
    "                        \"tcno\": tcno,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"results\": results\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    all_results.append({\n",
    "                        \"tcno\": tcno,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "                    })\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Base folder path\n",
    "    date_str = \"22-11-24\"  # You can modify this as needed\n",
    "    folder_path = f\"/data/txtfolder/dailydoc_test\"\n",
    "    \n",
    "    # Process all documents in the folder\n",
    "    results = process_folder(folder_path)\n",
    "    \n",
    "    # Print final results\n",
    "    print(f\"Processed {len(results['results'])} files.\")\n",
    "    for file_result in results['results']:\n",
    "        print(f\"Results for file {file_result['file_path']}:\")\n",
    "        for query_result in file_result[\"results\"]:\n",
    "            print(f\"{query_result['title']} -> {query_result['response']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tiktoken\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define constants\n",
    "MAX_TOKENS = 131072\n",
    "MAX_COMPLETION_TOKENS = 500  # Maximum tokens allowed for model's completion\n",
    "\n",
    "# Function to calculate token count\n",
    "def calculate_token_count(text: str) -> int:\n",
    "    \"\"\"Calculate the number of tokens in a text.\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")  # For Llama models\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def chunk_by_tokens(text: str, max_tokens: int = MAX_TOKENS) -> List[str]:\n",
    "    \"\"\"Chunk a long text into smaller chunks based on token limits.\"\"\"\n",
    "    tokens = tiktoken.get_encoding(\"cl100k_base\").encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    while len(tokens) > max_tokens:\n",
    "        # Find the cutoff point where token limit is reached\n",
    "        cutoff = max_tokens\n",
    "        while cutoff > 0 and tokens[cutoff] not in [b' ', b'.', b'?', b'!']:\n",
    "            cutoff -= 1\n",
    "        chunks.append(tiktoken.get_encoding(\"cl100k_base\").decode(tokens[:cutoff]))\n",
    "        tokens = tokens[cutoff:]\n",
    "    \n",
    "    # Add the final chunk (which is within limit)\n",
    "    if tokens:\n",
    "        chunks.append(tiktoken.get_encoding(\"cl100k_base\").decode(tokens))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    \"\"\"Main class for analyzing tender documents\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=MAX_COMPLETION_TOKENS,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Chunk the document into smaller parts\n",
    "            return chunk_by_tokens(text, max_tokens=MAX_TOKENS)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def process_query(self, query: str, text: str) -> str:\n",
    "        \"\"\"Process a single query against the text\"\"\"\n",
    "        try:\n",
    "            # Split text into smaller chunks to avoid token limit errors\n",
    "            chunks = chunk_by_tokens(text, max_tokens=MAX_TOKENS)\n",
    "            \n",
    "            full_response = \"\"\n",
    "            \n",
    "            # Process each chunk and combine the responses\n",
    "            for chunk in chunks:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = self.chain.run(\n",
    "                        input_documents=[Document(page_content=chunk)],\n",
    "                        question=query\n",
    "                    )\n",
    "                full_response += response.strip() + \" \"\n",
    "            \n",
    "            return full_response.strip()  # Combine all responses from chunks\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function with modified response format\"\"\"\n",
    "        try:\n",
    "            # Process document and get chunks\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            combined_text = \" \".join(chunks)\n",
    "            results = []\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, combined_text): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process all text files in the given folder and its subfolders\n",
    "    \n",
    "    Args:\n",
    "        base_folder (str): Base folder path containing subfolders with text files\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing results for all processed files\n",
    "    \"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "    \n",
    "    # Walk through all subfolders\n",
    "    for root, _, files in os.walk(base_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Get tender number from folder name\n",
    "                    tcno = os.path.basename(os.path.dirname(file_path))\n",
    "                    \n",
    "                    # Analyze the tender document\n",
    "                    results = analyzer.analyze_tender(file_path)\n",
    "                    \n",
    "                    # Add file information to results\n",
    "                    all_results.append({\n",
    "                        \"tcno\": tcno,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"results\": results\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    all_results.append({\n",
    "                        \"tcno\": tcno,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "                    })\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Base folder path\n",
    "    date_str = \"22-11-24\"  # You can modify this as needed\n",
    "    folder_path = f\"/data/txtfolder/dailydoc_test\"\n",
    "    \n",
    "    # Process all documents in the folder\n",
    "    results = process_folder(folder_path)\n",
    "    \n",
    "    # Print final results\n",
    "    print(f\"Processed {len(results['results'])} files.\")\n",
    "    for file_result in results['results']:\n",
    "        print(f\"Results for file {file_result['file_path']}:\")\n",
    "        for query_result in file_result[\"results\"]:\n",
    "            print(f\"{query_result['title']} -> {query_result['response']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Union\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.max_chunk_tokens = 100000  # Safe limit below model's maximum\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "        return len(text.split()) * 1.3  # Rough estimate of tokens\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            estimated_tokens = self._estimate_tokens(text)\n",
    "            \n",
    "            if current_tokens + estimated_tokens > self.max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [text]\n",
    "                current_tokens = estimated_tokens\n",
    "            else:\n",
    "                current_chunk.append(text)\n",
    "                current_tokens += estimated_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            sentences = self._split_into_sentences(text)\n",
    "            semantic_chunks = self._create_chunks(sentences)\n",
    "            return self._chunk_by_tokens(semantic_chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _merge_responses(self, responses: List[str]) -> str:\n",
    "        \"\"\"Merge multiple responses into a coherent summary\"\"\"\n",
    "        # Remove duplicates while maintaining order\n",
    "        unique_lines = []\n",
    "        seen = set()\n",
    "        for response in responses:\n",
    "            for line in response.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and line not in seen:\n",
    "                    seen.add(line)\n",
    "                    unique_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(unique_lines)\n",
    "\n",
    "    def process_query(self, query: str, chunks: List[str]) -> str:\n",
    "        \"\"\"Process a single query against multiple text chunks\"\"\"\n",
    "        try:\n",
    "            responses = []\n",
    "            for chunk in chunks:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = self.chain.run(\n",
    "                        input_documents=[Document(page_content=chunk)],\n",
    "                        question=query\n",
    "                    )\n",
    "                    responses.append(response.strip())\n",
    "            \n",
    "            return self._merge_responses(responses)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        try:\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, chunks): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process all text files in the given folder and its subfolders\"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_file = {}\n",
    "        \n",
    "        for root, _, files in os.walk(base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    future_to_file[executor.submit(analyzer.analyze_tender, file_path)] = file_path\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                results = future.result()\n",
    "                tcno = os.path.basename(os.path.dirname(file_path))\n",
    "                all_results.append({\n",
    "                    \"tcno\": tcno,\n",
    "                    \"file_path\": file_path,\n",
    "                    \"results\": results\n",
    "                })\n",
    "            except Exception as e:\n",
    "                all_results.append({\n",
    "                    \"tcno\": \"Unknown\",\n",
    "                    \"file_path\": file_path,\n",
    "                    \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "                })\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Base folder path\n",
    "    date_str = \"22-11-24\"  # You can modify this as needed\n",
    "    folder_path = f\"/data/txtfolder/dailydoc_test\"\n",
    "    \n",
    "    # Process all documents in the folder\n",
    "    results = process_folder(folder_path)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working with storepq in opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/QAAPI/qa/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed results for 78356751 in OpenSearch.\n",
      "Indexed results for 78341034 in OpenSearch.\n",
      "Indexed results for 78363562 in OpenSearch.\n",
      "Indexed results for 78353126 in OpenSearch.\n",
      "Indexed results for 78331655 in OpenSearch.\n",
      "Indexed results for 78353138 in OpenSearch.\n",
      "Indexed results for 78349785 in OpenSearch.\n",
      "Indexed results for 77909955 in OpenSearch.\n",
      "Indexed results for 78357275 in OpenSearch.\n",
      "Indexed results for 78348519 in OpenSearch.\n",
      "Indexed results for 78347571 in OpenSearch.\n",
      "Indexed results for 78362677 in OpenSearch.\n",
      "Indexed results for 78366106 in OpenSearch.\n",
      "Indexed results for 77968697 in OpenSearch.\n",
      "Indexed results for 78339728 in OpenSearch.\n",
      "Indexed results for 78346492 in OpenSearch.\n",
      "Indexed results for 78359456 in OpenSearch.\n",
      "Indexed results for 78359556 in OpenSearch.\n",
      "Indexed results for 78350030 in OpenSearch.\n",
      "Indexed results for 77495147 in OpenSearch.\n",
      "Indexed results for 78145210 in OpenSearch.\n",
      "Indexed results for 78326902 in OpenSearch.\n",
      "Indexed results for 78342527 in OpenSearch.\n",
      "Indexed results for 78355476 in OpenSearch.\n",
      "Indexed results for 78328170 in OpenSearch.\n",
      "Indexed results for 78348347 in OpenSearch.\n",
      "Indexed results for 78356749 in OpenSearch.\n",
      "Indexed results for 78356012 in OpenSearch.\n",
      "Indexed results for 78349018 in OpenSearch.\n",
      "Indexed results for 78362959 in OpenSearch.\n",
      "Indexed results for 78351704 in OpenSearch.\n",
      "Indexed results for 78340457 in OpenSearch.\n",
      "Indexed results for 78357713 in OpenSearch.\n",
      "Indexed results for 78344650 in OpenSearch.\n",
      "Indexed results for 77590876 in OpenSearch.\n",
      "Indexed results for 78339489 in OpenSearch.\n",
      "Indexed results for 78082736 in OpenSearch.\n",
      "Indexed results for 78331309 in OpenSearch.\n",
      "Indexed results for 78362200 in OpenSearch.\n",
      "Indexed results for 78330814 in OpenSearch.\n",
      "Indexed results for 78354703 in OpenSearch.\n",
      "Indexed results for 78354373 in OpenSearch.\n",
      "Indexed results for 78337066 in OpenSearch.\n",
      "Indexed results for 78344022 in OpenSearch.\n",
      "Indexed results for 78361329 in OpenSearch.\n",
      "Indexed results for 78350202 in OpenSearch.\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 147414 tokens (146902 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 147408 tokens (146896 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 147441 tokens (146929 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 147404 tokens (146892 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Error processing query: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 147417 tokens (146905 in the messages, 512 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "Indexed results for 78356247 in OpenSearch.\n",
      "Indexed results for 78341458 in OpenSearch.\n",
      "Indexed results for 78343958 in OpenSearch.\n",
      "Indexed results for 78350459 in OpenSearch.\n",
      "Indexed results for 78340292 in OpenSearch.\n",
      "Indexed results for 78356759 in OpenSearch.\n",
      "Indexed results for 77870568 in OpenSearch.\n",
      "Indexed results for 78340874 in OpenSearch.\n",
      "Indexed results for 77021394 in OpenSearch.\n",
      "Indexed results for 78339784 in OpenSearch.\n",
      "Indexed results for 78341288 in OpenSearch.\n",
      "Indexed results for 78352635 in OpenSearch.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Union\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os \n",
    "\n",
    "# # Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=512,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            # \"Extract a comprehensive list of all dates, times, and monetary values, along with their specific labels or descriptions as mentioned in the document.\": \"Important Dates\",\n",
    "            \"List of all the dates mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD amount,tender fee, tender value\":\"Importants Date\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.max_chunk_tokens = 100000  # Safe limit below model's maximum\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity\"\"\"\n",
    "        embeddings = self.model.encode([s['combined_sentence'] for s in sentences])\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "        return len(text.split()) * 1.3  # Rough estimate of tokens\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            estimated_tokens = self._estimate_tokens(text)\n",
    "            \n",
    "            if current_tokens + estimated_tokens > self.max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [text]\n",
    "                current_tokens = estimated_tokens\n",
    "            else:\n",
    "                current_chunk.append(text)\n",
    "                current_tokens += estimated_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            sentences = self._split_into_sentences(text)\n",
    "            semantic_chunks = self._create_chunks(sentences)\n",
    "            return self._chunk_by_tokens(semantic_chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # def _merge_responses(self, responses: List[str]) -> str:\n",
    "    #     \"\"\"Merge multiple responses into a coherent summary\"\"\"\n",
    "    #     # Remove duplicates while maintaining order\n",
    "    #     unique_lines = []\n",
    "    #     seen = set()\n",
    "    #     for response in responses:\n",
    "    #         for line in response.split('\\n'):\n",
    "    #             line = line.strip()\n",
    "    #             if line and line not in seen:\n",
    "    #                 seen.add(line)\n",
    "    #                 unique_lines.append(line)\n",
    "        \n",
    "    #     return '\\n'.join(unique_lines)\n",
    "    def _merge_responses(self, responses: List[str]) -> str:\n",
    "        \"\"\"Merge multiple responses into a coherent summary and remove duplicates.\"\"\"\n",
    "        unique_lines = []\n",
    "        seen = set()\n",
    "\n",
    "        for response in responses:\n",
    "            for line in response.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and line not in seen:\n",
    "                    seen.add(line)\n",
    "                    unique_lines.append(line)\n",
    "        \n",
    "        # Additional deduplication for repeated sections within a single line\n",
    "        cleaned_lines = []\n",
    "        for line in unique_lines:\n",
    "            parts = line.split()  # Tokenize the line\n",
    "            deduped_line = \" \".join(dict.fromkeys(parts))  # Remove repeated words\n",
    "            cleaned_lines.append(deduped_line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "\n",
    "\n",
    "    def process_query(self, query: str, chunks: List[str]) -> str:\n",
    "        \"\"\"Process a single query against multiple text chunks\"\"\"\n",
    "        try:\n",
    "            responses = []\n",
    "            for chunk in chunks:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = self.chain.run(\n",
    "                        input_documents=[Document(page_content=chunk)],\n",
    "                        question=query\n",
    "                    )\n",
    "                    responses.append(response.strip())\n",
    "            \n",
    "            return self._merge_responses(responses)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        try:\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, chunks): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process all text files in the given folder and its subfolders, and index results into OpenSearch.\"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "\n",
    "    # Set up OpenSearch client\n",
    "    index_name = 'tprocanswers'\n",
    "    opensearch_client = OpenSearch(\n",
    "        hosts=['https://localhost:9200'],\n",
    "        http_auth=(\"admin\", \"4Z*lwtz,,2T:0TGu\"),\n",
    "        use_ssl=True,\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_file = {}\n",
    "        \n",
    "        for root, _, files in os.walk(base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    future_to_file[executor.submit(analyzer.analyze_tender, file_path)] = file_path\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                results = future.result()\n",
    "                tcno = os.path.basename(os.path.dirname(file_path))\n",
    "                all_results.append({\n",
    "                    \"tcno\": tcno,\n",
    "                    # \"file_path\": file_path,\n",
    "                    \"results\": results\n",
    "                })\n",
    "                \n",
    "                # Index results into OpenSearch\n",
    "                opensearch_client.index(index=index_name, id=tcno, body={\"file_path\": file_path, \"results\": results})\n",
    "                print(f\"Indexed results for {tcno} in OpenSearch.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                all_results.append({\n",
    "                    \"tcno\": \"Unknown\",\n",
    "                    # \"file_path\": file_path,\n",
    "                    \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "                })\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Base folder path\n",
    "    date_str = \"22-11-24\"  # You can modify this as needed\n",
    "    folder_path = f\"/data/txtfolder/dailydocument_23-11-24_txt\"\n",
    "    \n",
    "    # Process all documents in the folder\n",
    "    results = process_folder(folder_path)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working with our hosted model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed results for 78356751 in OpenSearch.\n",
      "Indexed results for 78363562 in OpenSearch.\n",
      "Indexed results for 78341034 in OpenSearch.\n",
      "Indexed results for 78353138 in OpenSearch.\n",
      "Indexed results for 78353126 in OpenSearch.\n",
      "Indexed results for 78366106 in OpenSearch.\n",
      "Indexed results for 78331655 in OpenSearch.\n",
      "Indexed results for 78348519 in OpenSearch.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import requests\n",
    "from typing import Dict, List, Any, Union\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_embeddings_via_api(sentence):\n",
    "    \"\"\"Get embeddings from API (using all-mpnet-base-v2 model)\"\"\"\n",
    "    response = requests.post(\n",
    "        \"http://0.0.0.0:5002/embeddings\",\n",
    "        json={\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"input\": [sentence]}\n",
    "    )\n",
    "    return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=512,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"List of all the dates mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD amount,tender fee, tender value\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.max_chunk_tokens = 100000  # Safe limit below model's maximum\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity using API for embeddings\"\"\"\n",
    "        embeddings = [get_embeddings_via_api(s['combined_sentence']) for s in sentences]\n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "        return len(text.split()) * 1.3  # Rough estimate of tokens\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            estimated_tokens = self._estimate_tokens(text)\n",
    "            \n",
    "            if current_tokens + estimated_tokens > self.max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [text]\n",
    "                current_tokens = estimated_tokens\n",
    "            else:\n",
    "                current_chunk.append(text)\n",
    "                current_tokens += estimated_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            sentences = self._split_into_sentences(text)\n",
    "            semantic_chunks = self._create_chunks(sentences)\n",
    "            return self._chunk_by_tokens(semantic_chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _merge_responses(self, responses: List[str]) -> str:\n",
    "        \"\"\"Merge multiple responses into a coherent summary and remove duplicates.\"\"\"\n",
    "        unique_lines = []\n",
    "        seen = set()\n",
    "\n",
    "        for response in responses:\n",
    "            for line in response.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and line not in seen:\n",
    "                    seen.add(line)\n",
    "                    unique_lines.append(line)\n",
    "        \n",
    "        cleaned_lines = []\n",
    "        for line in unique_lines:\n",
    "            parts = line.split()  # Tokenize the line\n",
    "            deduped_line = \" \".join(dict.fromkeys(parts))  # Remove repeated words\n",
    "            cleaned_lines.append(deduped_line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "\n",
    "    def process_query(self, query: str, chunks: List[str]) -> str:\n",
    "        \"\"\"Process a single query against multiple text chunks\"\"\"\n",
    "        try:\n",
    "            responses = []\n",
    "            for chunk in chunks:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = self.chain.run(\n",
    "                        input_documents=[Document(page_content=chunk)],\n",
    "                        question=query\n",
    "                    )\n",
    "                    responses.append(response.strip())\n",
    "            \n",
    "            return self._merge_responses(responses)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        try:\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, chunks): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "        \n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process all text files in the given folder and its subfolders, and index results into OpenSearch.\"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "\n",
    "    # Set up OpenSearch client\n",
    "    index_name = 'tprocanswers'\n",
    "    opensearch_client = OpenSearch(\n",
    "        hosts=['https://localhost:9200'],\n",
    "        http_auth=(\"admin\", \"4Z*lwtz,,2T:0TGu\"),\n",
    "        use_ssl=True,\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_file = {}\n",
    "        \n",
    "        for root, _, files in os.walk(base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    future_to_file[executor.submit(analyzer.analyze_tender, file_path)] = file_path\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                results = future.result()\n",
    "                tcno = os.path.basename(os.path.dirname(file_path))\n",
    "                all_results.append({\n",
    "                    \"tcno\": tcno,\n",
    "                    # \"file_path\": file_path,\n",
    "                    \"results\": results\n",
    "                })\n",
    "                \n",
    "                # Index results into OpenSearch\n",
    "                opensearch_client.index(index=index_name, id=tcno, body={\"file_path\": file_path, \"results\": results})\n",
    "                print(f\"Indexed results for {tcno} in OpenSearch.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                all_results.append({\n",
    "                    \"tcno\": \"Unknown\",\n",
    "                    # \"file_path\": file_path,\n",
    "                    \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "                })\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Base folder path\n",
    "    date_str = \"22-11-24\"  # You can modify this as needed\n",
    "    folder_path = f\"/data/txtfolder/dailydocument_23-11-24_txt\"\n",
    "    \n",
    "    # Process all documents in the folder\n",
    "    results = process_folder(folder_path)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parrlel processing in embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed results for 78356751 in OpenSearch.\n",
      "Indexed results for 78363562 in OpenSearch.\n",
      "Indexed results for 78341034 in OpenSearch.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import requests\n",
    "from typing import Dict, List, Any, Union\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_embeddings_via_api(sentences: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Get embeddings from API concurrently for a list of sentences\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        results = list(executor.map(lambda sentence: requests.post(\n",
    "            \"http://0.0.0.0:5002/embeddings\",\n",
    "            json={\"model\": \"sentence-transformers/all-mpnet-base-v2\", \"input\": [sentence]}\n",
    "        ).json()[\"data\"][0][\"embedding\"], sentences))\n",
    "    \n",
    "    return results\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"List of all the dates mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD amount,tender fee, tender value\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.max_chunk_tokens = 100000  # Safe limit below model's maximum\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity using API for embeddings\"\"\"\n",
    "        sentences_text = [s['combined_sentence'] for s in sentences]\n",
    "        embeddings = get_embeddings_via_api(sentences_text)\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "        return len(text.split()) * 1.3  # Rough estimate of tokens\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            estimated_tokens = self._estimate_tokens(text)\n",
    "            \n",
    "            if current_tokens + estimated_tokens > self.max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [text]\n",
    "                current_tokens = estimated_tokens\n",
    "            else:\n",
    "                current_chunk.append(text)\n",
    "                current_tokens += estimated_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            sentences = self._split_into_sentences(text)\n",
    "            semantic_chunks = self._create_chunks(sentences)\n",
    "            return self._chunk_by_tokens(semantic_chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _merge_responses(self, responses: List[str]) -> str:\n",
    "        \"\"\"Merge multiple responses into a coherent summary and remove duplicates.\"\"\"\n",
    "        unique_lines = []\n",
    "        seen = set()\n",
    "\n",
    "        for response in responses:\n",
    "            for line in response.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and line not in seen:\n",
    "                    seen.add(line)\n",
    "                    unique_lines.append(line)\n",
    "        \n",
    "        cleaned_lines = []\n",
    "        for line in unique_lines:\n",
    "            parts = line.split()  # Tokenize the line\n",
    "            deduped_line = \" \".join(dict.fromkeys(parts))  # Remove repeated words\n",
    "            cleaned_lines.append(deduped_line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "\n",
    "    def process_query(self, query: str, chunks: List[str]) -> str:\n",
    "        \"\"\"Process a single query against multiple text chunks\"\"\"\n",
    "        try:\n",
    "            responses = []\n",
    "            for chunk in chunks:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = self.chain.run(\n",
    "                        input_documents=[Document(page_content=chunk)],\n",
    "                        question=query\n",
    "                    )\n",
    "                    responses.append(response.strip())\n",
    "            \n",
    "            return self._merge_responses(responses)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        try:\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, chunks): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "        \n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process all text files in the given folder and its subfolders, and index results into OpenSearch.\"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "\n",
    "    # Set up OpenSearch client\n",
    "    index_name = 'tprocanswers'\n",
    "    opensearch_client = OpenSearch(\n",
    "        hosts=['https://localhost:9200'],\n",
    "        http_auth=(\"admin\", \"4Z*lwtz,,2T:0TGu\"),\n",
    "        use_ssl=True,\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_file = {}\n",
    "        \n",
    "        for root, _, files in os.walk(base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    future_to_file[executor.submit(analyzer.analyze_tender, file_path)] = file_path\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                results = future.result()\n",
    "                tcno = os.path.basename(os.path.dirname(file_path))\n",
    "                all_results.append({\n",
    "                    \"tcno\": tcno,\n",
    "                    # \"file_path\": file_path,\n",
    "                    \"results\": results\n",
    "                })\n",
    "                \n",
    "                # Index results into OpenSearch\n",
    "                opensearch_client.index(index=index_name, id=tcno, body={\"file_path\": file_path, \"results\": results})\n",
    "                print(f\"Indexed results for {tcno} in OpenSearch.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                all_results.append({\n",
    "                    \"tcno\": \"Unknown\",\n",
    "                    # \"file_path\": file_path,\n",
    "                    \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "                })\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Base folder path\n",
    "    date_str = \"22-11-24\"  # You can modify this as needed\n",
    "    folder_path = f\"/data/txtfolder/dailydocument_23-11-24_txt\"\n",
    "    \n",
    "    # Process all documents in the folder\n",
    "    results = process_folder(folder_path)\n",
    "    \n",
    "    # Print results (optional)\n",
    "    import json\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the duplicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 13:25:41,499 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:41,501 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 80061 tokens (79037 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:41,638 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:41,639 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 80067 tokens (79043 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:41,770 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:41,771 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 80094 tokens (79070 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:41,906 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:41,908 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 80057 tokens (79033 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:42,045 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:42,046 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 80070 tokens (79046 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:42,098 - INFO: PUT https://localhost:9200/tprocanswers/_doc/78349785 [status:200 request:0.050s]\n",
      "2024-11-26 13:25:42,098 - INFO: Indexed results for 78349785 in OpenSearch.\n",
      "2024-11-26 13:25:47,541 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:47,543 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 106236 tokens (105212 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:47,899 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:47,899 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:47,901 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 106263 tokens (105239 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:47,903 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 106226 tokens (105202 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:48,078 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:48,080 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 106230 tokens (105206 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:48,262 - INFO: HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2024-11-26 13:25:48,263 - WARNING: Error processing chunk: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 106239 tokens (105215 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "2024-11-26 13:25:48,268 - INFO: PUT https://localhost:9200/tprocanswers/_doc/70398187 [status:200 request:0.003s]\n",
      "2024-11-26 13:25:48,269 - INFO: Indexed results for 70398187 in OpenSearch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"results\": [\n",
      "        {\n",
      "            \"tcno\": \"78349785\",\n",
      "            \"results\": [\n",
      "                {\n",
      "                    \"title\": \"Prequalification Criteria\",\n",
      "                    \"response\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"title\": \"Scope of Work\",\n",
      "                    \"response\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"title\": \"Important Dates\",\n",
      "                    \"response\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"title\": \"Supporting Documents\",\n",
      "                    \"response\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"title\": \"Contact Details\",\n",
      "                    \"response\": \"\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"tcno\": \"70398187\",\n",
      "            \"results\": [\n",
      "                {\n",
      "                    \"title\": \"Scope of Work\",\n",
      "                    \"response\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"title\": \"Important Dates\",\n",
      "                    \"response\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"title\": \"Supporting Documents\",\n",
      "                    \"response\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"title\": \"Prequalification Criteria\",\n",
      "                    \"response\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"title\": \"Contact Details\",\n",
      "                    \"response\": \"\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import requests\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Any, Union\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('tender_analyzer.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_embeddings_via_api(sentence):\n",
    "    \"\"\"Get embeddings from API (using all-mpnet-base-v2 model)\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://0.0.0.0:5002/embeddings\",\n",
    "            json={\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"input\": [sentence]}\n",
    "        )\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Embedding API error: {e}\")\n",
    "        return None\n",
    "\n",
    "class TenderAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            openai_api_base=\"http://localhost:8000/v1\",\n",
    "            openai_api_key=\"FAKE\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        self.chain = load_qa_chain(self.llm, chain_type='stuff')\n",
    "        self.queries = {\n",
    "            \"What are the functional requirements, also known as the scope of work, mentioned in the document?\": \"Scope of Work\",\n",
    "            \"Extract clauses that specify Pre-Qualification Criteria or eligibility criteria.\": \"Prequalification Criteria\",\n",
    "            \"List all supporting documents required for this tender.\": \"Supporting Documents\",\n",
    "            \"List of all the dates mentioned in the tender document which should include Bid submission end date or due date of tender, Bid validity, Opening date, closing date, pre bid meeting date, EMD amount,tender fee, tender value\": \"Important Dates\",\n",
    "            \"Extract the contact details of the officer from this document, including their name, email ID, and contact number.\": \"Contact Details\"\n",
    "        }\n",
    "        self.max_chunk_tokens = 100000  # Safe limit below model's maximum\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into sentences with metadata\"\"\"\n",
    "        sentences = [{'sentence': s, 'index': i} \n",
    "                    for i, s in enumerate(re.split(r'(?<=[.?!])\\s+', text))]\n",
    "        return self._combine_sentences(sentences)\n",
    "\n",
    "    def _combine_sentences(self, sentences: List[Dict[str, Any]], buffer_size: int = 1) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Combine sentences with context\"\"\"\n",
    "        combined = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            context = []\n",
    "            for j in range(max(0, i - buffer_size), i):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            context.append(sent['sentence'])\n",
    "            for j in range(i + 1, min(len(sentences), i + buffer_size + 1)):\n",
    "                context.append(sentences[j]['sentence'])\n",
    "            sent['combined_sentence'] = ' '.join(context)\n",
    "            combined.append(sent)\n",
    "        return combined\n",
    "\n",
    "    def _create_chunks(self, sentences: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Create document chunks based on semantic similarity using API for embeddings\"\"\"\n",
    "        embeddings = [get_embeddings_via_api(s['combined_sentence']) for s in sentences]\n",
    "        embeddings = [emb for emb in embeddings if emb is not None]\n",
    "        \n",
    "        if not embeddings:\n",
    "            logging.warning(\"No embeddings could be generated\")\n",
    "            return [' '.join([s['sentence'] for s in sentences])]\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = np.dot(embeddings[i], embeddings[i + 1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "            distances.append(1 - similarity)\n",
    "        \n",
    "        threshold = np.percentile(distances, 95)\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                chunk = ' '.join([s['sentence'] for s in sentences[start_idx:i + 1]])\n",
    "                chunks.append(chunk)\n",
    "                start_idx = i + 1\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk = ' '.join([s['sentence'] for s in sentences[start_idx:]])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "        return len(text.split()) * 1.3  # Rough estimate of tokens\n",
    "\n",
    "    def _chunk_by_tokens(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"Split texts into smaller chunks based on token count\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            estimated_tokens = self._estimate_tokens(text)\n",
    "            \n",
    "            if current_tokens + estimated_tokens > self.max_chunk_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [text]\n",
    "                current_tokens = estimated_tokens\n",
    "            else:\n",
    "                current_chunk.append(text)\n",
    "                current_tokens += estimated_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def _merge_responses(self, responses: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Merge multiple responses into a coherent summary with improved deduplication.\n",
    "        \n",
    "        Args:\n",
    "            responses (List[str]): List of response strings to merge\n",
    "        \n",
    "        Returns:\n",
    "            str: A merged, deduplicated response string\n",
    "        \"\"\"\n",
    "        # Remove empty responses\n",
    "        responses = [resp.strip() for resp in responses if resp.strip()]\n",
    "        \n",
    "        if not responses:\n",
    "            return \"\"\n",
    "        \n",
    "        # If only one response, return it directly\n",
    "        if len(responses) == 1:\n",
    "            return responses[0]\n",
    "        \n",
    "        # Tokenize and clean each response\n",
    "        cleaned_responses = []\n",
    "        for response in responses:\n",
    "            # Split into lines and clean each line\n",
    "            lines = response.split('\\n')\n",
    "            cleaned_lines = []\n",
    "            \n",
    "            for line in lines:\n",
    "                # Remove extra whitespace and convert to lowercase for comparison\n",
    "                cleaned_line = ' '.join(line.split())\n",
    "                \n",
    "                # Skip if line is too short or already seen\n",
    "                if len(cleaned_line) > 3 and cleaned_line not in cleaned_lines:\n",
    "                    cleaned_lines.append(cleaned_line)\n",
    "            \n",
    "            # Combine cleaned lines for this response\n",
    "            cleaned_responses.append('\\n'.join(cleaned_lines))\n",
    "        \n",
    "        # Merge unique responses\n",
    "        final_lines = []\n",
    "        seen_lines = set()\n",
    "        \n",
    "        for response in cleaned_responses:\n",
    "            for line in response.split('\\n'):\n",
    "                # Further clean and normalize the line\n",
    "                normalized_line = ' '.join(line.split())\n",
    "                \n",
    "                # Add line if it's not a duplicate and brings new information\n",
    "                if normalized_line and normalized_line not in seen_lines:\n",
    "                    final_lines.append(line)\n",
    "                    seen_lines.add(normalized_line)\n",
    "        \n",
    "        return '\\n'.join(final_lines)\n",
    "\n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and split into chunks\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            sentences = self._split_into_sentences(text)\n",
    "            semantic_chunks = self._create_chunks(sentences)\n",
    "            return self._chunk_by_tokens(semantic_chunks)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing document {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def process_query(self, query: str, chunks: List[str]) -> str:\n",
    "        \"\"\" Process a single query against multiple text chunks with strict response handling.for duplicated\"\"\"\n",
    "        try:\n",
    "            responses = []\n",
    "            for chunk in chunks:\n",
    "                try:\n",
    "                    with get_openai_callback() as cb:\n",
    "                        response = self.chain.run(\n",
    "                            input_documents=[Document(page_content=chunk)],\n",
    "                            question=query\n",
    "                        )\n",
    "                        responses.append(response.strip())\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing chunk: {e}\")\n",
    "            \n",
    "            return self._merge_responses(responses)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing query: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    # def process_query(self, query: str, chunks: List[str]) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Process a single query against multiple text chunks.\n",
    "        \n",
    "    #     Args:\n",
    "    #         query (str): The specific query to process\n",
    "    #         chunks (List[str]): Text chunks to analyze\n",
    "        \n",
    "    #     Returns:\n",
    "    #         str: Consolidated unique response\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         responses = []\n",
    "            \n",
    "    #         # Process each chunk with a timeout and error handling\n",
    "    #         for chunk in chunks:\n",
    "    #             try:\n",
    "    #                 # Use a shorter context window if possible\n",
    "    #                 if len(chunk) > self.max_chunk_tokens:\n",
    "    #                     chunk = chunk[:self.max_chunk_tokens]\n",
    "                    \n",
    "    #                 with get_openai_callback() as cb:\n",
    "    #                     # Add query refinement to improve precision\n",
    "    #                     refined_query = f\"Precisely and concisely {query}\"\n",
    "                        \n",
    "    #                     response = self.chain.run(\n",
    "    #                         input_documents=[Document(page_content=chunk)],\n",
    "    #                         question=refined_query\n",
    "    #                     )\n",
    "                        \n",
    "    #                     # Basic response validation\n",
    "    #                     cleaned_response = response.strip()\n",
    "    #                     if cleaned_response and len(cleaned_response.split()) > 3:\n",
    "    #                         responses.append(cleaned_response)\n",
    "                \n",
    "    #             except Exception as e:\n",
    "    #                 logging.warning(f\"Chunk processing error: {e}\")\n",
    "            \n",
    "    #         # Merge responses with strict deduplication\n",
    "    #         final_response = self._merge_responses(responses)\n",
    "            \n",
    "    #         # Final cleaning\n",
    "    #         return '\\n'.join(line.strip() for line in final_response.split('\\n') if line.strip())\n",
    "    \n",
    "    #     except Exception as e:\n",
    "    #         logging.error(f\"Query processing error: {e}\")\n",
    "    #         return f\"Error processing query: {str(e)}\"\n",
    "        \n",
    "\n",
    "\n",
    "    def analyze_tender(self, file_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Main analysis function\"\"\"\n",
    "        try:\n",
    "            chunks = self.process_document(file_path)\n",
    "            if not chunks:\n",
    "                return [{\"title\": title, \"response\": \"Error: Failed to process document\"} \n",
    "                        for title in self.queries.values()]\n",
    "            \n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=len(self.queries)) as executor:\n",
    "                future_to_query = {\n",
    "                    executor.submit(self.process_query, query, chunks): title\n",
    "                    for query, title in self.queries.items()\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(future_to_query):\n",
    "                    title = future_to_query[future]\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": response\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing {title}: {e}\")\n",
    "                        results.append({\n",
    "                            \"title\": title,\n",
    "                            \"response\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Overall analysis error: {e}\")\n",
    "            return [{\"title\": title, \"response\": f\"Error: {str(e)}\"} \n",
    "                    for title in self.queries.values()]\n",
    "\n",
    "def process_folder(base_folder: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process all text files in the given folder and its subfolders, and index results into OpenSearch.\"\"\"\n",
    "    analyzer = TenderAnalyzer()\n",
    "    all_results = []\n",
    "\n",
    "    # Set up OpenSearch client\n",
    "    index_name = 'tprocanswers'\n",
    "    try:\n",
    "        opensearch_client = OpenSearch(\n",
    "            hosts=['https://localhost:9200'],\n",
    "            http_auth=(\"admin\", \"4Z*lwtz,,2T:0TGu\"),\n",
    "            use_ssl=True,\n",
    "            verify_certs=False,\n",
    "            ssl_show_warn=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"OpenSearch connection error: {e}\")\n",
    "        return {\"results\": [], \"error\": str(e)}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_file = {}\n",
    "        \n",
    "        for root, _, files in os.walk(base_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    future_to_file[executor.submit(analyzer.analyze_tender, file_path)] = file_path\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                results = future.result()\n",
    "                tcno = os.path.basename(os.path.dirname(file_path))\n",
    "                result_entry = {\n",
    "                    \"tcno\": tcno,\n",
    "                    \"results\": results\n",
    "                }\n",
    "                all_results.append(result_entry)\n",
    "                \n",
    "                # Index results into OpenSearch\n",
    "                try:\n",
    "                    opensearch_client.index(index=index_name, id=tcno, body={\"file_path\": file_path, \"results\": results})\n",
    "                    logging.info(f\"Indexed results for {tcno} in OpenSearch.\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to index results for {tcno}: {e}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process file {file_path}: {str(e)}\")\n",
    "                all_results.append({\n",
    "                    \"tcno\": \"Unknown\",\n",
    "                    \"results\": [{\"title\": \"Error\", \"response\": f\"Failed to process file: {str(e)}\"}]\n",
    "                })\n",
    "    \n",
    "    return {\"results\": all_results}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Base folder path\n",
    "        date_str = \"22-11-24\"  # You can modify this as needed\n",
    "        folder_path = f\"/data/txtfolder/dailydoc_test\"\n",
    "        \n",
    "        # Process all documents in the folder\n",
    "        results = process_folder(folder_path)\n",
    "        \n",
    "        # Print results (optional)\n",
    "        print(json.dumps(results, indent=4))\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Main function error: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subfolder 78349785 copied to /data/txtfolder/dailydoc_test\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "main_folder =  \"/data/txtfolder/dailydocument_23-11-24_txt\"\n",
    "# main_folder = \n",
    "subfolder_name = '78349785'\n",
    "destination_folder = \"/data/txtfolder/dailydoc_test\"\n",
    "\n",
    "# Build paths\n",
    "source_path = os.path.join(main_folder, subfolder_name)\n",
    "destination_path = os.path.join(destination_folder, subfolder_name)\n",
    "\n",
    "# Copy subfolder\n",
    "shutil.copytree(source_path, destination_path)\n",
    "\n",
    "print(f\"Subfolder {subfolder_name} copied to {destination_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
